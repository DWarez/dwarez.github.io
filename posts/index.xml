<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on The ML Surgeon</title><link>https://dwarez.github.io/posts/</link><description>Recent content in Posts on The ML Surgeon</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 Dario Salvati</copyright><lastBuildDate>Mon, 06 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dwarez.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Hello CUDA: A Surgical Dissection</title><link>https://dwarez.github.io/posts/hello-cuda/</link><pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><guid>https://dwarez.github.io/posts/hello-cuda/</guid><description>In case you didn&amp;rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.</description></item><item><title>An Introduction to Sparsity for Efficient Neural Network Inference</title><link>https://dwarez.github.io/posts/pruning-intro/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://dwarez.github.io/posts/pruning-intro/</guid><description>Large Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models.</description></item></channel></rss>