
[{"content":"Undoubtedly, one of the most critical aspects of machine learning is understanding the theory—without grasping how machines learn, you\u0026rsquo;ll never excel as an ML Surgeon. But being a surgeon isn\u0026rsquo;t just about theory; it’s about getting your hands dirty—writing code, setting up infrastructures, and operating on the intricacies of data. That’s why having a tailored, efficient, and functional development setup is essential to stay productive and ensure everything gets done right.\nIn this brief article, I’ll walk you through my personalized setup for coding in Python, C/C++, and most importantly, CUDA. Keep in mind, this is a highly opinionated guide—what works for me might not work for everyone, so feel free to adapt it to your own needs.\nThis article will be over before you know it, so no need to put on your gloves!\nA Machine Learning Surgeon development setup # In general, I’m not a fan of abstraction—in fact, I despise it. I want full control and awareness of everything happening under the hood. This principle guides me across many domains, including software development. So, as you explore my setup, remember that this desire for control is often the reason behind the decisions I’ve made.\nThe OS # ⚠️Note: When I refer to \u0026ldquo;Linux,\u0026rdquo; I mean a Linux distribution, which serves as an operating system. I understand that Linux itself is just the kernel.\nFor the past 13 years, I’ve relied on Linux as my operating system of choice. I believe it’s the ideal platform for developers because it offers complete control over your machine, and the open-source community continuously creates and supports an array of incredible tools that you can use freely for personal projects or professional work. Unless you’re aiming to develop for specific environments, such as Apple software, I see no reason not to use Linux.\nThroughout my extensive experience with Linux, I’ve experimented with various distributions, desktop environments, tools, and much more. Ultimately, I always gravitate toward an Arch-based distribution. Currently, I’m using Arch Linux itself, but I also find myself comfortable with EndeavourOS and similar alternatives.\nI prefer to use a tiling window manager, specifically Hyprland. I believe that tiling window managers enhance productivity by optimizing screen space and reducing excessive mouse usage, which is often unnecessary for many tasks.\nThe terminal # The terminal is an essential tool for a developer, much like a scalpel is for a surgeon. You can accomplish nearly everything in it: writing code, managing files and Git repositories, compiling or interpreting code, and launching commands for all kinds of purposes. Because of its importance, it’s one of the tools I configure most thoroughly—it must seamlessly align with my workflow.\nMy current terminal emulator of choice is Kitty, though any terminal that suits your needs will do. As for the shell, I use zsh, which I find superior to bash in many ways. I’ve experimented with fish and nushell, but I always return to zsh. While zsh lacks certain features like syntax highlighting and command suggestions, these can easily be added through plugins managed by a plugin manager (I use OhMyZsh).\nIn combination with zsh, I rely on tmux for handling multiple windows and panes. This is invaluable when working with several terminal panes simultaneously. With tmux, I can avoid opening multiple instances of Kitty and instead efficiently navigate between panes, split views, detach commands, and more. If you haven’t used tmux yet, I highly recommend giving it a try. I also use TPM (Tmux Plugin Manager) to manage external plugins, which further enhance tmux’s functionality.\nTo streamline my workflow, I’ve customized my .zshrc file to automatically launch a tmux session whenever I open a terminal, which saves time and keeps everything organized.\nThe editor # If you use an IDE, I don’t blame you. Setting up a proper development environment can be challenging, and IDEs simplify that process by offering a suite of tools and configurations right out of the box. However, this convenience clashes with my need for total control, which is why I stopped using them a long time ago. For years, I relied on Visual Studio Code, which, at its core, is just an editor that can be transformed into an IDE with extensions and straightforward configuration. But recently, I’ve grown frustrated with some of its limitations—its sluggishness (mainly due to its background trackers), and the often confusing and overlapping configurations, which make it difficult to pinpoint the cause of specific behaviors. That’s when I decided to give Neovim a try, and I was genuinely impressed by its speed and workflow—once you get past the steep learning curve. Learning Vim motions transforms your workflow completely, reducing your dependency on the mouse. This approach aligns closely with the philosophy of tiling window managers and provides a significant boost to both productivity and speed.\nNeovim is highly customizable and extensible, much like how you can extend VS Code with plugins. However, the key difference is control. With Neovim, every aspect of the configuration is in your hands. You must be deliberate about which plugins you install, how you configure them, and their specific purposes. This results in a clean, optimized setup that is far faster and more efficient than any traditional IDE. If the process of configuring Neovim from scratch seems daunting, you can start with pre-built distributions like AstroVim or LunarVim, which handle most of the initial setup for you. However, I still recommend understanding their inner workings—it will be crucial if you want to further customize and optimize your environment down the road.\nA CUDA + Libtorch example # In this section, I\u0026rsquo;d like to give to you an idea of how how I use my environment to write a CUDA kernel also using Libtorch. In case you didn\u0026rsquo;t know, Libtorch is the C++ distribution of PyTorch, which provides C++ APIs still based on the ATen library. So, first thing first, let\u0026rsquo;s download the distribution on the Get Started Page. I\u0026rsquo;m downloading the distribution with CUDA 12.4 and cxx11 ABI support, but that\u0026rsquo;s not that relevant for our example.\nOnce downloaded, I will extract it in my home folder and that\u0026rsquo;s pretty much it as the \u0026ldquo;installation\u0026rdquo; goes.\nNow, I\u0026rsquo;ll create a folder project and open it using Neovim. Since I\u0026rsquo;ll be writing C++ and CUDA code, I made sure to install and configure clang, which is an LSP server which supports C++ and CUDA languages.\nI start by writing the CUDA kernel, and instantly notice that I have linting and formatting capabilities, thanks to clang. Quite cool!\nHowever, it seems like that the libtorch\u0026rsquo;s imports are not recognized by clang! This is expected, since we simply have the library\u0026rsquo;s content in our home folder, and clang is not aware of it. Tools like clang work using a compilation database, which informs the tool about the languages, external imports and commands that will be executed to compile the project and generate the binaries. We have to provide this information to clang in order to make it lint also for libtorch\u0026rsquo;s external source! We can generate a compile_commands.json file with bear\nBut a problem immediatly arises: I can\u0026rsquo;t remember the parameters for the torch::rand function! Now I have to leave my editor, go to the browser, use my mouse to click on results and waste time looking in the online documentation! Or, given my Neovim configuration, I can just hover over the method name, press Shift+K and ta-da, I can now read a documentation pop-up, directly from my editor!\nThis pop-up shows a lot of useful information, like which library provides the method, which are the input and output parameters and the signature of the method. When provided, it will also show the docstring of the method/function!\n","date":"8 October 2024","externalUrl":null,"permalink":"/posts/dev-setup/","section":"Posts","summary":"\u003cp\u003eUndoubtedly, one of the most critical aspects of machine learning is understanding the theory—without grasping how machines learn, you\u0026rsquo;ll never excel as an ML Surgeon. But being a surgeon isn\u0026rsquo;t just about theory; it’s about getting your hands dirty—writing code, setting up infrastructures, and operating on the intricacies of data. That’s why having a tailored, efficient, and \u003cstrong\u003efunctional\u003c/strong\u003e development setup is essential to stay productive and ensure everything gets done right.\u003c/p\u003e","title":"Development setup","type":"posts"},{"content":"","date":"8 October 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"8 October 2024","externalUrl":null,"permalink":"/tags/setup/","section":"Tags","summary":"","title":"Setup","type":"tags"},{"content":"","date":"8 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"8 October 2024","externalUrl":null,"permalink":"/","section":"The ML Surgeon","summary":"","title":"The ML Surgeon","type":"page"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/compiler/","section":"Tags","summary":"","title":"Compiler","type":"tags"},{"content":" You can take a look at the GitHub repository of this blogpost at this link Remember when machine learning was done using Caffe? The ML Surgeon remembers that. If you didn’t catch the reference, too bad for you!\nIn the last few days, I\u0026rsquo;ve been reflecting on how much easier machine learning has become in recent years. Not only do we now have a larger and higher-quality plethora of tools and frameworks—ones we could only dream of a few years ago—but the fact that these frameworks are so user-friendly is mind-boggling!\nThis got me thinking: are practitioners truly aware of the extreme complexity behind modern machine learning tools? Probably not. That’s why today, I want to dissect Torch Compile.\nThis article will be quite complex and lengthy, so gear up. But first, sterilize your hands.\nDissecting torch.compile: A Surgeon’s Approach to PyTorch Optimization # At the end of 2022, PyTorch 2.0 was released, bringing with it a host of improvements and new features. Among them, the standout addition was undoubtedly torch.compile, a method designed to speed up PyTorch code. Its usage is quite straightforward: pass either a torch.nn.Module or a function to the method, and you’ll get an optimized version of it. For example:\nclass MyModel(torch.nn.Module) ... model = MyModel() optimized_model = torch.compile(model) The optimized_model will, hopefully, run faster than the originally instantiated model. Later on, we’ll conduct some benchmarks to demonstrate these speedups. So far, so simple, right?\nBut what actually happens when we use torch.compile? How can a single line of code optimize a model and achieve up to 3x speedups compared to classic, eager-mode PyTorch?\nTo understand that, we’ll need to cut deeper—time to get some blood on our hands (or, ideally, gloves).\nThings to know before cutting deep # Before we dive into the complexities hidden within torch.compile, it’s essential to cover some foundational concepts. These basics are crucial for understanding the roles of the tools involved in the compilation pipeline. Trust me, the intricacies behind torch.compile are quite convoluted, and there’s a lot to grasp before you can see the full picture. So, please be patient and make sure you fully understand each step before proceeding to the next.\nSlicing Through the Layers: Dissecting PyTorch’s Computational Graphs # Have you ever wondered what really happens when you execute PyTorch code? If not, shame on you! Don’t take for granted the incredible features right at your fingertips!\nLet’s start from the beginning. Suppose you have code like this pseudocode:\nfn(x op y) where, x and y are two tensors, op is an operation (like the product * or sum +), and fn is a function (like log or sin).\nThis syntax results in a computational graph, which represents the operations that will be performed on the tensors. Below is a simple sketch of the computational graph that would result from the code above:\nIs this crooked? I can\u0026rsquo;t really tell. In Pytorch, the graph is built dynamically as operations are applied to tensors, which is often referred as define-by-run.\nBut wait! That\u0026rsquo;s only the forward step! We can’t train our models with just that! Luckily for us, Pytorch provides autograd, a system responsible for automatic differentiation. It records operations on tensors to form an autograd graph. Long story short, PyTorch automatically computes gradients for tensors. A computational graph for the backward pass looks something like this:\nIt\u0026rsquo;s definitely crooked ⚠️ Note: I forgot to add arrows from op and Z to the Derivative Magic block!\nDamn, I\u0026rsquo;m good at drawing. Anyway, in case you missed your calculus classes, the notation \\(\\frac{\\partial Z}{\\partial X}\\) and \\(\\frac{\\partial Z}{\\partial Y}\\) stands for the partial derivative of Z with respect to X (or Y).\nAs you can see, there are a lot of graphs involved. So, guess what torch.compile does to these graphs? That’s right—it optimizes them to make the overall computation faster.\nProbing the Depths: Surgical Insights into PyTorch’s FX Graphs # Let’s dive deeper into the world of graphs. In the previous section, we explored the critical role of computational graphs. But how do we go about optimizing them? I’m not referring to techniques or methodologies—I\u0026rsquo;m talking about the nuts and bolts of how we can technically modify and optimize PyTorch\u0026rsquo;s computational graphs.\nTo do that, we need a specialized toolkit. Fortunately, PyTorch equips us with just what we need: the FX toolkit.\nThe FX toolkit allows to modify torch.nn.Modules by implementing a pipeline consisting of a symbolic tracer, an intermediate representation (IR) and a Python code generator. This makes FX a powerful Python-to-Python transformation toolkit.\nThe symbolic tracer constructs a torch.fx.GraphModule by recording the operations that occur when the nn.Module is fed with fake data, called proxies.\nA GraphModule is essentially a nn.Module generated from a torch.fx.Graph, which serves as the core data structure for FX’s internal representation.\nWith just a few lines of code, we can observe how the symbolic tracer and intermediate representation function:\nimport torch import torch.fx import torch.nn as nn class MyModule(nn.Module): def __init__(self): super().__init__() self.weights = torch.nn.Parameter(torch.rand(4, 4)) self.linear = torch.nn.Linear(4, 5) def forward(self, x): return (self.linear(x) + x).relu() m = MyModule() gm = torch.fx.symbolic_trace(m) print(gm.graph) This script outputs the following graph representation:\ngraph(): %x : [num_users=2] = placeholder[target=x] %linear : [num_users=1] = call_module[target=linear](args = (%x,), kwargs = {}) %add : [num_users=1] = call_function[target=operator.add](args = (%linear, %x), kwargs = {}) %relu : [num_users=1] = call_method[target=relu](args = (%add,), kwargs = {}) return relu This output shows the graph’s intermediate representation, which is made up of Nodes. Without going too deep into the details, you can see references to module calls (e.g. linear), function calls (e.g. add), and method calls (e.g. relu). Each node also specifies the args for the operation, which are other nodes within the graph.\nOnce we have this graph, we can modify it as needed. Afterward, the code generator component takes over, creating a new GraphModule from the modified Graph data structure. I won’t dive into the specific techniques for modifying a graph here—this article is already long enough!\nStitching Together Efficiency: Introducing CUDA Graphs # While we\u0026rsquo;re on the subject of graphs, it’s worth highlighting another important feature: CUDA Graphs. Introduced in 2021, CUDA Graphs are a relatively new addition to the PyTorch ecosystem, specifically available for NVIDIA GPUs with CUDA version 10 or higher.\nTypically, when operations are executed on the GPU, each kernel launch must be initiated from the CPU—a process that introduces noticeable overhead, especially when dealing with thousands of operations. Each individual launch might be small, but when accumulated, this overhead can impact performance.\nCUDA Graphs address this by representing GPU operations as a single, cohesive graph. While building and launching this graph may initially be slower, the advantage lies in the fact that all subsequent operations remain on the GPU, significantly reducing the overhead caused by CPU-GPU communication.\nThe image below illustrates this concept perfectly:\nCredits to this blogpost Into the Operating Room: Dissecting the Mechanics of Torch Compile # After all this talk about graphs, it\u0026rsquo;s finally time to get down to business. Now, we’re ready to make the incision and dive deep into torch.compile to explore its inner workings. Armed with the knowledge we’ve gained in the previous sections, this should feel like a well-prepared field trip into the body of PyTorch, right? I certainly hope so—my head’s already spinning from the sheer complexity of it all!\nOperating on the Fly: Torch Dynamo’s JIT Bytecode Transformation # Let\u0026rsquo;s start with a definition: Torch Dynamo is a Python JIT compiler that uses CPython\u0026rsquo;s frame evaluation API to dynamically modify the bytecode generated from Pytorch source. That sentence might sound a bit overwhelming, so let’s take it step by step.\nFirst, what is Just-in-time (JIT) compilation? It’s a compilation process that occurs during the execution of a program, rather than before (as with languages like C). In Python, this means that while the program is running, its bytecode is translated into machine code, which the system then executes. Here’s a simple diagram to illustrate:\nAs you can see, the original Python source code is parsed into bytecode, which is easier to manage during execution. Now, thanks to the frame evaluation API, we can insert a middleware between the bytecode and the interpreter, as shown in the diagram below:\nThis is where Torch Dynamo comes in. It acts as a middleware, intercepting the bytecode to rewrite it and extract FX graphs from the PyTorch operations defined in the source code.\nSince Dynamo operates just-in-time, it dynamically intercepts bytecode during execution and extracts graphs based on the current state of the code. This allows us to work with dynamic graphs, adapting to the changing flow of execution. However, for the sake of performance, we want to avoid re-capturing graphs every time the same code runs—doing so repeatedly, as seen in frameworks like JAX, would result in unnecessary overhead.\nTo address this, Dynamo uses guards. These guards are conditions that check whether the graph needs to be re-captured. If nothing significant has changed since the last run, Dynamo will use the previously captured graph, avoiding the need to reconstruct it from scratch.\nHere’s a code snippet to illustrate how guards work:\nfrom typing import Callable, List import torch from torch import _dynamo as torchdynamo def custom_compiler(graph_module: torch.fx.GraphModule, dummy_inputs: List[torch.Tensor]) -\u0026gt; Callable: graph_module.graph.print_tabular() return graph_module.forward @torchdynamo.optimize(custom_compiler) def example(a: torch.Tensor, b: torch.Tensor) -\u0026gt; torch.Tensor: x = a / (torch.abs(a) + 1) return x * b for _ in range(100): example(torch.randn(10), torch.randn(10)) To observe Dynamo in action, run the script with the following command to enable the appropriate logging level:\nTORCH_LOGS=guards uv run src/dynamo.py Here’s a snippet of the output:\n[__guards] | +- GuardManager: source=L[\u0026#39;a\u0026#39;], accessed_by=DictGetItemGuardAccessor(a) [__guards] | | +- TENSOR_MATCH: check_tensor(L[\u0026#39;a\u0026#39;], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1]) # x = a / (torch.abs(a) + 1) # src/dynamo.py:14 in example As you can see, the output shows guards being used. These are essentially assertions that determine whether the graph should be reused or re-captured. For example, the check_tensor guard verifies properties of the torch.Tensor, such as dtype, device, requires_grad, and size. If any of these properties change, the guard triggers a re-capture of the graph, ensuring that it remains accurate for the current execution.\nHandling Dynamic Flow with Surgical Precision: Torch Dynamo vs. Static Tracing Tools # One of the standout features of Torch Dynamo, compared to other tracing tools like TorchScript or FX tracing, is its ability to trace dynamic graphs that involve data-dependent control flow. In simpler terms, this means that the execution path of the code depends on a dynamic value, which makes it impossible to capture within a static graph.\nHere’s a simple example:\ndef function(x: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: return y if x.sum() \u0026gt; 0 else -y In this case, the returned value depends on the sum of the tensor x. Because of this dynamic condition, the function can’t be traced by a tool that only works with static graphs.\nIf we attempt to trace this function using TorchScript, it will fail silently, producing a static graph. This means that even if the condition x.sum() \u0026gt; 0 is false, the traced function will still return y, which is incorrect.\nWith FX tracing, however, we would get an exception like:\nraise TraceError('symbolically traced variables cannot be used as inputs to control flow') torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\nTrying to bypass this by providing concrete input arguments won’t work either, as FX tracing will still generate a static graph—leading to the same issues as TorchScript.\nAlthough TorchScript can technically support data-dependent control flow, this requires significant changes to the codebase. Torch Dynamo, on the other hand, handles data-dependent control flow seamlessly, with no need for code modifications. When Dynamo encounters unsupported Python code (like control flow dependent on dynamic data), it breaks the computation graph, allows Python’s interpreter to process the unsupported code, and then resumes graph capture.\nThis feature also allows Dynamo to trace and optimize non-PyTorch code—another major limitation of both TorchScript and FX tracing.\nTorch Inductor: The Final Scalpel for Optimizing Computational Graphs # The last crucial tool in our arsenal is a compiler that knows how to transform the computational graph into highly efficient machine code. In this realm, this is typically called deep learning compiler. This is where TorchInductor comes in.\nTorchInductor acts as the key performance engine for PyTorch’s deep learning models by compiling and optimizing the graph for both inference and training modes—something that many traditional backends struggle with. Most alternative backends are limited to inference-only optimizations, leaving a significant gap in performance during the training phase. TorchInductor fills this gap by targeting both modes of operation, offering a unified solution that performs well across different stages of machine learning pipelines.\nTorchInductor supports both CPU and GPU architectures, adapting its optimizations based on the target hardware:\nFor CPUs, TorchInductor generates highly efficient C++/OpenMP code. OpenMP is a popular framework for parallel computing on CPUs, making it ideal for distributing workloads across multiple cores in modern processors. By leveraging OpenMP, Inductor ensures that the compiled code scales with CPU architectures for tasks like training and inference.\nFor GPUs, the generated code is written in Triton, a high-level programming language designed for ease of use and speed, serving as a flexible alternative to CUDA. Triton is designed to simplify GPU programming by providing Python-like syntax, making it accessible to a broader range of developers. It supports NVIDIA GPUs with compute capability 7.0 or greater, as well as AMD GPUs with ROCm 5.2 or later. Though Triton’s support for CPUs is still evolving, its GPU capabilities make it a powerful ally in the quest for optimization.\nKey Optimization Techniques with TorchInductor # Once the computational graph is fed into TorchInductor, a number of advanced optimization techniques are applied to significantly speed up the execution of your machine learning models.\nOperation Fusion\nOne of the primary optimizations that TorchInductor performs is operation fusion. This technique merges multiple operations into a single kernel, reducing the overhead associated with launching separate kernels and minimizing memory bandwidth usage. By combining operations, the system executes more tasks with fewer memory transactions, leading to noticeable performance boosts. This is particularly effective for GPU optimization, where kernel launch overhead can become a bottleneck.\nMemory Layout Transformations\nAnother key technique involves memory layout transformations. The layout of tensors in memory can have a substantial impact on performance, especially when accessing data in parallel. TorchInductor reorders tensors to match the access patterns of the target hardware, ensuring that memory accesses are efficient. This process helps reduce cache misses, improve memory locality, and maximize the performance of both CPU and GPU systems.\nLoop Unrolling\nFor CPU-bound operations, loop unrolling is a critical optimization. It involves transforming loops to execute multiple iterations in one go, reducing the overhead associated with loop control and improving cache utilization. By expanding loops, TorchInductor increases the efficiency of the CPU\u0026rsquo;s instruction pipeline, making it better suited to handle parallel workloads and improving overall throughput.\nParallelism and Hardware Utilization\nTorchInductor doesn\u0026rsquo;t just focus on memory and execution optimizations—it also maximizes hardware utilization through enhanced parallelism. For GPUs, this means leveraging more cores simultaneously, while for CPUs, this means distributing tasks efficiently across multiple cores. The overall effect is that models run faster and scale better across different hardware setups.\nScalpel in Hand: A Practical Dive into torch.compile # Enough theory—let’s get our hands dirty! As any good surgeon knows, the best way to master a technique is through practice. In this section, we’ll explore the magic of torch.compile by dissecting its output and performance. We’ll first analyze what happens under the hood when we use torch.compile, and then we’ll run a benchmark to determine if the compiled function is truly faster.\nLet’s start with a straightforward PyTorch function to keep things simple. This will allow us to focus on understanding how the compilation process works without being overwhelmed by complexity:\ndef simple_fn(x: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: z = torch.matmul(x, y) return torch.nn.functional.softmax(z, dim=1) Why this function?\nReadability: It’s easier to understand and debug the logs with a minimal example compared to a full nn.Module. Interesting Output: This function performs a matrix multiplication (matmul) followed by a softmax operation. These are common operations in deep learning, and their transformation during compilation will give us insights into PyTorch’s lower-level optimizations. Now, let’s compile the function using torch.compile with the TorchInductor backend, and then run it with some random input tensors:\n# Create input tensors x = torch.rand((100, 100)) y = torch.rand((100, 100)) # Compile the function using torch.compile() with TorchInductor backend compiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;) # Call the compiled function result = compiled_fn(x, y) If we run this script with debugging enabled (TORCH_COMPILE_DEBUG=1), a folder named torch_compile_debug will appear in the directory where the script was executed. This folder contains artifacts from the compilation process, such as:\nfx_graph_readable.py fx_graph_runnable.py fx_graph_transformed.py ir_post_fusion.txt ir_pre_fusion.txt output_code.py Let’s break down the most interesting files:\nIntermediate Representation (IR) # The Intermediate Representation (IR) files provide snapshots of the computational graph before and after fusion. These files contain a low-level, abstracted view of the operations PyTorch will perform, allowing for optimizations to be applied.\nPre-Fusion IR: Shows the state of the computational graph before any optimizations have been applied. Post-Fusion IR: Displays the graph after key optimizations, such as operation fusion, have been performed. FX Graph Artifacts # The FX graph files offer another layer of insight into the internal workings of torch.compile. Let’s open fx_graph_readable.py to examine how PyTorch translates the original function into an intermediate, traceable format:\n1 2 3 4 5 6 7 8 9 10 11 12 class \u0026lt;lambda\u0026gt;(torch.nn.Module): def forward(self, arg0_1: \u0026#34;f32[100, 100]\u0026#34;, arg1_1: \u0026#34;f32[100, 100]\u0026#34;): # File: /home/dwarez/Documents/workspace/torch_compile/inductor.py:6 in simple_fn, code: z = torch.matmul(x, y) mm: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.mm.default(arg1_1, arg0_1); arg1_1 = arg0_1 = None # File: /home/dwarez/Documents/workspace/torch_compile/inductor.py:7 in simple_fn, code: return torch.nn.functional.softmax(z, dim=1) amax: \u0026#34;f32[100, 1]\u0026#34; = torch.ops.aten.amax.default(mm, [1], True) sub: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.sub.Tensor(mm, amax); mm = amax = None exp: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.exp.default(sub); sub = None sum_1: \u0026#34;f32[100, 1]\u0026#34; = torch.ops.aten.sum.dim_IntList(exp, [1], True) div: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.div.Tensor(exp, sum_1); exp = sum_1 = None return (div,) Here:\nThe graph is structured as a PyTorch nn.Module with a forward method. Each operation (such as matmul and softmax) is translated into lower-level ATen operations (torch.ops.aten), which represent the core tensor computations that PyTorch relies on. The code clearly shows how PyTorch’s softmax function has been decomposed into its mathematical components:\namax: Maximum value extraction across the specified dimension. sub: Subtraction of the maximum value from each element in the matrix. exp: Exponentiation of the result. sum: Summation across the same dimension. div: Division to normalize the output, resulting in the final softmax. This low-level breakdown reveals the granular steps involved in even simple operations, showing how TorchInductor prepares the graph for optimization.\nWhen you see torch.ops.aten, it refers to ATen, the backend engine that powers PyTorch’s tensor computations. ATen is responsible for handling fundamental operations like matrix multiplication, element-wise functions, and reductions, ensuring that these operations are executed efficiently on both CPU and GPU.\nLet’s dive into the contents of fx_graph_runnable.py, which provides another view of the computational graph. This file is generated as part of the artifact collection during the compilation process, and it’s crucial for understanding how PyTorch turns your high-level code into something that can actually run on your hardware.\nHere’s what the file looks like:\nimport torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf import torch._inductor.inductor_prims import torch._dynamo.config import torch._inductor.config import torch._functorch.config import torch.fx.experimental._config torch._functorch.config.debug_partitioner = True torch._functorch.config.unlift_effect_tokens = True isolate_fails_code_str = None # torch version: 2.4.1+cu121 # torch cuda version: 12.1 # torch git version: 38b96d3399a695e704ed39b60dac733c3fbf20e2 # CUDA Info: # nvcc: NVIDIA (R) Cuda compiler driver # Copyright (c) 2005-2024 NVIDIA Corporation # Built on Wed_Aug_14_10:10:22_PDT_2024 # Cuda compilation tools, release 12.6, V12.6.68 # Build cuda_12.6.r12.6/compiler.34714021_0 # GPU Hardware Info: # NVIDIA GeForce RTX 4060 : 1 from torch.nn import * class Repro(torch.nn.Module): def __init__(self): super().__init__() def forward(self, arg0_1, arg1_1): mm = torch.ops.aten.mm.default(arg1_1, arg0_1); arg1_1 = arg0_1 = None amax = torch.ops.aten.amax.default(mm, [1], True) sub = torch.ops.aten.sub.Tensor(mm, amax); mm = amax = None exp = torch.ops.aten.exp.default(sub); sub = None sum_1 = torch.ops.aten.sum.dim_IntList(exp, [1], True) div = torch.ops.aten.div.Tensor(exp, sum_1); exp = sum_1 = None return (div,) def load_args(reader): buf0 = reader.storage(None, 40000) reader.tensor(buf0, (100, 100), is_leaf=True) # arg0_1 buf1 = reader.storage(None, 40000) reader.tensor(buf1, (100, 100), is_leaf=True) # arg1_1 load_args._version = 0 mod = Repro() if __name__ == \u0026#39;__main__\u0026#39;: from torch._dynamo.repro.after_aot import run_repro with torch.no_grad(): run_repro(mod, load_args, accuracy=False, command=\u0026#39;run\u0026#39;, save_dir=None, tracing_mode=\u0026#39;real\u0026#39;, check_str=None) # To run it separately, do # mod, args = run_repro(mod, load_args, accuracy=False, command=\u0026#39;get_args\u0026#39;, save_dir=None, tracing_mode=\u0026#39;real\u0026#39;, check_str=None) # mod(*args) By now you should be understaing this quite easily. Additionally to the Repro class (which is simply a runnable nn.Module of the computational graph), there\u0026rsquo;s an argument loader function, called load_args and some useful debugging information in the comments, like CUDA version and underlying hardware.\nNow, let’s take a look at the output code file (output_code.py), which contains the actual machine code generated by TorchInductor. Although the file is quite long, here’s a snippet that highlights a key portion:\ncpp_fused__softmax_0 = async_compile.cpp_pybinding([\u0026#39;const float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;], \u0026#39;\u0026#39;\u0026#39; #include \u0026#34;/tmp/torchinductor_dwarez/sk/cskh5dx62fglpphcrl6723dnmowdabouerrzy3dmqcngbxwfa7bv.h\u0026#34; extern \u0026#34;C\u0026#34; void kernel(const float* in_ptr0, ... What’s Happening Here?\nC++ to Python Binding: This snippet shows the generation of a C++ binding for the fused softmax operation. Here, PyTorch has automatically compiled the softmax function into efficient C++ code, and this function will be called directly during execution.\nAsynchronous Compilation: The async_compile.cpp_pybinding function allows this C++ kernel to be invoked asynchronously from Python, ensuring that the CPU or GPU isn’t sitting idle waiting for Python’s GIL (Global Interpreter Lock).\nFused Operations: Notice that the softmax operation has been fused into a single C++ function. Operation fusion is one of the key optimizations that TorchInductor applies to improve performance by reducing memory bandwidth and kernel launch overhead.\nBenchmarking: Is the Compiled Function Actually Faster? # Let\u0026rsquo;s put theory into practice and check if the compiled function outperforms the eager-mode function. We’ll use PyTorch\u0026rsquo;s torch.cuda utilities to measure the execution time of both. Here’s the code:\nimport numpy as np import torch N_ITERS = 10 def timed(fn): start = torch.cuda.Event(enable_timing=True) end = torch.cuda.Event(enable_timing=True) start.record() result = fn() end.record() torch.cuda.synchronize() return result, start.elapsed_time(end) / 1000 # Define a simple function def simple_fn(x, y): z = torch.matmul(x, y) return torch.nn.functional.softmax(z, dim=1) def generate_data(): return ( torch.randn((100, 100)).to(torch.float32).cuda(), torch.randn(100, 100).to(torch.float32).cuda(), ) # Compile the function using torch.compile() with TorchInductor backend compiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;) eager_times = [] for i in range(N_ITERS): inp = generate_data() with torch.no_grad(): _, eager_time = timed(lambda: simple_fn(inp[0], inp[1])) eager_times.append(eager_time) print(f\u0026#34;eager eval time {i}: {eager_time}\u0026#34;) print(\u0026#34;~\u0026#34; * 10) compile_times = [] for i in range(N_ITERS): inp = generate_data() with torch.no_grad(): _, compile_time = timed(lambda: compiled_fn(inp[0], inp[1])) compile_times.append(compile_time) print(f\u0026#34;compile eval time {i}: {compile_time}\u0026#34;) print(\u0026#34;~\u0026#34; * 10) eager_med = np.median(eager_times) compile_med = np.median(compile_times) speedup = eager_med / compile_med print( f\u0026#34;(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\u0026#34; ) print(\u0026#34;~\u0026#34; * 10) When running this script, you might see an output like this:\neager eval time 0: 0.018083839416503905 eager eval time 1: 6.585600227117538e-05 eager eval time 2: 2.332800067961216e-05 eager eval time 3: 1.8239999189972877e-05 eager eval time 4: 1.744000054895878e-05 eager eval time 5: 1.6383999958634378e-05 eager eval time 6: 1.6383999958634378e-05 eager eval time 7: 1.711999997496605e-05 eager eval time 8: 1.5231999568641186e-05 eager eval time 9: 1.535999961197376e-05 ~~~~~~~~~~ compile eval time 0: 0.8755618286132812 compile eval time 1: 0.00010204800218343735 compile eval time 2: 5.8816000819206235e-05 compile eval time 3: 4.831999912858009e-05 compile eval time 4: 4.790399968624115e-05 compile eval time 5: 4.1280001401901244e-05 compile eval time 6: 3.82080003619194e-05 compile eval time 7: 3.683200106024742e-05 compile eval time 8: 3.686400130391121e-05 compile eval time 9: 3.481600061058998e-05 ~~~~~~~~~~ (eval) eager median: 1.7280000261962412e-05, compile median: 4.45920005440712e-05, speedup: 0.3875134564748722x ~~~~~~~~~~ Surprisingly, the compiled function was slower than the eager-mode one, with a speedup less than 1 (0.39x). Let\u0026rsquo;s break down why this happens:\nLow Computational Complexity: The benchmarked function is too simple: it performs just a matrix multiplication followed by a softmax. These operations are lightweight, meaning they don’t provide enough work for the compilation optimizations to shine. The overhead introduced by the compilation process outweighs the benefits of the optimizations, resulting in slower execution.\nIn this case, there\u0026rsquo;s no significant performance gain from calling a fused softmax function because there’s nothing substantial to fuse—just a basic matrix operation and activation function.\nCompilation Overhead: Notice that the first couple of iterations of the compiled function are much slower than the rest. This is due to the overhead introduced by the compilation step. The TorchInductor backend needs time to analyze and compile the computational graph into optimized code. Once the graph is compiled, subsequent iterations are much faster because PyTorch uses the cached version of the graph, avoiding recompilation. So, if you run many iterations, the compilation time becomes less of a factor, and you’ll see the true performance gains.\nEager Mode’s Instant Execution: Eager mode is designed to run PyTorch operations immediately without any ahead-of-time optimizations, which is ideal for small, one-off operations like this. While compiled execution becomes faster over time, eager mode benefits from its simplicity and immediacy for lightweight tasks.\nWhile this example shows a slowdown, that’s because the workload is too small to benefit from TorchInductor’s optimizations. Compiled execution shines when dealing with larger models or heavier workloads. For a more substantial example where the compiled function outperforms eager mode, check out this link.\nGPU-Poor and Proud: Tweaking torch.compile Parameters for Fun (and Slower Functions) # Let\u0026rsquo;s try to tweak some torch.compile parameters and see what happens, shall we?\nFirst up, let\u0026rsquo;s play around with CUDA graphs. While CUDA graphs are great at reducing host-to-device communication overhead, that’s not exactly helpful when our function is so simple it might as well be considered an atomic operation. But hey, let\u0026rsquo;s try it anyway!\nWe can try this backend by simply specifying it in torch.compile\u0026rsquo;s arguments, like this:\ncompiled_fn = torch.compile(simple_fn, backend=\u0026#34;cudagraphs\u0026#34;) And here’s the \u0026ldquo;amazing\u0026rdquo; speedup result:\n(eval) eager median: 1.635199971497059e-05, compile median: 0.00019219200313091278, speedup: 0.08508158221251444x Oh no! It\u0026rsquo;s slower\u0026hellip; again! Turns out wrapping a super-simple function in a CUDA graph is like putting racing tires on a tricycle—you’re not going anywhere faster.\nAnother thing that we could try is to change the mode parameter. This is a way of specifying what should be the focus of the compilation. The default mode is a good balance between performance and overhead, but there are other modalities like reduce-overhead which relies on CUDA graphs or max-autotune, which also relies on CUDA graphs and Triton based matrix multiplications and convolutions.\nSo naturally, let\u0026rsquo;s crank it up to \u0026ldquo;max\u0026rdquo; with:\ncompiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;, mode=\u0026#34;max-autotune\u0026#34;) Drumroll, please\u0026hellip; and the results:\n(eval) eager median: 1.5343999955803157e-05, compile median: 4.190399870276451e-05, speedup: 0.36617030428627984x Once again, we\u0026rsquo;ve hit a slowdown. But wait, what\u0026rsquo;s this warning?\nW0924 11:37:01.192000 123584511351680 torch/_inductor/utils.py:977] [0/0] Not enough SMs to use max_autotune_gemm mode Uh-oh. PyTorch is warning me that I don’t have enough SMs. Let’s check the code that triggered this passive-aggressive insult from PyTorch:\n@functools.lru_cache(None) def is_big_gpu(index) -\u0026gt; bool: min_sms = 68 # 3080 avail_sms = torch.cuda.get_device_properties(index).multi_processor_count if avail_sms \u0026lt; min_sms: log.warning( \u0026#34;Not enough SMs to use max_autotune_gemm mode\u0026#34;, extra={\u0026#34;min_sms\u0026#34;: min_sms, \u0026#34;avail_sms\u0026#34;: avail_sms}, ) return False return True So, PyTorch basically looked at my hardware and said, \u0026ldquo;Sorry, but you\u0026rsquo;re too GPU-poor to hang with the big boys.\u0026rdquo; Not cool, PyTorch, not cool at all.\nAnd with that, I’m officially done with this article!\nConclusion: Closing the Incision on torch.compile # After a deep surgical dive into the world of torch.compile, we\u0026rsquo;ve explored key components like Torch Dynamo, Torch Inductor, CUDA graphs, and more—each acting like specialized tools in the operating room of PyTorch optimization. Dynamo handles the \u0026ldquo;diagnosis,\u0026rdquo; tracing and transforming eager-mode operations into a computational graph, while Inductor steps in like a skilled surgeon to optimize that graph and generate fast machine code for CPUs and GPUs. CUDA graphs reduce the back-and-forth between the host and the GPU, making the \u0026ldquo;patient\u0026rdquo; function more efficient by minimizing communication.\nHowever, as our benchmarks revealed, not every function needs major surgery. Sometimes, a simple bandage is better than complex procedures. In fact, when we tried to optimize a lightweight function, the compilation overhead actually made it slower. And yes, it turns out being GPU-poor can hold you back, as our GPU didn\u0026rsquo;t have the strength for max-autotune—PyTorch’s gentle reminder that sometimes, you just need a more powerful toolkit.\nIn the end, optimization is like surgery—use the right tool for the right task, and remember, not every patient needs to be on the table!\nNow, if you’ll excuse me, I’m off to cry over my GPU specs. Bye!\n","date":"1 September 2024","externalUrl":null,"permalink":"/posts/torch-compile/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003eYou can take a look at the GitHub repository of this blogpost \u003ca href=\"https://github.com/DWarez/torch_compile_blogpost\" target=\"_blank\"\u003e\u003cstrong\u003eat this link\u003c/strong\u003e\u003c/a\u003e   \n\n  \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\u003c/p\u003e","title":"Dissecting torch.compile: Surgical Precision in PyTorch Optimization","type":"posts"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/torch-compile/","section":"Tags","summary":"","title":"Torch-Compile","type":"tags"},{"content":"Hello, fellow surgeons! How is life treating you? I hope you\u0026rsquo;ve spent your vacation relaxing, far, far away from the tools of our trade. After all, a good surgeon needs to rest after a long year of work and learning, right? With that in mind, I\u0026rsquo;ve chosen a simple yet useful topic to discuss today, so you can stay relaxed and not worry about the tremendous complexity of our field—at least for now.\nI\u0026rsquo;m sure you\u0026rsquo;ve come across the term RAG at least once. It stands for Retrieval-Augmented Generation, a technique that\u0026rsquo;s become quite popular these days. Its popularity stems from the fact that RAG systems are relatively simple to implement yet highly effective in terms of performance, all while keeping infrastructure costs reasonably low.\nDon\u0026rsquo;t worry, you won\u0026rsquo;t need your gloves today. This incision will be quick and easy, and we won\u0026rsquo;t go too deep—no risk of getting blood on you!\nPrepping the Instrument: Understanding RAG # Imagine this scenario: you\u0026rsquo;re a med student who has, over the years, compiled a vast set of notes from your courses, covering all the topics you\u0026rsquo;ve studied. You’re confident that this material will help you become a great surgeon! But there\u0026rsquo;s a problem: the sheer volume of this knowledge base makes it difficult to query. Even with well-organized notes, pinpointing the exact information you need can be a long and tedious task.\nPressed for time and with many questions that need answering, you turn to a large language model (LLM) for help. You start a conversation with a free conversational agent, but soon realize that the answers are not precise enough. Maybe the responses are too generic, or in some cases, they’re completely off the mark.\nThen an idea hits you: wouldn\u0026rsquo;t it be nice if you could incorporate your own information into the model?\nSure, one way to do it is through fine-tuning: all you need to do is prepare your data into a dataset, select a pretrained, open-weights model, set up the training and evaluation scripts, pay for the infrastructure and computing power, and finally—after a few days and several thousand dollars—you’ll have your fine-tuned model! Easy, right? What do you mean you don’t want to spend thousands of dollars fine-tuning a model?! Don’t you know AI is just for the rich?\nWell, if you’re GPU-poor, you can try building a RAG system. It’s quite straightforward. First, take your knowledge base and embed it into a vector database. Then, when querying your LLM, simply add the top-k most similar documents of your knowledge base to the prompt, based on the input question. By doing this, you’re injecting knowledge that the LLM might not be aware of, helping it reason better and providing a more accurate answer.\nI like to define a RAG system as a component in a conversational pipeline that extends the knowledge base of an LLM by using the prompt, all while leaving the model’s weights unchanged. As you can imagine, this technique is much cheaper and faster to prototype compared to model fine-tuning.\nOf course, we don’t expect the full answer to a question to be contained within the retrieved documents. Instead, we assume that these documents will provide relevant information about the question, thereby aiding the agent’s reasoning process.\nSuturing the Code: Implementing RAG # Now that we have a solid understanding of what a RAG system is, let’s dive into some code that demonstrates this technique. It might seem a bit unconventional, but this time I’ll be using Python instead of CUDA! Remember, this is a relaxed article!\n🚀 I started using uv as my go-to Python package manager. I highly suggest you to try it, it\u0026rsquo;s blazing fast!\nStoring the Memories # It may seem incredible, but a vector database is exactly what it sounds like—a database for vectors. Yes, I know, take a moment to let that sink in. What do you mean it was obvious? Well, I guess you\u0026rsquo;re right. Anyhow, we’ll be using Qdrant, a blazing-fast vector database, written in Rust. It’s very easy to set up, thanks to their Docker image. Simply pull the image and make sure to install their Python client. For example:\nuv add qdrant-client The great thing about vector databases is that you can associate each vector with a payload, which is essentially a set of information about that data point. In our RAG case, we’ll map the text embedding to the text itself, allowing us to quickly find texts similar to the user’s prompt.\nSince I’m not solving a specific problem here, I didn’t have any particular data to put in the vector database. So, I created a character named Mr. Fat Raccoon and generated some sentences about him. Here’s our example knowledge base:\nimport pandas as pd data = pd.DataFrame({ \u0026#39;text\u0026#39;: [ \u0026#34;Mr. Fat Raccoon was born in Trash City.\u0026#34;, \u0026#34;This raccoon, known as Mr. Fluffy, is 80cm long.\u0026#34;, \u0026#34;Mr. Fat Raccoon weighs 27 kgs.\u0026#34;, \u0026#34;In Trash City, a raccoon named Mr. Fat Raccoon was born.\u0026#34;, \u0026#34;At 80cm long, Mr. Fat Raccoon is quite large.\u0026#34;, \u0026#34;Weighing 27 kgs, Mr. Fat Raccoon is one hefty raccoon.\u0026#34;, \u0026#34;Trash City is the birthplace of Mr. Fat Raccoon.\u0026#34;, \u0026#34;Mr. Fat Raccoon, a native of Trash City, is known for his 80cm length.\u0026#34;, \u0026#34;The weight of Mr. Fat Raccoon is 27 kgs.\u0026#34;, \u0026#34;Born in Trash City, Mr. Fat Raccoon is 80cm long and weighs 27 kgs.\u0026#34; ] }) Now that we have the data, it’s time to populate the database. First, we instantiate the embedding model, which will be used to generate text embeddings. Then, we create a collection in Qdrant, where we will insert our data points.\nWhen creating the collection, we must specify its name, the vector size—which corresponds to the size of the embeddings and is therefore model-dependent—and the type of distance metric we want to use. For this example, we’ll use cosine distance.\nEach data point will be represented by its embedding and will include the original text in its payload. Here’s the code to do that:\nfrom qdrant_client import QdrantClient, models from sentence_transformers import SentenceTransformer client = QdrantClient(url=\u0026#34;http://localhost:6333\u0026#34;) collection_name = \u0026#34;raccoon_info\u0026#34; embedding_model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) def create_knowledge_base(): if collection_name not in client.get_collections().collections: client.recreate_collection( collection_name=collection_name, vectors_config= models.VectorParams( size=384, distance=models.Distance.COSINE ) ) embeddings = embedding_model.encode(data[\u0026#39;text\u0026#39;].tolist()) points = [ models.PointStruct(id=idx, vector=embedding.tolist(), payload={\u0026#34;text\u0026#34;: row[\u0026#34;text\u0026#34;]}) for (idx, row), embedding in zip(data.iterrows(), embeddings) ] client.upsert( collection_name=collection_name, points=points ) print(\u0026#34;Data inserted into Qdrant collection successfully.\u0026#34;) Great, we now have our knowledge base embedded in a vector database! We can proceede with our experiment.\nAsking about the unknown # Now, let’s choose an LLM and ask it about Mr. Fat Raccoon. What do you expect the output to be? I’m fairly confident the model will hallucinate, but let’s find out.\nI decided to use Microsoft’s Phi-3.5-mini-instruct, just to ensure we’re working with a good conversational model.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\u0026#34;microsoft/Phi-3.5-mini-instruct\u0026#34;, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(\u0026#34;microsoft/Phi-3.5-mini-instruct\u0026#34;, trust_remote_code=True) Here’s the function we’ll call to generate the response:\ndef ask_question(prompt: str, inject_knowledge: bool = True) -\u0026gt; str: if inject_knowledge: prompt_vector = embedding_model.encode(prompt).tolist() # type: ignore search_result = client.search( collection_name=collection_name, query_vector=prompt_vector, limit=2, ) injection = \u0026#34;Considering that: \\n\u0026#34; + \u0026#34;\\n\u0026#34;.join([point.payload[\u0026#34;text\u0026#34;] for point in search_result]) # type: ignore prompt = injection + prompt inputs = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;) output = model.generate(**inputs, max_length=128, do_sample=True, temperature=0.1) return tokenizer.decode(output[0], skip_special_tokens=True) ⚠️ Don\u0026rsquo;t be an amateur! Be sure to always use the same model for both the embedding and retrieving phase!\nLet’s give it a try:\nprint(ask_question(\u0026#34;What is double the weight of Mr. Fat Raccoon?\u0026#34;, inject_knowledge=False)) The result is as follows:\nFirst, we need to calculate the weight of Mr. Fat Raccoon. We know that Mr. Fat Raccoon weighs 30 pounds more than Mr. Scary Raccoon. Since Mr. Scary Raccoon weighs 30 pounds, we add this to Mr. Fat Raccoon\u0026rsquo;s weight. So, Mr. Fat Raccoon\u0026rsquo;s weight = Mr. Scary Raccoon\u0026rsquo;s weight + 30 pounds\nWell, that’s disappointing—the model is just spitting out nonsense! First of all, Mr. Fat Raccoon weighs 27 kilos (we’re not confused Americans here; we use the metric system). And who on earth is Mr. Scary Raccoon?\nNow, let’s see what happens when we add information to the prompt using RAG.\nInjecting the Insight: Retrieving Data for the Prompt # As you can see from the previous code snippet, when setting the inject_knowledge parameter to True, the pipeline changes slightly. First, we use the embedding model to embed the input question. Then, we retrieve the top-k results from our vector database (in this case, the top two). The payload of the most similar data points is then injected into the user prompt.\nLet’s see if things change when using RAG:\nprint(ask_question(\u0026#34;What is double the weight of Mr. Fat Raccoon?\u0026#34;, inject_knowledge=True)) To find double the weight of Mr. Fat Raccoon, we simply multiply his weight by 2: 27 kgs * 2 = 54 kgs Double the weight of Mr. Fat Raccoon is 54 kgs.\nThat’s spot on! Well done, Phi-3.5! Now, let’s check the prompt that generated this response:\nConsidering that: The weight of Mr. Fat Raccoon is 27 kgs. Mr. Fat Raccoon weighs 27 kgs. What is double the weight of Mr. Fat Raccoon?\nAs you can see, our RAG system successfully inserted relevant information into the prompt, enabling the model to respond correctly. Easier said than done!\nWrapping Up the Operation: Final Thoughts # Well, there you have it, folks! We’ve successfully implemented a RAG system, enhancing our LLM’s ability to provide accurate responses by injecting relevant information into the prompt. By leveraging a vector database like Qdrant, we avoided the costly and time-consuming process of fine-tuning, all while improving the model’s performance.\nRemember, not every procedure requires a complex or expensive solution. Sometimes, a well-placed stitch—in this case, a bit of embedded knowledge—is all it takes to get the job done right. So next time you find yourself with a data-heavy problem, consider RAG as your go-to surgical tool.\nUntil next time, keep your scalpels sharp and your models smarter!\n","date":"29 August 2024","externalUrl":null,"permalink":"/posts/ten-minutes-to-rag/","section":"Posts","summary":"\u003cp\u003eHello, fellow surgeons! How is life treating you? I hope you\u0026rsquo;ve spent your vacation relaxing, far, far away from the tools of our trade. After all, a good surgeon needs to rest after a long year of work and learning, right? With that in mind, I\u0026rsquo;ve chosen a simple yet useful topic to discuss today, so you can stay relaxed and not worry about the tremendous complexity of our field—at least for now.\u003c/p\u003e","title":"A quick incision: ten minutes to RAG","type":"posts"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag","type":"tags"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/vector-db/","section":"Tags","summary":"","title":"Vector-Db","type":"tags"},{"content":"","date":"15 July 2024","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"Cuda","type":"tags"},{"content":"","date":"15 July 2024","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"Being a Machine Learning Surgeon is not an easy life. We not only have to deal with intricate machine learning systems but also navigate the additional complexities surrounding them. To be a proficient ML Surgeon, we must develop a diverse skill set. First and foremost, we need a deep understanding of machine learning and deep learning. Additionally, we must be adept at writing software, building infrastructures to host and integrate models, managing large volumes of data, and much more. This requires familiarity with numerous tools.\nOne fundamental tool in our toolkit is the profiler. Profiling involves performing dynamic code analysis to illustrate various performance indicators, such as memory usage and computational complexity.\nWhen dealing with GPU programming, profiling becomes even more crucial. Our main objective is to maximize performance by utilizing hardware resources as efficiently as possible. Given the unique architecture of GPUs and their specific characteristics, writing a perfect kernel that leverages all possible optimizations is a challenging task.\nFortunately, tools are available to help us with this difficult task. Today, we will explore how to use NVIDIA Nsight Compute to profile kernels and understand how we can optimize them.\nThis time, you won\u0026rsquo;t need any scalpels, so sit back and relax!\nPreparing for Surgery # Before we can proceed with our profiling sessions, we need two things: the profiling tool and a kernel to profile.\nI particularly enjoy using NVIDIA Nsight Compute —a tool developed by NVIDIA— because it provides an excellent interface to the CLI programs of the NVIDIA CUDA Toolkit. It offers a wealth of information and insights about the profiled kernel or application. The tool is very easy to install; simply follow the installation guide.\nFor this introductory tutorial, I\u0026rsquo;ll profile some matrix multiplication kernels, which are slightly modified versions of the ones you can find in this previous blogpost.\nTo illustrate the profiler\u0026rsquo;s capabilities, we will analyze a poorly performing kernel. This will result in a much more interesting and insightful report!\nInitiating the Procedure # After opening our project, we can click on Open Connection Dialog to configure the profiling activity we are going to launch, as shown in the figure below:\nConnection Dialog interface As you can see, this menu is split into two parts: platform-specific settings on the top and activity specifics on the bottom.\nFor the Platform Settings, we simply specify which executable to use for profiling the kernel. An executable must be launched for the kernel to be profiled. In my case, since I\u0026rsquo;m using Windows (I know, I know, but I don\u0026rsquo;t have a Linux machine at the moment), I\u0026rsquo;ll specify the .exe filepath in the Application Executable parameter while leaving everything else unchanged.\nThe Activity Settings are more interesting and should be tweaked based on the metrics we are interested in.\nFirst things first, we specify the Output Path, which is where the .rep (report) file will be stored.\nNext, we can select the Replay Mode. Replaying parts of the code during the profiling procedure is necessary because not all metrics can be computed in a single execution. For this reason, different replay modes (or strategies) can be applied, depending on the intended metrics we want to obtain and the behavior of the application. In kernel replay, the kernel is executed multiple times, for application replay, the whole application is run multiple times, while for range replay only a specified range of CUDA APIs are replayed. You can check the exact behavior on the official NVIDIA docs. For our case, let\u0026rsquo;s set the replay mode to kernel.\nThe Graph Profiling option specifies if the CUDA execution graph should be seen as a single workload or a set of individual kernel nodes. For our example, let\u0026rsquo;s leave this option set to node.\nLastly, let\u0026rsquo;s navigate to the Metrics tab and select detailed from the checkbox menu. This will ensure that addtional metrics will be collected, including the ones relative to memory usage. With this setting, a total of 459 metrics will be collected in the report —a substantial amount!\nThere are many more options available, but we\u0026rsquo;ll focus on the essentials for now. Let\u0026rsquo;s start profiling!\nDiagnosing the Patient # Initial Assessment # When we open the report, the first thing we encounter is a summary of the profiling activity, as shown in the picture below. This summary provides a wealth of useful information. Let\u0026rsquo;s break it down.\nSummary of the report ID: an unique value given to the activity in the context of the project.\nEstimated Speedup: this predicts how much faster the kernel can run with appropriate optimizations. In our case, a potential 43.75% speedup is estimated—quite significant! This is expected since we intentionally used a slow kernel for profiling.\nFunction Name and Demangled Name: these columns show the kernel\u0026rsquo;s name and input parameters from the source code, useful when the source code isn\u0026rsquo;t readily available.\nDuration: the elapsed GPU time.\nRuntime Improvement: the absolute improvement in kernel duration corresponding to the estimated speedup.\nCompute Throughput and Memory Throughput: these metrics provide insights into the efficiency of different kernel implementations.\n# Registers: the number of registers allocated per thread, important for checking occupancy.\nThe last two columns of the summary store the Grid Size and Block Size. As you can see from the example, the kernel has a block size of (6,6,1), which is defenitely sub-optimal. Could it be one of the reasons of the kernel\u0026rsquo;s inefficiency?\nAt the top of the summary, additional information about the run is provided, such as hardware used, execution time, and number of cycles. In this example, I ran the experiment on a GTX 4060—yes, I’m GPU-poor!\nAdditional information about the profiling activity At the bottom of the page, the profiler offers suggestions for improving the kernel, along with estimated speedups for each fix. Let\u0026rsquo;s examine the first suggestion:\nNote: If something is not clear to you, please read the following article.\nThat\u0026rsquo;s excellent advice! The tool not only suggests effective ways to enhance our kernel\u0026rsquo;s performance but also educates us in the process.\nThere are two additional suggestions from the profiler, but I won\u0026rsquo;t show them here as they relate to the aforementioned block size issue. Solving the block size problem will also address the other issues.\nIn-Depth Analysis # Let\u0026rsquo;s now navigate to the Details tab. Here, we can find tables containing all the numerical values for the various metrics collected during the profiling activity. The number of metrics displayed in this tab depends on the options selected in the Connection Dialog before starting the profiling activity. If you don\u0026rsquo;t see memory metrics, ensure you selected the detailed option, as mentioned earlier.\nWe won\u0026rsquo;t go through all the details here, but let\u0026rsquo;s examine a few key tables.\nIn the GPU Speed of Light Throughput table below, we can see compute and memory throughput information. The profiler also provides analytics to help us understand what\u0026rsquo;s happening at the hardware level. In this case, the compute throughput is much higher than the memory throughput, indicating that the kernel is compute-bound. This means arithmetic operations dominate the execution time. We can actually see that this is true through the Compute Workload Analysis, which shows us insight on the compute workload:\nAt the same time, the low memory throughput suggests inefficient memory access patterns in the kernel. Spoiler alert: this inefficiency is intentional for demonstration purposes. I\u0026rsquo;m not that awful at writing kernels!\nThe Memory Workload Analysis table is great to understand what is happening at the memory level. This analysis shows us different critical aspects of the kernel. First and foremost, the kernel performs operations only using the Global Memory, which is slow! Thankfully, the L1 cache has a good hit rate, of about 87%. The L2 cache, instead, is completely wasted: less than 50% hit rate an 0% compression ratio.\nThere are many, many more metrics we could check, but that\u0026rsquo;s out of the scope of this article. Feel free to explore and learn on your own!\nExamining the Source # In the Source tab, we find the original source code of the application alongside the corresponding assembly code, available in both SASS and PTX representations.\nQuoting the official NVIDIA documentation:\nPTX is a low-level parallel-thread execution virtual machine and instruction set architecture. PTX provides a stable programming model and instruction set for general purpose parallel programming, and is designed to be efficient on NVIDIA GPUs. High-level language compilers for languages such as CUDA and C/C++ generate PTX instructions, which are optimized for and translated to native target-architecture instructions.\nSASS is the low-level assembly language that compiles to binary microcode, which executes natively on NVIDIA GPU hardware.\nFrom this interface, it\u0026rsquo;s possible to quickly check the resulting assembly code by clicking on the lines of the source code. Not only that, we also have statistics and details about memory operations for each row of the original source code, which makes it easy to check the memory implications of each instruction!\nMoreover, the profiler highlights efficiency warnings near the lines that create bottlenecks. In this case, it indicates that we are performing excessive global memory accesses. This issue is explained in detail in our previous blog post about matrix multiplication.\nPost-Op Review # As a Machine Learning Surgeon, diagnosing and treating the performance ailments of your kernels is crucial. Profiling CUDA kernels with NVIDIA Nsight Compute is like having an advanced diagnostic tool in your surgical kit. It allows you to pinpoint exactly where the bottlenecks and inefficiencies lie, so that you will be guided on how to operate on the kernel to improve its performances.\nIn this blog post, we walked through the essential features of Nsight Compute, from setting up the connection dialog to analyzing key metrics and reports. By examining a poorly performing kernel, we demonstrated how to use these insights to prescribe the right optimizations and improve performance—akin to a successful surgical intervention.\nRemember, the profiler offers an extensive array of additional metrics and features that we haven\u0026rsquo;t covered here. Just as a surgeon continually learns new techniques, I encourage you to explore these tools further, experiment with different kernels, and refine your skills to ensure your models perform at their peak.\nHappy profiling, and may your kernels be ever efficient and healthy!\n","date":"15 July 2024","externalUrl":null,"permalink":"/posts/profiling-introduction/","section":"Posts","summary":"\u003cp\u003eBeing a Machine Learning Surgeon is not an easy life. We not only have to deal with intricate machine learning systems but also navigate the additional complexities surrounding them. To be a proficient ML Surgeon, we must develop a diverse skill set. First and foremost, we need a deep understanding of machine learning and deep learning. Additionally, we must be adept at writing software, building infrastructures to host and integrate models, managing large volumes of data, and much more. This requires familiarity with numerous tools.\u003c/p\u003e","title":"Performing Kernel Surgery: Profiling CUDA Kernels with NVIDIA Nsight Compute","type":"posts"},{"content":"","date":"15 July 2024","externalUrl":null,"permalink":"/tags/profiling/","section":"Tags","summary":"","title":"Profiling","type":"tags"},{"content":"During the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.\nThat project was crucial for me because it revealed an inconvenient truth: matrix multiplication is the most fundamental aspect of Machine Learning. Hence, I named my project \u0026ldquo;ML is ML\u0026rdquo;: Machine Learning is Matrix Multiplication. Although the course professor didn\u0026rsquo;t fully appreciate the title, it still earned me the highest grade. Because deep down, they knew it was true. It was, indeed, an inconvenient truth.\nNOTE: Yes, I\u0026rsquo;m well aware that tensor contraction is not the same as matrix multiplication. Still, the \u0026ldquo;ML is ML\u0026rdquo; joke only works when talking about matrices, so stick with it.\nHowever, this is not the place to explain why matrix multiplication is so fundamental to modern AI. Besides, if you\u0026rsquo;re reading this blog, you probably already know.\nInstead, as Machine Learning Surgeons, we should ask ourselves how such an important operation is implemented to fully utilize the power of GPUs. It\u0026rsquo;s easy to implement something, but it\u0026rsquo;s much harder to make it run fast! Just like humans need to train their reaction times to do tasks like driving an F1 car, we Machine Learning Surgeons must operate on the muscle fibres of kernels in order to make them fast and powerful!\nSo, put on your gloves, wield your scalpels and let\u0026rsquo;s start the operation!\nThe First Cut # Since you\u0026rsquo;re still a pratictioner, I\u0026rsquo;ll walk you through the simplest matrix multiplication kernel. But first, there is a basic concept that you have to grasp before proceeding with the code.\nMatrix Linearization # As you may have learned from the Hello CUDA article, when writing CUDA kernels, we typically use block and thread indices to select elements for computation. But what happens when dealing with higher-dimensional data structures, such as matrices? Moreover, how is memory organized on GPUs when dealing with such data structures?\nIn CUDA, memory on the device is managed linearly, meaning it is stored as a single, contiguous block of memory. Therefore, matrices are stored in this contiguous block of memory in a row-major order. Linearization involves mapping the multi-dimensional indices of a matrix to a single-dimensional index. It\u0026rsquo;s much easier to do than to explain.\nConsider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{\\mathrm{N} x \\mathrm{N}}\\) stored in a row-major order, which is the representation used by CUDA. The linear index of the element \\(\\mathrm{A}_{i, j}\\) (i-th row and j-th column of \\(\\mathrm{A}\\)) is simply given by \\(i * \\mathrm{N} + j\\). This is because in the row-major representation, the array that represents the 2D matrix is a continuous sequence of the rows of the matrix. Therefore, to acces the item \\(\\mathrm{A}_{i, j}\\) we must skip \\(i\\) rows, doing \\(i * \\mathrm{N}\\) and sum to it the column index \\(j\\).\nThis is a fundamental concept to understand before proceeding, as we will be performing numerous memory accesses using this technique.\nA Naive Kernel # Great! Now you are ready to fully understand the simplest matrix multiplication kernel. Keep in mind that I\u0026rsquo;ll avoid writing all the usual boilerplate code for instantiating variables on the host, moving data to the device, and printing results. If you have any doubts, you can check the full code here 1 2 3 4 5 6 7 8 9 10 11 12 __global__ void matMulKernel(float* C, float* A, float* B, int N) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; N \u0026amp;\u0026amp; col \u0026lt; N) { float value = 0; for (int k = 0; k \u0026lt; N; ++k) { value += A[row * N + k] * B[k * N + col]; } C[row * N + col] = value; } } You should be able to understand most of this code, but let\u0026rsquo;s quickly walk through it.\nIn lines 2-3, we define the index of the row and column of the matrix element that the thread will use for the dot product computation. Remember that the mathematical notation and the CUDA representation are inverted. While in mathematical notation we use the x-axis for referring to the rows and the y-axis for referring to the columns, in CUDA we use the y-components to compute the row index and the x-components to compute the column index.\nLine 5 simply checks the index boundaries. We must check this condition because we will likely spawn more threads than there are elements in the matrix, so some threads may compute out-of-bounds indices.\nLines 6-10 perform the actual dot product computation. We start by initializing a local variable to accumulate the result. Given that this is a local scalar variable, it will be stored in a register, the fastest type of memory available. This approach works well because the dot product, which involves summing the products of corresponding elements, can be computed incrementally. Remember this, as it will be essential for the upcoming optimization steps!\nLine 7 defines the loop that allows the thread to iterate over all elements of the rows of A and the columns of B. In each iteration, we accumulate the value by adding the product of the linearized row element from A and the linearized column element from B. If you find it difficult to visualize the linearizations, I suggest writing out a few loop iterations on a piece of paper.\nLastly, line 10 stores the computed dot product into the linearized element of C.\nFor completeness, I will also include the kernel invocation. Keep in mind that you can use the ceil() function for the gridSize\ndim3 blockSize(16, 16); dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (N + blockSize.y - 1) / blockSize.y); matMulKernel\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(d_C, d_A, d_B, N); Advanced Surgical Techniques # Now that we have a basic implementation, let\u0026rsquo;s pause to consider the kernel\u0026rsquo;s behavior.\nA subtle issue to notice is that all memory accesses we perform are done using Global Memory. If you need an explanation about memory types in CUDA, please read this blog post. In brief, Global Memory is much slower compared to other types of memory, such as Shared Memory. As a result, we are wasting time with these accesses.\nFurthermore, the kernel is actually performing more data loads than necessary. Consider the first thread of the grid, let\u0026rsquo;s call it \\(thread_{0,0}\\). This thread will compute the indexes \\((0,0)\\), therefore it will load the elements \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,0}\\) in its first iteration. Then, given that we perform a loop over the elements of the matrices, \\(thread_{0,0}\\) will also load \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,0}\\), \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,0}\\) and so on.\nLet\u0026rsquo;s now consider the second thread of the grid which we will call \\(thread_{0,1}\\). This kernel will load \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,1}\\), then \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,1}\\), then \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,1}\\) and so on.\nNotice how element \\(\\mathrm{A}_{0,0}\\) is loaded by both kernels. Since Global Memory access is not shared between threads, each thread must perform a separate loading operation to obtain the value of the matrix element. In fact, in this example, it is easy to see how all elements of \\(\\mathrm{A}\\) are loaded by both kernels. I encourage you to continue this analysis on your own so that you can see for yourself that the same is true for all elements of \\(\\mathrm{B}\\) as well.\nIt\u0026rsquo;s now clear what we can do to optimize this kernel: use Shared Memory to improve the speed of loading operations, while also avoiding redundant loading operations within a block. Specifically, we will use tiling to manage the memory traffic reduction factor. If we assume 16x16 tiles, we can reduce the memory traffic by a factor of 1/16 compared to the naive implementation, because all threads will cooperate for the memory accesses. In general, given a tile size \\(T_N\\), we reduce the memory traffic by \\(1/T_N\\). If we take this idea to the extreme, by setting \\(N = T_N\\)we could load the entire matrix into a single tile in Shared Memory, thereby maximizing the reduction in memory traffic. However, this is rarely feasible. In real-world scenarios, the size of matrices is usually so large that the entire matrix cannot fit into the Shared Memory.\nUnderstanding the balance between tile size and Shared Memory usage is crucial for optimizing performance. While larger tiles can reduce memory traffic, they are limited by the available shared memory, and practical implementations must account for this constraint.\nAdvanced Incision Techniques: Tiling # NOTE: A bit of linear algebra ahead, proceed with caution.\nAs mentioned before, the dot product can be computed incrementally. We can leverage this property to split the computation of a single dot product into phases, which correspond directly to tiles. Let\u0026rsquo;s consider two matrices \\(\\mathrm{A} \\in \\mathrm{R}^{16x16}\\) and \\(\\mathrm{B} \\in \\mathrm{R}^{16x16}\\). We want to perform a matrix multiplication which result will be represented by \\(\\mathrm{C} \\in \\mathrm{R}^{16x16}\\).\nLet\u0026rsquo;s split \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\) into 4 tiles of size 4x4 each, let\u0026rsquo;s call them \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\) and \\(\\mathrm{T_B}_{i} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\).\nAssume we want to compute the first element \\(\\mathrm{C}_{0,0}\\). In the previous kernel code, we fully loaded the first row of \\(\\mathrm{A}\\) and the first column of \\(\\mathrm{B}\\) and performed the dot product. Now, instead, we will use tiles.\nSince the dot product can be computed incrementally, we can procede as follows:\nLoad two tiles into memory. We load tiles based on the current phase. At the start, we are in phase 0, so we load \\(\\mathrm{T_A}_{0}\\) and \\(\\mathrm{T_B}_{0}\\).\nCompute all elements of the dot product that can be computed with the loaded tiles. Note that these are partial computations! The remaining parts of the computations will be performed in subsequent phases. In phase 0, we would compute:\n\\(\\mathrm{C}^{0}_{0,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{0,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,1}\\) \\(\\mathrm{C}^{0}_{1,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{1,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,1}\\) Notice that the notation \\(\\mathrm{C}^{0}\\) indicates the computation of and element of \\(\\mathrm{C}\\) for the first phase, phase 0. Subsequent phases will sum upon this initial computation to obtain the final result.\nThen, increase the phase counter and repeat the process until all computational phases are complete. Note that the number of phases is \\(N/size\\_of\\_tile\\), where \\(N\\) is the dimension of the square input matrices.\nAfter all the computations for a point of \\(\\mathrm{C}\\) are done, store the result in the actual output matrix.\nIf you\u0026rsquo;ve made it through this notation hell, you\u0026rsquo;re ready to see the actual kernel code!\nBrain Cells Cooperating # An attentive reader might ask: How is this algorithm faster? From a mathematical and algorithmic perspective, the benefits of splitting computations and using tiling might not be immediately clear and can even seem to complicate matters. However, tiling is crucial for optimizing memory access patterns, as we discussed earlier. Let\u0026rsquo;s examine the code to understand how tiling enhances performance in practice:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 __global__ void matMulKernel(float* C, float* A, float* B, int N){ __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; Width/TILE_WIDTH; ++ph) { Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } C[row * Width + col] = Cvalue; } In lines 2-3, we define the two data structures used to load the tiles from \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\). We use the __shared__ keyword to specify that this memory allocation will be performed in Shared Memory.\nIn lines 5-8, we define some variables to store the block and thread IDs for both the x and y dimensions. This is done for convenience, as writing these values explicitly each time would make the code less readable. Since we are not even close to register saturation, it’s a luxury we can afford. In lines 10-11, we simply compute the row and column indexes, since the kernel assumes that each thread will compute one element of \\(\\mathrm{C}\\).\nLine 15 starts the for loop that iterates over the computational phases. As mentioned before, there are \\(N/size\\_of\\_tile\\) phases in total. For simplicity, we assume that the dimension of the matrices is divisible by the tile size, so the division yields a whole number.\nFinally, lines 16-18 demonstrate thread cooperation. Here, the threads use the shared variables Ads and Bds to load the elements into the tiles for the current phase, ph. If you look closely at the index computations, you’ll see they are almost identical to those used in the naive implementation. The key difference is that here we skip elements by using the contribution ph * TILE_WIDTH. This adjustment is necessary because, at each phase, we have already performed the required computations using elements from the previous phases (and therefore the previous tiles). The picture below illustrates this behavior perfectly.\nVisualization of indexes computation for tiled matrix multiplication. Credits to the PMPP book Furthermore, line 18 introduces a barrier synchronization. Since threads are loading elements into memory asynchronously, we must ensure that all elements of the tiles for the current phase have been loaded before proceeding with the computation of the partial dot product.\nLines 20-22 perform exactly the operation listed as step 2 in the previous section, which is the computation of the partial dot product using the previously loaded tiled elements. This step also requires a barrier synchronization, as the elements stored in Ads and Bds will be overwritten in the next phase. We must ensure that all threads have finished the computation of the partial dot product for the current phase before proceeding to the next phase.\nFinally, in line 26, we write the computed value to the output matrix C.\nFlexible Incisions: Arbitrary Tile Size # In our previous kernel code, we assumed that the matrix size is divisible by the tile size. However, this is a strong assumption that may not always hold true. Is there a way to relax this assumption? Yes, through boundary checking.\nFirst, let\u0026rsquo;s examine what happens when the matrix size is not divisible by the tile size.\nThe first observation we make is that issues arise only with the last tile. For instance, consider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{16,16}\\) and tiles \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{3x3}, i = 0,1,2,3,4,5 \\land j=0,1,2,3,4,5\\). For example, the tile \\(\\mathrm{T_A}_{0,0}\\) is within the bounds of the matrix, so it does not present any issues. However, a tile like \\(\\mathrm{T_A}_{5,5}\\) is problematic because it extends beyond the matrix boundaries. This issue occurs because 16 is not divisible by 6, therefore some tiles will go out of bounds.\nWhat exactly happens when we go out of bounds? If we access an element beyond the last row, we will end up accessing elements from subsequent rows due to the row-major linearization of the matrix. This will lead to incorrect results. When it comes to columns, accessing elements outside the allocated memory of the matrix can have various outcomes. We might load random values, load zeros, or even cause a kernel crash, depending on the system and its handling of such memory accesses.\nWe cannot solve this problem by simply not using the aforementioned tiles, as this would leave some elements unprocessed.\nTo address this, we can perform boundary checks to ensure that the computed x and y indexes for the thread are responsible for loading valid elements. The same checks apply to the output indexes. For rows, we check that \\(row \\text{\\textless} Width \\land (ph * TILE\\_WIDTH) \\text{\\textless} Width\\). For columns we check that \\((ph * TILE\\_WIDTH + ty) \\text{\\textless} Width \\land Col \\text{\\textless} Width\\). If these conditions are not met, we load a neutral value, such as 0.0, into shared memory for the operation, so that such elements will not effect the computations.\nLastly, we will store the value only if both the row and column indexes are within the bounds of the output matrix. Therefore, the kernel code is modified as follows:\n__global__ void matMulKernel(float* A, float* B, float* C, int Width) { __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; ceil(Width/float(TILE_WIDTH)); ++ph) { if(row \u0026lt; Width \u0026amp;\u0026amp; ph * TILE_WIDTH + tx \u0026lt; Width) Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; else Ads[ty][tx] = 0; if (ph * TILE_WIDTH + ty \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; else Bds[ty][tx] = 0; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } if (row \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) C[row * Width + col] = Cvalue; } Closing the Incision # We’ve come a long way in our exploration of matrix multiplication in CUDA, haven\u0026rsquo;t we? We began with the basics—our \u0026ldquo;first cut\u0026rdquo;—where we learned how to implement a straightforward matrix multiplication kernel. From there, we moved on to the exciting phase of optimization and refinement, focusing on making our GPU computations faster and more efficient. By leveraging shared memory with tiling, we achieved significant performance improvements. We also incorporated boundary checking to allow for flexible tile sizes.\nFun fact: adapting the boundary-checked, tiled version of the kernel to support rectangular matrices is surprisingly straightforward! Just follow these steps:\nReplace Width with unsigned integer arguments: j, k, l. Update Width for Matrix Heights and Widths: Replace Width with j where it refers to the height of matrix A or the height of matrix C. Replace Width with k where it refers to the width of matrix A or the height of matrix B. Replace Width with l where it refers to the width of matrix B or the width of matrix C. That\u0026rsquo;s it, you just wrote a general matrix multiplication CUDA kernel!\nWhat? You’re not convinced? You don\u0026rsquo;t trust your brilliant doctor?!\nI see that you want proof. You want some empirical data that proves that the optimizations we saw today actually worked! Fear not! In the next blog post, we’ll dive into the art of profiling CUDA kernels. I’ll guide you through the process of using tools like NVIDIA’s Nsight Compute to measure performance improvements, analyze memory usage, and make data-driven decisions about further optimizations. We’ll get down to the nitty-gritty details of profiling, so you can see exactly how your changes impact performance. It’s going to be a thrilling ride through the land of performance metrics and profiling!\nUntil then, happy coding, and remember to wash your hands!\n","date":"10 July 2024","externalUrl":null,"permalink":"/posts/matrix-multiplication/","section":"Posts","summary":"\u003cp\u003eDuring the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.\u003c/p\u003e","title":"A Machine Learning Surgeon’s Toolkit: Advanced Matrix Multiplication in CUDA","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/matrix-multiplication/","section":"Tags","summary":"","title":"Matrix-Multiplication","type":"tags"},{"content":"","date":"14 June 2024","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"Would you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function. Knowing the philosophy behind their architectural design, the problems they aim to solve, and the reasons for their specific construction is essential for leveraging their full potential.\nExploring such a vast and complex topic can indeed be challenging. Fortunately, we don\u0026rsquo;t need to be hardware engineers. We just need to firmly grasp the basics to understand how to fully utilize these processors in our machine learning endeavors.\nThe Battle of the Brains: CPU vs. GPU # I bet my scalpels that you are familiar with programming using a CPU. Indeed, every machine learning engineer (or software engineer) must be able to do so. However, many of you may not be fully aware of the CPU\u0026rsquo;s architectural design, perhaps because you never studied it during your academic journey. Still, I\u0026rsquo;m sure you can write perfectly functioning code.\nNowadays, a programmer can effectively write code that accomplishes tasks and solves problems without a deep understanding of the underlying CPU architecture. Modern programming practices and tools abstract away much of the complexity associated with hardware specifics. High-level programming languages and comprehensive development environments allow programmers to focus on algorithm design and functionality rather than hardware details. Additionally, compilers and interpreters perform automatic code optimizations, translating high-level code into efficient machine code tailored to the specific architecture. These optimizations ensure that the code runs efficiently on various CPUs, further diminishing the need for programmers to have in-depth knowledge of the hardware. Consequently, the ability to produce functional and efficient software relies more on logical and conceptual skills than on hardware-specific expertise.\nIn contrast to CPU programming, GPU programming necessitates a more intimate understanding of the underlying hardware to achieve optimal performance. This is because GPUs are designed with a fundamentally different architecture, emphasizing parallel processing capabilities. To leverage these capabilities effectively, programmers need to be aware of concepts such as thread hierarchies, memory coalescing, and warp execution. Unlike CPUs, where compilers can often perform extensive optimizations automatically, GPU programming requires manual optimization to fully exploit the hardware\u0026rsquo;s potential. The programmer must write code that efficiently manages thousands of parallel threads and minimizes memory access latency. Consequently, achieving high performance in GPU programming involves a deep understanding of the GPU\u0026rsquo;s architecture and the intricacies of its operation, making it significantly different from the more abstracted approach possible in CPU programming.\nThe figure below shows a comparison between the CPU and GPU architecture design, which is very helpful in understanding the purpose of both processors.\nFigure 1.1 of the PMPP book The CPU is designed for general-purpose processing and optimized for single-thread performance, enabling it to run a wide range of applications, from operating systems to specialized software. As illustrated in the figure, the CPU features a large cache memory and a few powerful cores. Additionally, it employs a hierarchical memory system, consisting of registers, multiple levels of cache, and main memory, ensuring rapid data access. To enhance its general-purpose processing capabilities, the CPU includes a complex instruction set, allowing it to perform a broad spectrum of operations.\nOn the other hand, the GPU is designed for parallel computing, following the Single Instruction Multiple Data (SIMD) paradigm. The GPU design aims to maximize throughput by utilizing thousands of cores that compute simple operations in parallel. For memory, GPUs use High Bandwidth Memory (HBM) to facilitate very fast data flows in and out of the processor, while maintaining unified memory to enable communication between threads.\nEnough with these generic notions. Put on your gloves, and let\u0026rsquo;s cut open a GPU!\nCerebral Cortex # The figure below illustrates the fundamental design of a modern GPU, primarily consisting of an array of Streaming Multiprocessors (SMs) and Global Memory.\nFigure 4.1 of the PMPP book Each SM contains a grid of CUDA cores, local memory, and a control unit. The distinction between local and global memory is crucial and must be considered when writing kernels, as loading and storing data between SMs and Global Memory can introduce significant overhead.\nBlock Scheduling # Recall that in CUDA we call kernels as follows (example from HelloCuda):\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); This results in the CUDA runtime spawning blocks of threads, which are assigned to individual SMs. In practice, each SM is responsible for executing multiple blocks. However, since each SM has a limited number of CUDA cores and memory, there is a limit to how many blocks can be assigned to it at a given time. Consequently, there is also a limit to how many threads can be executed simultaneously on a CUDA device. To manage this limitation, the CUDA runtime keeps track of all the spawned blocks and assigns new blocks to the SMs as they complete the execution of previous blocks.\nThe effort of assigning entire blocks to a single SM is well worth it. Since threads within the same block often need to communicate with each other, having them share the same SM allows them to use the SM\u0026rsquo;s internal memory. This approach avoids the need for global memory access, which requires more time-consuming load and store operations.\nAnother key advantage of dispatching entire blocks to a single SM is synchronization. Kernel logic often requires synchronization to ensure that all threads reach a certain execution point before continuing with the computation. Even if threads are started simultaneously, their execution times can vary, and perfect timing synchronization is practically impossible. In practice, the threads\u0026rsquo; execution order is random.\nFortunately, the CUDA APIs provide a straightforward way to perform barrier synchronization with a single command:\n__syncthreads() This command causes all threads in the block to wait at that point until every thread has reached the instruction. This ensures that all threads have completed the code preceding the barrier, allowing safe continuation with the subsequent code execution.\nWarps # As mentioned earlier, each SM contains a certain number of CUDA cores. Suppose that each core individually performes an instruction at a given time; in that case, the design would need to provide an instruction fetch/dispatch unit for each core, which would be inefficient given the large number of cores per SM. Additionally, to achieve significant throughput, the device must implement the SIMD paradigm.\nTo address these requirements, each SM is divided into processing blocks, each containing a certain number of cores and a single instruction fetch/dispatch unit to instruct the group of cores. The figure below illustrates an SM equipped with 16 CUDA cores, divided into 2 processing blocks.\nFigure 4.8 of the PMPP book To align the execution of kernels with this architectural implementation, warps were conceived. Whenever a block is assigned to an SM, it is further divided into warps, which are typically 32-thread units for NVIDIA GPUs. Note that the size of warps is implementation-specific and can vary between different architectures.\nA warp is essentially the unit of thread scheduling in SMs. The 32 threads in a warp are contiguous, meaning they have a contiguous range of thread IDs. For example, if a block generates only one warp during execution, that warp will contain thread IDs ranging from 0 to 31.\nWe must consider how thread IDs are assigned within a warp. For a one-dimensional grid, the partitioning is straightforward: the array of threads is simply split into N contiguous parts, each consisting of 32 threads. In the case of a two-dimensional grid, which forms a matrix, the array is first flattened into a one-dimensional array in a row-major order (i.e., first all elements of the first row, then all elements of the second row, and so on) and then split as the one-dimensional case. For three-dimensional grids, we fix the value for the z-axis and then flatten the array as we would for the two-dimensional case. This process is repeated for each value of the z-axis.\nControl Divergence # While the use of processing blocks and warps provides execution optimization, it also introduces a hidden problem.\nAs mentioned earlier, all cores in a processing block share the same fetch/dispatch unit. Consequently, all threads in a warp execute the same instruction at a given time. But what happens with branches?\nConsider a piece of code like:\nif (thread_id%2 == 0) { ... } else { ... } In this case, the execution of operations depends on the thread ID, so not all threads in a warp will be executing the same code simultaneously. Specifically, when threads with even thread IDs are computing the code inside the if brackets, threads with odd thread IDs will be idle, and vice versa.\nThis phenomenon is called control divergence, and it generally occurs whenever there is a control structure in the code that depends on the thread ID.\nControl divergence silently suggests that we should try to avoid such control structures when writing a kernel to prevent code execution from diverging and wasting time on idle threads. However, it\u0026rsquo;s not always possible to avoid these control structures, such as when checking the portion of data on which the thread must perform computations (similar to what we did in HelloCuda).\nHippocampus # When optimizing GPU kernels, it\u0026rsquo;s crucial to remember that performance improvements from processing optimizations are only part of the overall picture. A significant contribution also comes from efficient memory usage and effectively leveraging the GPU memory hierarchy.\nWith that in mind, let\u0026rsquo;s explore a high-level overview of the GPU memory architecture design.\nHierarchical Memory Design # Just as for the CPU design, the GPU architecture also specifies a hierarchical memory design, composed by several levels. As always, the size of the memory is inversely proportional to its speed and cost.\nGlobal Memory is the largest and slowest type of memory on a GPU. If you followed the Hello CUDA tutorial, you have already used it without even knowing. Both the host and the device have read and write access to this memory. The long access latencies and relatively low access bandwidth of the Global Memory derives from the fact that it is not an on-chip memory and it is also implemented using the DRAM technology. Constant Memory is similar to Global Memory but with one key difference: it supports high-bandwidth, read-only access by the device.\nGlobal Memory is also used to create Local Memory, which shares the same properties in terms of latency and access. However, Local Memory is reserved for a single thread, meaning it is not shared among threads. Each thread has its own portion of dedicated Global Memory, typically used to allocate static arrays, spilled registers, and other elements of the thread’s call stack.\nShared Memory is an on-chip memory that allows threads within the same block to share data efficiently. It is much faster than Global Memory and helps minimize latency by reducing the number of accesses to Global Memory. Its primary purpose is to facilitate high-speed data sharing and cooperation among threads.\nRegisters are another type of on-chip memory. They are the fastest memory type and are assigned to individual threads, storing the most frequently accessed data for each thread.\nLast but not least, Texture Memory is a specialized type of read-only memory in GPUs, optimized for two-dimensional spatial locality and specific access patterns commonly found in graphics rendering and image processing tasks. Its design provides significant performance benefits in various computational scenarios, utilizing techniques such as spatial locality and hardware interpolation/filtering. However, we will not delve too deeply into this type of memory here, as it is beyond the scope of this article.\nIt\u0026rsquo;s crucial to remember this hierarchy when writing a kernel, as the speed differences between these memory types are highly noticeable and have a significant impact on the overall performance of the kernel. In modern GPUs, the combined access bandwidth of all the register files across all Streaming Multiprocessors (SMs) is at least two orders of magnitude higher than that of global memory!\nMemory Types in CUDA # Now that we have a clear high-level overview of the different types of device memory, the next logical step is to understand how to utilize them effectively when writing kernels. In CUDA, variables are declared with specific properties that dictate the type of memory they will use. These declarations also define the scope and lifetime of each variable, allowing for precise control over memory usage and performance optimization.\nHere\u0026rsquo;s a quick overview of the syntax and the resulting behaviour.\nDeclaration Memory Type Scope Lifetime Automatic Variables (arrays excluded) Register Thread Grid Automatic Array Variable Local Thread Grid __device__ __shared__ int SharedVariable Shared Block Grid __device__ int GlobalVariable Global Grid Application __device__ __constant__ int ConstantVariable Constant Grid Application Let\u0026rsquo;s explain in detail each row of this table.\nSaying non-array automatic variables is just a fancy way of referring to scalars. Every variable of this type defined in a kernel or device function is automatically placed inside a thread register, meaning each thread will have its own version of the variable. Consequently, their lifespan is limited to the execution time of the thread. You already know that registers are extremely fast, but be careful not to allocate too many scalar variables, as this can exceed the maximum capacity of the register storage. Using too many registers can negatively affect the occupancy of each Streaming Multiprocessor (SM).\nArray variables are typically instead allocated in the Local Memory, however their scope and lifetime are the same of scalar variables.\nUsing the syntax __shared__, we specify that the variable must be shared among threads in the same block. Typically, a shared variable is declared inside the kernel function and used by threads within the same block to cooperate in computation. This is a convenient approach because Shared Memory is very fast.\nBy specifying only __device__, the variable will be allocated in Global Memory, making it a global variable. This means the variable is visible to all threads and persists throughout the entire application execution. Be cautious when dealing with global variables: the only robust way to ensure data consistency and synchronization across different blocks is to use atomic instructions.\nLastly, by using the __constant__ syntax, we can define a constant variable, which will be allocated in Constant Memory. These variables are defined outside any function body, are visible to all threads in the grid, and their lifespan matches that of the application. This type of variable is typically used to provide constant inputs or data to kernels, as the variables\u0026rsquo; values cannot be modified by the kernels.\nConclusion # I hope you enjoyed this dissection! By now, you should have gained a comprehensive high-level understanding of both the Cerebral Cortex and the Hippocampus of a GPU.\nThroughout our exploration, we delved into the intricacies of Streaming Multiprocessors (SMs), warps, control divergence, and various memory types inherent to a GPU\u0026rsquo;s architecture. These components collectively form the brain of a GPU, crucial for its processing power and efficiency.\nIt\u0026rsquo;s important to recognize the profound interconnection between these two brain regions. When developing GPU code, one must consider both the Cerebral Cortex and the Hippocampus to ensure optimal performance and functionality. Neglecting either could lead to inefficiencies or errors in your code—something no true surgeon of GPU programming would permit!\nThat wraps up today\u0026rsquo;s session! Remember to remove your gloves and wash your hands! Clear your mind and prepare for the next fascinating dive into the GPU programming world!\n","date":"14 June 2024","externalUrl":null,"permalink":"/posts/cpu-gpu-architecture/","section":"Posts","summary":"\u003cp\u003eWould you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function. Knowing the philosophy behind their architectural design, the problems they aim to solve, and the reasons for their specific construction is essential for leveraging their full potential.\u003c/p\u003e","title":"Cerebral Cortex and Hippocampus: Understanding the Computational and Memory Design of GPUs","type":"posts"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/basics/","section":"Tags","summary":"","title":"Basics","type":"tags"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/gpu-programming/","section":"Tags","summary":"","title":"Gpu-Programming","type":"tags"},{"content":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\nWith the introduction of CUDA, everything changed. Its integration with languages like C, C++, or Python, along with the elimination of the need for knowledge in graphics programming, made GPU programming much more accessible.\nHowever, many individuals today still feel daunted by the complexity of GPU programming, both in terms of the language (which is too low-level compared to the most popular programming languages today) and the architecture and development knowledge required to write this kind of software.\nThis article aims to introduce you to this realm through an in-depth technical analysis of a HelloWorld, which is famously the starting point for every programmer who ventures into a new programming language. Just as a surgeon carefully dissects to reveal the inner workings of the human body, we\u0026rsquo;ll break down each component of CUDA\u0026rsquo;s basic architecture and functionality.\nSince we\u0026rsquo;re discussing GPU programming, the classic print(\u0026quot;hello world\u0026quot;) won\u0026rsquo;t be applicable. Instead, we\u0026rsquo;ll analyze a complete program that adds two vectors, using the GPU.\nPrerequisites # I will briefly present some prerequisites necessary to fully understand the remaining content of this post. Even if you don\u0026rsquo;t meet all of them, I still suggest you keep reading. I\u0026rsquo;m confident you can still enjoy the content and find it useful.\nBasic C/C++ knowledge. Given that CUDA is originally a superset of C/C++, it comes natural that you have to know the basics of these languages. Basic knowledge of a GPU architecture. I won\u0026rsquo;t examine deeply this topic here, but understanding how a GPU works and its computational paradigm is essential for grasping CUDA programming. Installation of CUDA on your machine to try out the code yourself. That\u0026rsquo;s about it.\nCode Dissection # In the link below, you will find the full code snippet for our HelloWorld program. I believe that reading the entire code first piques your interest, as you will likely have questions you want answered.\nIf this code seems intimidating to you, please don\u0026rsquo;t be afraid. I assure you that by the end of your reading, you will fully understand it and realize its simplicity.\nHere\u0026rsquo;s the code We are now ready to dissect this code, top to bottom.\nCUDA Error Handling # The code example begins with the definition of a macro, which will be utilized throughout the program.\n#define CUDA_CHECK_ERROR(err) \\ if (err != cudaSuccess) { \\ printf(\u0026#34;CUDA err: %s at line %d\\n\u0026#34;, cudaGetErrorString(err), __LINE__); \\ exit(EXIT_FAILURE); \\ } Within this macro, we utilize several CUDA-defined functions and symbols, such as cudaSuccess and cudaGetErrorString(cudaError_t). While these are fairly self-explanatory, let\u0026rsquo;s delve deeper into their significance.\nWe consider the err variable a cudaError_t type variable, intended to store errors encountered during execution. Within the macro, we check if the error status is cudaSuccess, indicating successful code execution. If this condition is not met, we invoke the cudaGetErrorString(cudaError_t) function to retrieve the error string, providing clarity on the encountered issue, and then we exit the code execution.\nWhile this is obviously the most basic way of doing error handling, it\u0026rsquo;s crucial to remember its necessity whenever utilizing CUDA APIs, as errors may arise during their invocation.\nThe Kernel # Now, we\u0026rsquo;ll delve into what is arguably the most intriguing segment of the code, which will unveil some pivotal technical insights into CUDA programming and hardware.\nTo begin, it\u0026rsquo;s essential to understand that the term \u0026ldquo;kernel\u0026rdquo; typically denotes functions that can be computed by the device. In this context, we\u0026rsquo;re referring to the code responsible for orchestrating the GPU\u0026rsquo;s addition of the two input vectors.\nLet\u0026rsquo;s now examine the function\u0026rsquo;s prototype.\n__global__ void vecAddKernel(float* A, float* B, float* C, int n) If you are familiar with the C programming language, you may have noticed that the keyword __global__ doesn\u0026rsquo;t come from the standard. It is in fact a keyword implemented by the CUDA programming language, which specifies the visibility of the function.\nFunction Execution Space # In the GPU programming paradigm, we distinguish between two entities: the host and the device. The host refers to the CPU along with its memory, while the device pertains to the GPU. Consequently, the CUDA programming language incorporates three function execution specifiers:\n__global__ denotes a kernel. The function is executed on the device, and is callable both by the host and the device (only for devices of compute capability 5.0 or higher). It\u0026rsquo;s crucial to note that functions of this type must return void. __device__ signifies code executed exclusively on the device. It can only be called by another device execution and not from the host. __host__ denotes code that can be called and executed solely by the host. This is the default execution space if no execution space is specified. Now that we understand what a function execution space entails, let\u0026rsquo;s briefly delve into the remainder of the prototype. As mentioned earlier, given that this function serves as a kernel, it must inherently return void. Consequently, we\u0026rsquo;ll need both the input (A, B) and output (C) vectors, as well as explicitly specify the length of the vectors (n).\nBlocks and threads # The subsequent lines of code may appear cryptic at first glance, as they encompasses several complexities. To unravel their intricacies, let\u0026rsquo;s begin with the fundamentals.\nint i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; n) { C[i] = A[i] + B[i]; } This article doesn\u0026rsquo;t delve deeply into GPU architecture, so I won\u0026rsquo;t discuss its overall structure here. However, it\u0026rsquo;s crucial to understand that GPUs operate on the SIMD (Single Instruction Multiple Data) paradigm. This allows GPUs to process multiple data in parallel, executing a single instruction.\nIn essence, the heart of GPU processing lies in threads. The kernel we\u0026rsquo;re defining will be executed by the device, which will use a certain number of threads to execute the kernel instructions.\nLet\u0026rsquo;s illustrate this with a practical example using the code snippet above. Suppose the length of both vectors is 100. We\u0026rsquo;ll write code to execute the kernel, allocating 100 threads. Each thread will be responsible for summing a specific position in the vectors.\nIn the code snippet, the position is indicated by the variable i. It\u0026rsquo;s crucial for each thread to have a distinct i value; otherwise, all threads would operate on the same position in the vectors. Fortunately, the CUDA API provides built-in thread-specific variables that help us compute the correct positional index. Specifically, we\u0026rsquo;re utilizing the following built-ins:\nthreadIdx: Represents the index of the thread currently executing the code. blockIdx: Denotes the index of the block containing the executing thread. blockDim: Specifies the size of each spawned block. A block is a group of threads that collaborate and communicate through shared memory. These blocks are scheduled and executed together by the hardware. Moreover, all blocks together form what is termed as a grid. This implies that the structure is inherently multi-dimensional: a grid has three dimensions—x, y, and z. Hence, when referencing the built-in variables threadIdx, blockIdx, and blockDim, we utilize their x attribute, which corresponds to the x-dimension of the grid. The rationale behind this will become clearer when we examine the kernel invocation.\nThe final piece to explain is the if statement preceding the actual sum computation. This if statement verifies that the computed index falls within the bounds of the input vectors. While this check may initially seem unnecessary, its significance will become apparent later on.\nHost code # Following the discussion on the kernel, let\u0026rsquo;s shift our attention to the host code of the program. Here, we\u0026rsquo;re tasked with memory allocation, kernel invocation, and error checking.\nDevice Memory # A crucial consideration is that device memory is distinct from host memory. Consequently, the device cannot directly operate on data residing in host memory. This necessitates the allocation of device memory and the transfer of data from the host to the device. While this may appear trivial, it\u0026rsquo;s one of the most significant bottlenecks in modern software, particularly when handling large amounts of data, leading to substantial communication overhead between the host and the device. Given this distinction, as a best practice, we use the notation V_h to indicate a variable used by the host and V_d to denote a variable used by the device.\nGiven the simplicity of our case study, we\u0026rsquo;ll overlook these technical intricacies and focus on accomplishing the bare minimum to ensure functionality.\nvoid vecAdd(float* A_h, float* B_h, float* C_h, int n) The vecAdd function accepts three host-vectors as arguments: A_h and B_h, which are to be summed, and C_h, which will store the result. It\u0026rsquo;s important to note that an output vector C is necessary since the kernel cannot directly return anything. Finally, n represents the length of the input vectors.\nOur initial step is to determine the size of the device-vectors. This computation follows a standard C approach:\nint size = n * sizeof(float) Next, we proceed to allocate memory on the device to store the device-vectors:\nfloat *A_d, *B_d, *C_d; cudaMalloc((void**)\u0026amp;A_d, size); cudaMalloc((void**)\u0026amp;B_d, size); cudaMalloc((void**)\u0026amp;C_d, size); Fortunately, the CUDA programming language provides us with the cudaMalloc function, which functions similarly to the malloc function but operates on the device.\nBefore invoking the kernel, the final step is to transfer data from the host to the device. At this stage, we have:\nOn the host, three vectors containing the data for computation On the device, three memory allocations initialized but lacking of the necessary data. To address this, we leverage another CUDA programming function, cudaMemcpy:\ncudaError_t error = cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); error = cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); We invoke the function with the following parameters: the destination pointer, the source pointer, the size to copy, and the direction. The parameter cudaMemcpyHostToDevice specifies that we intend to copy data from the host to the device. Additionally, observe how we utilize the previously defined macro to check for errors in case the memory copying operation fails.\nKernel Invocation # Assuming everything executes correctly, we\u0026rsquo;ve successfully copied the input vectors to the device memory. Hence, we can proceed to call the kernel.\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); First, we define the number of threads we want to allocate per block. Next, we define two dim3 type variables, which represent 3-dimensional vectors typically used to specify grid and block sizes. Since we are dealing with a 1-dimensional problem, we only need to use the x-dimension.\nWe compute the grid size as the number of elements in the input vectors, n, divided by the number of threads per block. To ensure the grid dimension can accommodate all threads, even if n is not perfectly divisible by the number of threads per block, we use the ceil operator, which will round-up the division. The block dimension is simply set to the number of threads we want to allocate per block.\nLastly, we invoke the kernel, as specified in the previous sections.\nNotice two things:\nWe can now fully understand why we need to use an if statement in the kernel, as explained in the Blocks and threads. Let\u0026rsquo;s illustrate with a practical example. Suppose n = 100 and number_of_threads = 16. When computing the x-dimension of the grid size, the exact result is 6.25. However, if we set the grid x-size to 6, we won\u0026rsquo;t allocate enough threads because we\u0026rsquo;ll have 6 blocks of 16 elements each, totaling 96 threads, leaving out 4 positions of the vectors. On the other hand, if we set the grid size to 7, we\u0026rsquo;ll allocate more threads than the number of positions to compute because 7x16=112. We can\u0026rsquo;t allow the extra threads to access memory indicated by their int i = threadIdx.x + blockIdx.x * blockDim.x; because it would be out of bounds. Therefore, the necessity of using an if statement to check such boundaries.\nOften, tutorials do not explicitly allocate the dimGrid and dimBlock variables, but instead invoke the kernel directly like this: vecAddKernel\u0026lt;\u0026lt;\u0026lt;ceil(n/number_of_threads), number_of_threads\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n). This syntax is valid because the dim3 data type automatically defaults unspecified dimensions to 1. However, I chose to specify all dimensions to clarify the explanation provided in the section Blocks and threads.\nStoring and cleaning up # After the kernel invocation, we need to retrieve the computation result. Since the kernel cannot return anything, it\u0026rsquo;s the programmer\u0026rsquo;s responsibility to fetch the output of the computation. We can easily accomplish this as follows:\nerror = cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost); CUDA_CHECK_ERROR(error); cudaFree(A_d); cudaFree(B_d); cudaFree(C_d); Once again, we utilize the cudaMemcpy function, but this time we specify cudaMemcpyDeviceToHost, indicating that we are transferring data from the device to the host. Notice also how the order of the host and device variables has switched in the function call compared to previous occurrences.\nFinally, we must free the memory that was used. This isn\u0026rsquo;t just a best practice—it\u0026rsquo;s an absolute necessity!\nMain # Just for completeness, let\u0026rsquo;s quickly go through the main function:\nint n = 512; float A_h[n], B_h[n], C_h[n]; for (int i = 0; i \u0026lt; n; i++) { A_h[i] = i; B_h[i] = i; } vecAdd(A_h, B_h, C_h, n); for (int i = 0; i \u0026lt; n; i++) { printf(\u0026#34;%g\\n\u0026#34;, C_h[i]); } return 0; There isn\u0026rsquo;t much to say here. We define the length of the vectors n, then allocate and initialize the input vectors A_h and B_h with a range from 0 to n-1. Next, we call the host code, which also invokes the kernel, and finally, we print the result stored in C_h.\nCompile and run # Compiling and running this example is straightforward. First, we compile the source as we would for a C file, but we use the CUDA compiler, called nvcc. Assuming the source is named hello_world.cu and we are in its parent directory, we can build the executable like this:\nnvcc hello_world.cu -o hello_world Then, we simply run the executable:\n./hello_world And we will receive the output (truncated for readability):\n0 2 4 ... 1018 1020 1022 Wrapping Up # In summary, we\u0026rsquo;ve explored the fundamentals of CUDA programming, which enables us to harness the power of GPUs for parallel computing tasks. We began by understanding the distinction between the host (CPU) and the device (GPU) and delved into CUDA\u0026rsquo;s function execution specifiers (__global__, __device__, __host__) that define where functions can be executed.\nWe discussed the importance of memory management, highlighting how data must be transferred between host and device memories using functions like cudaMalloc and cudaMemcpy. We also saw the significance of error handling throughout the CUDA programming process, utilizing macros to check for errors and ensure smooth execution.\nThe heart of CUDA programming lies in kernels, functions executed on the GPU, typically invoked in a grid of threads. We learned about grid and block dimensions, and the necessity of boundary checks within kernels to avoid out-of-bounds memory access.\nWe then examined a practical example of vector addition, illustrating how to define and invoke kernels, manage memory, and handle errors.\nLastly, we discussed compiling and running CUDA programs using the nvcc compiler and executing the resulting executables.\nOverall, CUDA provides a powerful framework for parallel programming on GPUs, offering immense computational capabilities for a wide range of applications. With a solid understanding of its principles and techniques, developers can unlock the full potential of GPU-accelerated computing.\nTime to close the operating theater! You\u0026rsquo;ve witnessed the basic intricacies of CUDA programming under the scalpel today. Keep honing those skills, and you\u0026rsquo;ll soon be performing GPU surgeries with finesse. Until our next exploration, stay sharp and happy coding! 🤖\n","date":"6 May 2024","externalUrl":null,"permalink":"/posts/hello-cuda/","section":"Posts","summary":"\u003cp\u003eIn case you didn\u0026rsquo;t already know, \u003ca href=\"https://it.wikipedia.org/wiki/CUDA\" target=\"_blank\"\u003eCUDA\u003c/a\u003e is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\u003c/p\u003e","title":"Hello CUDA: A Surgical Dissection","type":"posts"},{"content":" NOTE: This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\nLarge Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\nWhen I first started learning about neural networks -about 5 years ago-, one of the key questions I had was if it would be possible to know, a priori, the minimum number of parameters that a network must implement to achieve a certain metric value when solving a specific problem. Unfortunately, as far as I know, there is no such theorem. Surely, we know about convergence theorems -like the Universal Approximation Theorem-, but nothing tells us the optimal number of parameters, given an architecture and a problem to solve.\nA very interesting methodology is the Cascade Correlation, which is a constructive approach for obtaining a network that solves a specific problem. However, this methodology is impractical in modern times: the size of neural networks is just too big, especially when talking about LLMs, which typically span from 7 to 70 billion parameters.\nTherefore, a follow-up question: given a trained, large network, can we reduce its size while maintaining the same accuracy?\nThat’s the key idea behind Pruning.\nPruning for Sparsity # Let’s start from a biological point of view: humans drastically reduce the number of synapses per neuron from early childhood to adolescence. This has been shown in various papers (e.g. Synapse elimination accompanies functional plasticity in hippocampal neurons) and it is a way for the brain to optimize its knowledge and remove redundancy. As often happens, this concept can also be applied to learning architectures.\nAssume to have a relatively large neural network that has been trained to approximate the solving function for a given problem, which was described by a dataset. This means that, by training, we minimized the loss function defined as:\n\\(\\mathrm{L}(\\theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{(x,y)\\in(X,Y)}{L(\\theta, x, y)}\\)\nwhere \\(\\theta\\) represents the weights of the network and \\(\\mathcal{D}\\) symbolizes the data used to train it.\nLet’s now assume that we are capable of pruning the weights \\(\\theta\\), therefore obtaining \\(\\theta_{pruned}\\). Our objective is to get approximately the same loss value when using the pruned weights. In math:\nThe advantage here is obvious: \\(\\theta_{pruned}\\) is a smaller, more efficient model that is capable of replicating the accuracy of a bigger, slower model.\nTake a look at the picture below: by pruning the red neuron, we remove 8 connections and the activation of the neuron itself. If both the networks had the same accuracy, we would prefer the right one, because its inference is more efficient.\ncredits to Nvidia blog Choosing what to prune # The pruning criterion is a very important factor to take into account, and research is still very active in this specific field. The common idea that unites all the pruning criteria is to prune parameters that are less useful in solving the problem. Therefore, we need a metric that describes the importance of a neuron in the overall network.\nIn this post, I’ll only talk about the Magnitude-based criterion, but I encourage interested readers to research other methodologies, e.g. Scaling-based pruning, Regression-based pruning, and Percentage-of-zero-based pruning.\nThe Magnitude-based criterion selects parameters to prune based on, guess what, their magnitude, therefore using an L_p norm. For example, we could choose to use an L_1 norm (a.k.a. the absolute value) as the importance metric for the parameter:\n\\(importance(\\theta_{i}) = ||\\theta_{i}||\\)\nThe rationale behind this is that the smaller the parameter, the less its influence is in the activation of the neuron.\nSo, if you assume to have a neural network with 100 parameters, and you want to prune 80% of them, you should first define an importance metric, then compute it for each parameter, sort the result, and pick the top-k parameters for that importance.\nPruning Granularity # Another key concept for pruning is granularity. We can proceed either by selecting parameters in an unstructured way or using a pattern. Let’s just see two examples to grasp this concept.\nAssume to have a matrix that represents the parameters we want to prune:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n} \\)\nand also assume that we picked the L_2norm as the importance metric for the Magnitude-based criterion.\nThen, we can perform either:\nunstructured pruning: we iterate over each parameter, compute its L_2 norm, and prune it if it’s not in the top-k percentage of neurons per importance.\nstructured pruning: instead of computing the L_2 norm for the single parameter, we first select a pattern. Let’s keep things simple and assume that our pattern corresponds to the row of the matrix P. Therefore, we will not prune the single parameter, but instead, we will prune the rows that do not belong to the top-k percentage of the highest-importance rows in the matrix.\nThe picture below depicts the difference between the two approaches.\ncredits to efficientml.ai This picture also foretells the advantages of pattern-based pruning, which I will explain in a later section.\nPruning Ratio # The pruning ratio is the last key concept we need to understand. Luckily for us, it’s quite straightforward. The ratio indicates how much sparsity we want in our neural network.\nFor example, if we pick a uniform pruning ratio of 80%, we will remove 80% of the parameters of the network, which will be selected based on the criterion and importance measure, defined before.\nThe word “uniform” is not randomly put. In fact, we can also provide a different pruning ratio for different levels of the network architecture. For example, we could select different pruning ratios for different layers of the architecture, or different channels.\nThere are several methodologies to find the optimal pruning ratio, but I will not cover them in this introduction.\nDoes pruning work? # Yes, and incredibly, if done correctly.\nThe first thing to notice is that we should always retrain the network after pruning. This is done because, when pruning aggressively (typically \u0026gt; 90%), the accuracy of the network drops significantly. Performing a fine-tuning step is crucial to retain information in the parameters that survived the pruning phase, and therefore stabilize the accuracy of the model.\nThis excellent picture from efficientml.ai very clearly depicts this concept\ncredits to efficientml.ai Computations with sparsity # When talking about the pruning granularity, we saw the conceptual difference between unstructured and structured pruning. Let’s consider again the matrix:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n}\\)\nLet’s now assume another matrix, which will be multiplied with matrix \\(\\mathcal{P}\\):\n\\(\\mathsf{L} \\in \\mathcal{R}^{n,l}\\)\nComputing \\(\\mathsf{P}\\) x \\(\\mathsf{L}\\) in the most straightforward way possible, we perform a number of MACs equal to:\n\\(MACs = m \\times n \\times l\\)\nIf we manage to reduce the number of rows of \\(\\mathsf{P}\\) -therefore reducing \\(m\\)-, it’s easy to see how the number of computations is reduced, as the size of the matrix \\(\\mathsf{P}\\) itself.\nIn the unstructured case, instead, we did not shrink the dimension of the \\(\\mathsf{P}\\) matrix, but we just made it sparse, by setting some elements to 0. How are we getting a benefit from that?\nThat’s the catch: we don’t. If we don’t have the availability of an efficient system for sparsity, we will not get any benefit from having a sparse \\(\\mathsf{P}\\) matrix. This applies to both the computational and memory point of view.\nThe topic of efficient systems for sparsity computations is just too vast to be discussed here. Let me know if you are interested in the topic and would like to read a post that delves into it!\nConclusions # In this brief introduction to pruning for efficient neural networks’ inference, we learned about the three key concepts for this methodology: selecting the parameters to prune, the pruning granularity, and the pruning ratio.\nWe also saw how much benefit can be obtained by pruning a model, both in terms of computational and memory complexity.\nThere’s still much to say about this technique, so the learning doesn’t stop here! I hope this post was a good introduction, and that it conveyed well to you these basic concepts.\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/pruning-intro/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNOTE:\u003c/em\u003e\u003c/strong\u003e  This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eLarge Language Model\u003c/strong\u003e. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\u003c/p\u003e","title":"An Introduction to Sparsity for Efficient Neural Network Inference","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/inference/","section":"Tags","summary":"","title":"Inference","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/pruning/","section":"Tags","summary":"","title":"Pruning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]