
[{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"Cuda","type":"tags"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/gpu-programming/","section":"Tags","summary":"","title":"Gpu-Programming","type":"tags"},{"content":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\nWith the introduction of CUDA, everything changed. Its integration with languages like C, C++, or Python, along with the elimination of the need for knowledge in graphics programming, made GPU programming much more accessible.\nHowever, many individuals today still feel daunted by the complexity of GPU programming, both in terms of the language (which is too low-level compared to the most popular programming languages today) and the architecture and development knowledge required to write this kind of software.\nThis article aims to introduce you to this realm through an in-depth technical analysis of a HelloWorld, which is famously the starting point for every programmer who ventures into a new programming language.\nSince we\u0026rsquo;re discussing GPU programming, the classic print(\u0026quot;hello world\u0026quot;) won\u0026rsquo;t be applicable. Instead, we\u0026rsquo;ll analyze a complete program that adds two vectors, using the GPU.\nPrerequisites # I will briefly present some prerequisites necessary to fully understand the remaining content of this post. Even if you don\u0026rsquo;t meet all of them, I still suggest you keep reading. I\u0026rsquo;m confident you can still enjoy the content and find it useful.\nBasic C/C++ knowledge. Given that CUDA is originally a superset of C/C++, it comes natural that you have to know the basics of these languages. Basic knowledge of a GPU architecture. I won\u0026rsquo;t examine deeply this topic here, but understanding how a GPU works and its computational paradigm is essential for grasping CUDA programming. Installation of CUDA on your machine to try out the code yourself. That\u0026rsquo;s about it.\nCode Analysis # In the link below, you will find the full code snippet for our HelloWorld program. I believe that reading the entire code first piques your interest, as you will likely have questions you want answered.\nIf this code seems intimidating to you, please don\u0026rsquo;t be afraid. I assure you that by the end of your reading, you will fully understand it and realize its simplicity.\nTODO: inserisci link a Github gist We are now ready to analyze this code, top to bottom.\nCUDA Error Handling # The code example begins with the definition of a macro, which will be utilized throughout the program.\n#define CUDA_CHECK_ERROR(err) \\ if (err != cudaSuccess) { \\ printf(\u0026#34;CUDA err: %s at line %d\\n\u0026#34;, cudaGetErrorString(err), __LINE__); \\ exit(EXIT_FAILURE); \\ } Within this macro, we utilize several CUDA-defined functions and symbols, such as cudaSuccess and cudaGetErrorString(cudaError_t). While these are fairly self-explanatory, let\u0026rsquo;s delve deeper into their significance.\nWe consider the err variable a cudaError_t type variable, intended to store errors encountered during execution. Within the macro, we check if the error status is cudaSuccess, indicating successful code execution. If this condition is not met, we invoke the cudaGetErrorString(cudaError_t) function to retrieve the error string, providing clarity on the encountered issue, and then we exit the code execution.\nWhile this is obviously the most basic way of doing error handling, it\u0026rsquo;s crucial to remember to remember its necessity whenever utilizing CUDA APIs, as errors may arise during their invocation.\nThe Kernel # Now, we\u0026rsquo;ll delve into what is arguably the most intriguing segment of the code, which will unveil some pivotal technical insights into CUDA programming and hardware.\nTo begin, it\u0026rsquo;s essential to understand that the term \u0026ldquo;kernel\u0026rdquo; typically denotes functions that can be computed by the device. In this context, we\u0026rsquo;re referring to the code responsible for orchestrating the GPU\u0026rsquo;s addition of the two input vectors.\nLet\u0026rsquo;s now examine the function\u0026rsquo;s prototype.\n__global__ void vecAddKernel(float* A, float* B, float* C, int n) If you are familiar with the C programming language, you may have noticed that the keyword __global__ doesn\u0026rsquo;t come from the standard. It is in fact a keyword implemented by the CUDA programming language, which specifies the visibility of the function.\nFunction Execution Space # In the GPU programming paradigm, we distinguish between two entities: the host and the device. The host refers to the CPU along with its memory, while the device pertains to the GPU. Consequently, the CUDA programming language incorporates three function execution specifiers:\n__global__ denotes a kernel. The function is executed on the device, and is callable both by the host and the device (only for devices of compute capability 5.0 or higher). It\u0026rsquo;s crucial to note that functions of this type must return void. __device__ signifies code executed exclusively on the device. It can only be called by another device execution and not from the host. __host__ denotes code that can be called and executed solely by the host. This is the default execution space if no execution space is specified. Now that we understand what a function execution space entails, let\u0026rsquo;s briefly delve into the remainder of the prototype. As mentioned earlier, given that this function serves as a kernel, it must inherently return void. Consequently, we\u0026rsquo;ll need both the input and output vectors, as well as explicitly specify the length of the vectors.\nBlocks and threads # The subsequent lines of code may appear cryptic at first glance, as it encompasses several complexities. To unravel its intricacies, let\u0026rsquo;s begin with the fundamentals.\nint i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; n) { C[i] = A[i] + B[i]; } This article doesn\u0026rsquo;t delve deeply into GPU architecture, so I won\u0026rsquo;t discuss its overall structure here. However, it\u0026rsquo;s crucial to understand that GPUs operate on the SIMD (Single Instruction Multiple Data) paradigm. This allows GPUs to process multiple data in parallel, executing a single instruction.\nIn essence, the heart of GPU processing lies in threads. The kernel we\u0026rsquo;re defining will be executed by the device, which will use a certain number of threads to execute the kernel instructions.\nLet\u0026rsquo;s illustrate this with a practical example using the code snippet above. Suppose the length of both vectors is 100. We\u0026rsquo;ll write code to execute the kernel, allocating 100 threads. Each thread will be responsible for summing a specific position in the vectors.\nIn the code snippet, the position is indicated by the variable i. It\u0026rsquo;s crucial for each thread to have a distinct i value; otherwise, all threads would operate on the same position in the vectors. Fortunately, the CUDA API provides built-in thread-specific variables that help us compute the correct positional index. Specifically, we\u0026rsquo;re utilizing the following built-ins:\nthreadIdx: Represents the index of the thread currently executing the code. blockIdx: Denotes the index of the block containing the executing thread. blockDim: Specifies the size of each spawned block. ","date":"6 May 2024","externalUrl":null,"permalink":"/posts/hello-cuda/","section":"Posts","summary":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.","title":"Hello CUDA: the righteous way","type":"posts"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/","section":"NeuronBit","summary":"","title":"NeuronBit","type":"page"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Large Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\nWhen I first started learning about neural networks -about 5 years ago-, one of the key questions I had was if it would be possible to know, a priori, the minimum number of parameters that a network must implement to achieve a certain metric value when solving a specific problem. Unfortunately, as far as I know, there is no such theorem. Surely, we know about convergence theorems -like the Universal Approximation Theorem-, but nothing tells us the optimal number of parameters, given an architecture and a problem to solve.\nA very interesting methodology is the Cascade Correlation, which is a constructive approach for obtaining a network that solves a specific problem. However, this methodology is impractical in modern times: the size of neural networks is just too big, especially when talking about LLMs, which typically span from 7 to 70 billion parameters.\nTherefore, a follow-up question: given a trained, large network, can we reduce its size while maintaining the same accuracy?\nThat’s the key idea behind Pruning.\nPruning for Sparsity # Let’s start from a biological point of view: humans drastically reduce the number of synapses per neuron from early childhood to adolescence. This has been shown in various papers (e.g. Synapse elimination accompanies functional plasticity in hippocampal neurons) and it is a way for the brain to optimize its knowledge and remove redundancy. As often happens, this concept can also be applied to learning architectures.\nAssume to have a relatively large neural network that has been trained to approximate the solving function for a given problem, which was described by a dataset. This means that, by training, we minimized the loss function defined as:\n\\(\\mathrm{L}(\\theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{(x,y)\\in(X,Y)}{L(\\theta, x, y)}\\)\nwhere \\(\\theta\\) represents the weights of the network and \\(\\mathcal{D}\\) symbolizes the data used to train it.\nLet’s now assume that we are capable of pruning the weights \\(\\theta\\), therefore obtaining \\(\\theta_{pruned}\\). Our objective is to get approximately the same loss value when using the pruned weights. In math:\nThe advantage here is obvious: \\(\\theta_{pruned}\\) is a smaller, more efficient model that is capable of replicating the accuracy of a bigger, slower model.\nTake a look at the picture below: by pruning the red neuron, we remove 8 connections and the activation of the neuron itself. If both the networks had the same accuracy, we would prefer the right one, because its inference is more efficient.\ncredits to Nvidia blog Choosing what to prune # The pruning criterion is a very important factor to take into account, and research is still very active in this specific field. The common idea that unites all the pruning criteria is to prune parameters that are less useful in solving the problem. Therefore, we need a metric that describes the importance of a neuron in the overall network.\nIn this post, I’ll only talk about the Magnitude-based criterion, but I encourage interested readers to research other methodologies, e.g. Scaling-based pruning, Regression-based pruning, and Percentage-of-zero-based pruning.\nThe Magnitude-based criterion selects parameters to prune based on, guess what, their magnitude, therefore using an L_p norm. For example, we could choose to use an L_1 norm (a.k.a. the absolute value) as the importance metric for the parameter:\n\\(importance(\\theta_{i}) = ||\\theta_{i}||\\)\nThe rationale behind this is that the smaller the parameter, the less its influence is in the activation of the neuron.\nSo, if you assume to have a neural network with 100 parameters, and you want to prune 80% of them, you should first define an importance metric, then compute it for each parameter, sort the result, and pick the top-k parameters for that importance.\nPruning Granularity # Another key concept for pruning is granularity. We can proceed either by selecting parameters in an unstructured way or using a pattern. Let’s just see two examples to grasp this concept.\nAssume to have a matrix that represents the parameters we want to prune:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n} \\)\nand also assume that we picked the L_2norm as the importance metric for the Magnitude-based criterion.\nThen, we can perform either:\nunstructured pruning: we iterate over each parameter, compute its L_2 norm, and prune it if it’s not in the top-k percentage of neurons per importance.\nstructured pruning: instead of computing the L_2 norm for the single parameter, we first select a pattern. Let’s keep things simple and assume that our pattern corresponds to the row of the matrix P. Therefore, we will not prune the single parameter, but instead, we will prune the rows that do not belong to the top-k percentage of the highest-importance rows in the matrix.\nThe picture below depicts the difference between the two approaches.\ncredits to efficientml.ai This picture also foretells the advantages of pattern-based pruning, which I will explain in a later section.\nPruning Ratio # The pruning ratio is the last key concept we need to understand. Luckily for us, it’s quite straightforward. The ratio indicates how much sparsity we want in our neural network.\nFor example, if we pick a uniform pruning ratio of 80%, we will remove 80% of the parameters of the network, which will be selected based on the criterion and importance measure, defined before.\nThe word “uniform” is not randomly put. In fact, we can also provide a different pruning ratio for different levels of the network architecture. For example, we could select different pruning ratios for different layers of the architecture, or different channels.\nThere are several methodologies to find the optimal pruning ratio, but I will not cover them in this introduction.\nDoes pruning work? # Yes, and incredibly, if done correctly.\nThe first thing to notice is that we should always retrain the network after pruning. This is done because, when pruning aggressively (typically \u0026gt; 90%), the accuracy of the network drops significantly. Performing a fine-tuning step is crucial to retain information in the parameters that survived the pruning phase, and therefore stabilize the accuracy of the model.\nThis excellent picture from efficientml.ai very clearly depicts this concept\ncredits to efficientml.ai Computations with sparsity # When talking about the pruning granularity, we saw the conceptual difference between unstructured and structured pruning. Let’s consider again the matrix:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n}\\)\nLet’s now assume another matrix, which will be multiplied with matrix \\(\\mathcal{P}\\):\n\\(\\mathsf{L} \\in \\mathcal{R}^{n,l}\\)\nComputing \\(\\mathsf{P}\\) x \\(\\mathsf{L}\\) in the most straightforward way possible, we perform a number of MACs equal to:\n\\(MACs = m \\times n \\times l\\)\nIf we manage to reduce the number of rows of \\(\\mathsf{P}\\) -therefore reducing \\(m\\)-, it’s easy to see how the number of computations is reduced, as the size of the matrix \\(\\mathsf{P}\\) itself.\nIn the unstructured case, instead, we did not shrink the dimension of the \\(\\mathsf{P}\\) matrix, but we just made it sparse, by setting some elements to 0. How are we getting a benefit from that?\nThat’s the catch: we don’t. If we don’t have the availability of an efficient system for sparsity, we will not get any benefit from having a sparse \\(\\mathsf{P}\\) matrix. This applies to both the computational and memory point of view.\nThe topic of efficient systems for sparsity computations is just too vast to be discussed here. Let me know if you are interested in the topic and would like to read a post that delves into it!\nConclusions # In this brief introduction to pruning for efficient neural networks’ inference, we learned about the three key concepts for this methodology: selecting the parameters to prune, the pruning granularity, and the pruning ratio.\nWe also saw how much benefit can be obtained by pruning a model, both in terms of computational and memory complexity.\nThere’s still much to say about this technique, so the learning doesn’t stop here! I hope this post was a good introduction, and that it conveyed well to you these basic concepts.\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/pruning-intro/","section":"Posts","summary":"Large Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models.","title":"An Introduction to Sparsity for Efficient Neural Network Inference","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/inference/","section":"Tags","summary":"","title":"Inference","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/pruning/","section":"Tags","summary":"","title":"Pruning","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]