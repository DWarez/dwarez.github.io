<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Inference on The ML Surgeon</title><link>https://dwarez.github.io/tags/inference/</link><description>Recent content in Inference on The ML Surgeon</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024</copyright><lastBuildDate>Sun, 05 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dwarez.github.io/tags/inference/index.xml" rel="self" type="application/rss+xml"/><item><title>An Introduction to Sparsity for Efficient Neural Network Inference</title><link>https://dwarez.github.io/posts/pruning-intro/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://dwarez.github.io/posts/pruning-intro/</guid><description>&lt;blockquote>
&lt;p>&lt;strong>&lt;em>NOTE:&lt;/em>&lt;/strong> This post was written before the Machine Learning Surgeon got in charge of the blog, that&amp;rsquo;s why there are no references to surgical operations!&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Large Language Model&lt;/strong>. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.&lt;/p></description></item></channel></rss>