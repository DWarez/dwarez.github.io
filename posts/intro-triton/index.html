<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>From Sequential to Parallel: Your Journey into GPU Programming with Triton &#183; The ML Surgeon</title><meta name=title content="From Sequential to Parallel: Your Journey into GPU Programming with Triton &#183; The ML Surgeon"><meta name=description content="The low level side of machine learning."><meta name=keywords content="gpu-programming,triton,basics,"><link rel=canonical href=https://dwarez.github.io/posts/intro-triton/><link type=text/css rel=stylesheet href=/css/main.bundle.min.5a22c02f652c059fb591ca9b98ce644b93828aaf3b59b8aba7547920d2fbb522ad160a339e241cdacbc5919583541aa8e93c346b97f21e1b15c8426f916128f6.css integrity="sha512-WiLAL2UsBZ+1kcqbmM5kS5OCiq87Wbirp1R5INL7tSKtFgozniQc2svFkZWDVBqo6Tw0a5fyHhsVyEJvkWEo9g=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.f7e04724dda02f8ffdc6b4507869a4010cd1ae9151a97168d5fd3112b9825c5c6ac376f7d11b0bc353a66d91d614f06e749efb7e56da4ee6fa0c900ed45c85f7.js integrity="sha512-9+BHJN2gL4/9xrRQeGmkAQzRrpFRqXFo1f0xErmCXFxqw3b30RsLw1OmbZHWFPBudJ77flbaTub6DJAO1FyF9w==" data-copy=Copy data-copied=Copied></script><script src=/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj+Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://dwarez.github.io/posts/intro-triton/"><meta property="og:site_name" content="The ML Surgeon"><meta property="og:title" content="From Sequential to Parallel: Your Journey into GPU Programming with Triton"><meta property="og:description" content="We all know that GPU programming is hard. There are many layers of complexity that make writing a kernel difficult: thinking in a parallel way (which goes against our natural sequential thinking), understanding the math behind a certain operation, keeping track of data movement, memory hierarchies, occupancy, concurrency, boundaries, and much more. Adding to that complexity, the languages used to write kernels aren’t straightforward for all developers, especially if you come from an AI/ML background and aren’t familiar with low-level programming."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-14T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-14T00:00:00+00:00"><meta property="article:tag" content="Gpu-Programming"><meta property="article:tag" content="Triton"><meta property="article:tag" content="Basics"><meta name=twitter:card content="summary"><meta name=twitter:title content="From Sequential to Parallel: Your Journey into GPU Programming with Triton"><meta name=twitter:description content="We all know that GPU programming is hard. There are many layers of complexity that make writing a kernel difficult: thinking in a parallel way (which goes against our natural sequential thinking), understanding the math behind a certain operation, keeping track of data movement, memory hierarchies, occupancy, concurrency, boundaries, and much more. Adding to that complexity, the languages used to write kernels aren’t straightforward for all developers, especially if you come from an AI/ML background and aren’t familiar with low-level programming."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"From Sequential to Parallel: Your Journey into GPU Programming with Triton","headline":"From Sequential to Parallel: Your Journey into GPU Programming with Triton","abstract":"\u003cp\u003eWe all know that GPU programming is hard. There are many layers of complexity that make writing a kernel difficult: thinking in a parallel way (which goes against our natural sequential thinking), understanding the math behind a certain operation, keeping track of data movement, memory hierarchies, occupancy, concurrency, boundaries, and much more.\nAdding to that complexity, the languages used to write kernels aren\u0026rsquo;t straightforward for all developers, especially if you come from an AI\/ML background and aren\u0026rsquo;t familiar with low-level programming.\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/dwarez.github.io\/posts\/intro-triton\/","author":{"@type":"Person","name":"Dario Salvati"},"copyrightYear":"2025","dateCreated":"2025-06-14T00:00:00\u002b00:00","datePublished":"2025-06-14T00:00:00\u002b00:00","dateModified":"2025-06-14T00:00:00\u002b00:00","keywords":["gpu-programming","triton","basics"],"mainEntityOfPage":"true","wordCount":"2164"}]</script><meta name=author content="Dario Salvati"><link href=https://linkedin.com/in/dwarez rel=me><link href=https://github.com/dwarez rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PEDMYR1V0K"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PEDMYR1V0K")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">The ML Surgeon</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Blog</p></a><a href=https://linkedin.com/in/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://github.com/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Blog</p></a></li><li class=mt-1><a href=https://linkedin.com/in/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://github.com/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div><script>(function(){var e=$(".main-menu"),t=window.location.pathname;e.find('a[href="'+t+'"]').each(function(e,t){$(t).children("p").addClass("active")})})()</script></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/img/gpu_power_hu_57149ed645ba7af8.jpg)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>The ML Surgeon</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/intro-triton/>From Sequential to Parallel: Your Journey into GPU Programming with Triton</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">From Sequential to Parallel: Your Journey into GPU Programming with Triton</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">11 mins</span><span class="px-2 text-primary-500">&#183;</span>
<span class=mb-[2px]><a href=https://github.com/DWarez/dwarez.github.io/tree/master/content/posts/intro_triton/index.md class="text-lg hover:text-primary-500" rel="noopener noreferrer" target=_blank title="Edit content"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg>
</span></span></a></span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.2a0a1375509c03daf41c53a9831aa2d560895b7a2528c3eb3b56e7b11a135c7529e49727162a053b6193be8340270a8342f42c69f3d7e262ae54bc248eaac8f5.js integrity="sha512-KgoTdVCcA9r0HFOpgxqi1WCJW3olKMPrO1bnsRoTXHUp5JcnFioFO2GTvoNAJwqDQvQsafPX4mKuVLwkjqrI9Q=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 50 50" width="50" height="50"><path fill="currentColor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/gpu-programming/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Gpu-Programming
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/triton/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Triton
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/basics/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Basics</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Dario Salvati" src=/img/avatar_hu_20729d8348a32780.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Dario Salvati</div><div class="text-sm text-neutral-700 dark:text-neutral-400">A surgeon for Machine Learning models.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/dwarez target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/dwarez target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#before-starting-setting-the-foundation>Before Starting: Setting the Foundation</a><ul><li><a href=#leetgpu-your-virtual-laboratory-for-parallel-computing>LeetGPU: Your Virtual Laboratory for Parallel Computing</a></li><li><a href=#tritons-programming-model-abstracting-complexity-into-elegance>Triton&rsquo;s Programming Model: Abstracting Complexity into Elegance</a></li></ul></li><li><a href=#first-puzzle-vector-addition---the-hello-world-of-parallel-computing>First Puzzle: Vector Addition - The &ldquo;Hello World&rdquo; of Parallel Computing</a><ul><li><a href=#implementing-the-kernel-from-theory-to-practice>Implementing the Kernel: From Theory to Practice</a></li></ul></li><li><a href=#wrapping-up-your-first-steps-into-gpu-programming>Wrapping Up: Your First Steps into GPU Programming</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#before-starting-setting-the-foundation>Before Starting: Setting the Foundation</a><ul><li><a href=#leetgpu-your-virtual-laboratory-for-parallel-computing>LeetGPU: Your Virtual Laboratory for Parallel Computing</a></li><li><a href=#tritons-programming-model-abstracting-complexity-into-elegance>Triton&rsquo;s Programming Model: Abstracting Complexity into Elegance</a></li></ul></li><li><a href=#first-puzzle-vector-addition---the-hello-world-of-parallel-computing>First Puzzle: Vector Addition - The &ldquo;Hello World&rdquo; of Parallel Computing</a><ul><li><a href=#implementing-the-kernel-from-theory-to-practice>Implementing the Kernel: From Theory to Practice</a></li></ul></li><li><a href=#wrapping-up-your-first-steps-into-gpu-programming>Wrapping Up: Your First Steps into GPU Programming</a></li></ul></nav></div></details><script>(function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=decodeURIComponent(t.attr("id")))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active")}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){n()})}})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>We all know that GPU programming is hard. There are many layers of complexity that make writing a kernel difficult: thinking in a parallel way (which goes against our natural sequential thinking), understanding the math behind a certain operation, keeping track of data movement, memory hierarchies, occupancy, concurrency, boundaries, and much more.
Adding to that complexity, the languages used to write kernels aren&rsquo;t straightforward for all developers, especially if you come from an AI/ML background and aren&rsquo;t familiar with low-level programming.</p><p>But here&rsquo;s an even bigger catch: once you write your kernel in, let&rsquo;s say, CUDA, it&rsquo;s typically optimized for a specific GPU. Sure, you can compile it for different platforms, but all the considerations based on your hardware specs won&rsquo;t necessarily apply to other hardware. Plus, the kernel will only run on Nvidia GPUs, what about everything else? Portability is a huge factor in programming. Being able to reuse code and run it anywhere is one of the main benefits of modern languages that abstract away the underlying hardware. However, this doesn&rsquo;t apply to GPU programming.
These are some of the reasons why OpenAI announced Triton in 2021, a Python-native language that lets programmers write kernels in pure Python and run them on a wide range of accelerators. The idea of writing a performant and efficient kernel in pure Python that could run on many different platforms seemed like a pipe dream, but nowadays it&rsquo;s reality.</p><p>This is why I&rsquo;d like to introduce you to this fantastic tool that&rsquo;s now at our disposal, teach you the basics, and show you an amazing website that the ML Sys community built from scratch to help democratize GPU programming.</p><p>Put on your gloves, we&rsquo;re getting started!</p><h2 class="relative group">Before Starting: Setting the Foundation<div id=before-starting-setting-the-foundation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#before-starting-setting-the-foundation aria-label=Anchor>#</a></span></h2><h3 class="relative group">LeetGPU: Your Virtual Laboratory for Parallel Computing<div id=leetgpu-your-virtual-laboratory-for-parallel-computing class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#leetgpu-your-virtual-laboratory-for-parallel-computing aria-label=Anchor>#</a></span></h3><p>First, let me introduce you to the leetgpu.com platform. Think of it as Leetcode, but for GPU programming. The incredible part is that the platform not only supports different languages (CUDA, Triton, PyTorch, Tinygrad, and Mojo) but also provides free GPU access to run and submit your code! This is absolutely huge! You now have the opportunity to learn GPU programming even without owning a GPU!</p><p>The website offers resources for all these frameworks plus a collection of puzzles with increasing difficulty that you can freely run and submit on various hardware options. The platform also lets you compare your solutions with others, so you can see how efficient your implementation is compared to the community.</p><p>This truly represents an extraordinary opportunity to dive into GPU programming, so don&rsquo;t let it slip by! I wish this platform had existed when I was starting out, it would have transformed my entire learning journey!</p><h3 class="relative group">Triton&rsquo;s Programming Model: Abstracting Complexity into Elegance<div id=tritons-programming-model-abstracting-complexity-into-elegance class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#tritons-programming-model-abstracting-complexity-into-elegance aria-label=Anchor>#</a></span></h3><p>Before exploring the code, let&rsquo;s first talk a bit about Triton&rsquo;s programming model.</p><p>If you&rsquo;re familiar with CUDA, you know there are two levels of computation: blocks and threads. Each block contains a certain number of threads, and everything exists within a grid structure. For more details about CUDA&rsquo;s programming model, check out my article [[TODO:]].</p><p>Triton, however, operates only at the block level. This means we work with vectors rather than the scalars you&rsquo;d handle in CUDA. Another key difference is that shared memory becomes abstracted from the programming model. Triton manages it for us automatically. You can see that CUDA offers finer granularity, which gives you more control but also adds complexity.</p><p>To illustrate this clearly: in CUDA, we access matrix elements like this: A[m, k], which represents a scalar, a single value. In Triton, we access elements through slicing: A[m: m+MB, k: k+KB].
Understanding these fundamental differences is crucial for making a smooth transition from CUDA to Triton programming.</p><h2 class="relative group">First Puzzle: Vector Addition - The &ldquo;Hello World&rdquo; of Parallel Computing<div id=first-puzzle-vector-addition---the-hello-world-of-parallel-computing class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#first-puzzle-vector-addition---the-hello-world-of-parallel-computing aria-label=Anchor>#</a></span></h2><p>Alright, time to dive in. Let&rsquo;s start with the first LeetGPU puzzle: Vector Addition. As you probably know, this is essentially the &ldquo;Hello World&rdquo; of GPU programming.</p><p>The problem statement is straightforward:</p><blockquote><p>Implement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum.</p></blockquote><p>Pretty clear. The constraints are:</p><blockquote><ul><li>Input vectors A and B have identical lengths</li><li>1 ≤ N ≤ 100,000,000</li></ul></blockquote><p>The platform also provides a starting script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># The use of PyTorch in Triton programs is not allowed for the purposes of fair benchmarking.</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vector_add_kernel</span><span class=p>(</span><span class=n>a_ptr</span><span class=p>,</span> <span class=n>b_ptr</span><span class=p>,</span> <span class=n>c_ptr</span><span class=p>,</span> <span class=n>n_elements</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>a_ptr</span> <span class=o>=</span> <span class=n>a_ptr</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>pointer_type</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>b_ptr</span> <span class=o>=</span> <span class=n>b_ptr</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>pointer_type</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>c_ptr</span> <span class=o>=</span> <span class=n>c_ptr</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>pointer_type</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># a_ptr, b_ptr, c_ptr are raw device pointers</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>solve</span><span class=p>(</span><span class=n>a_ptr</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b_ptr</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>c_ptr</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>N</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>    
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span> <span class=o>=</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>    <span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>),)</span>
</span></span><span class=line><span class=cl>    <span class=n>vector_add_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span><span class=n>a_ptr</span><span class=p>,</span> <span class=n>b_ptr</span><span class=p>,</span> <span class=n>c_ptr</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span></code></pre></div><p>Let&rsquo;s analyze the information from this snippet.</p><p>First, the <code>@triton.jit</code> decorator specifies that the function will be compiled just-in-time using the Triton compiler. So we can consider <code>def vector_add_kernel</code> the actual kernel we need to implement. Something very important to understand: whenever a function is decorated with <code>triton.jit</code>, its input parameters get implicitly converted to pointers. That&rsquo;s why we refer to <code>a_ptr</code>, it means we&rsquo;re working with the pointer addressing tensor <code>a</code>.</p><p>The <code>tl.pointer_type</code> function gets called to cast the tensors to the <code>tl.float32</code> type.</p><p>For this problem, we accept three tensor pointers as input: two for the input tensors to sum, and one to store the result. We also need the number of elements <code>n_elements</code> (an integer) and the block size <code>BLOCK_SIZE</code> (also an integer). However, for the latter, we use the type <code>tl.constexpr</code>, which simply states that the variable&rsquo;s value is known at compile time. This applies to the block size variable because it&rsquo;s statically set to 1024, but not to the number of elements since that depends on the input tensor shapes.</p><p>But what exactly is the block size? It&rsquo;s very similar to the same concept in CUDA, except here we refer to the number of program instances grouped together in a single block, rather than threads. Think of it as a hyperparameter: its optimal value depends on both the hardware and the kernel we&rsquo;re executing. Just remember that the block size <strong>must</strong> be a power of 2!</p><p>Finally, these lines create the execution grid and invoke the kernel:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>),)</span>
</span></span><span class=line><span class=cl><span class=n>vector_add_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span><span class=n>a_ptr</span><span class=p>,</span> <span class=n>b_ptr</span><span class=p>,</span> <span class=n>c_ptr</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span></code></pre></div><p>Notice how the grid defines the number of kernel instances (programs) that will run in parallel. It can be either a tuple (<code>Tuple[int]</code>) or a callable function (<code>Callable</code>) with metaparameters. Here, the grid size is computed using the <code>triton.cdiv</code> function, which is simply a ceiling division. This ensures we have enough blocks to cover all elements in the input vectors.</p><h3 class="relative group">Implementing the Kernel: From Theory to Practice<div id=implementing-the-kernel-from-theory-to-practice class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#implementing-the-kernel-from-theory-to-practice aria-label=Anchor>#</a></span></h3><p>We can now proceed and implement the actual kernel that will perform the vector sum.</p><p>The steps are quite simple and straightforward. First, we need to load the tensors from DRAM. Then, we simply sum them. Finally, we store the result in the output tensor.</p><p>This problem is particularly simple because all three tensors have the same length, which greatly simplifies the kernel since we don&rsquo;t need to manage different lengths or dimensions like in 2D or 3D problems.</p><p>Enough talk, let&rsquo;s write some code and follow the steps we just outlined.</p><p>To load the tensors into memory, we need to invoke the <code>tl.load</code> function. We&rsquo;ll call this function by specifying a pointer and a mask. But how do we compute the pointer we need to use? We have the start of the pointer, specified by the input parameter <code>a_ptr</code> (or <code>b_ptr</code>, assuming <code>a</code> and <code>b</code> are our input tensors and <code>c</code> is the output), but that&rsquo;s just the beginning of the address that stores the tensor. We need to move by an offset that depends on which program we&rsquo;re in, because each program has to compute the sum for a specific subset of the tensor.</p><p>We can identify which program we&rsquo;re in using the <code>program_id</code> function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></div><p>Here we specify <code>axis=0</code> because we&rsquo;ll launch a 1D grid. If you know CUDA, this will sound very familiar, but it&rsquo;s definitely more straightforward than using block and thread IDs.</p><p>Now, since each program must compute the sum over only a section of the input vectors, we need to skip the first <code>N</code> block sizes, where <code>N</code> is the program identifier. In practice, if we&rsquo;re in <code>program 1</code>, that means <code>program 0</code> will process the first <code>BLOCK_SIZE</code> elements, so we skip them. The same applies to any other index and its predecessors. We achieve this by simply multiplying the program ID by the block size:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span>
</span></span></code></pre></div><p>Great! Now we have the start of the address that must be processed by the program. To access all the elements that must be processed by the program, we need to create a range of pointers spanning from the start to the end of the section that must be accessed by the program. The span is, by definition in this case, the block size. We&rsquo;ll use the <code>tl.arange</code> function for this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span></code></pre></div><p>This function simply creates a tensor with values spanning from <code>0</code> to <code>BLOCK_SIZE</code>.</p><p>Then, we sum these two components to obtain the final offsets we need to use in our program:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>offset</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span></code></pre></div><p>Here&rsquo;s an example of what would happen:<figure><img class="my-0 rounded-md" loading=lazy decoding=async fetchpriority=low alt=offsets srcset="/posts/intro-triton/offsets_hu_a5e4b8738c75fa24.png 330w,
/posts/intro-triton/offsets_hu_c9b6dc8fccc3ba4b.png 660w,
/posts/intro-triton/offsets_hu_4b8f84c6ad4e4b48.png 1280w" data-zoom-src=/posts/intro-triton/offsets.png src=/posts/intro-triton/offsets.png><figcaption>This demonstrates how each program instance claims its designated slice of the computational workspace. Program 0 handles indices [0, 1], Program 1 processes [2, 3], Program 2 takes [4, 5], and Program 3 naturally inherits [6, 7]. The offset calculation serves as our addressing mechanism, ensuring each parallel execution unit knows precisely which data elements belong to its computational domain.</figcaption></figure></p><p>Now that we have the offsets, we can load our tensors. But wait, there&rsquo;s a catch: since the block size can only be a power of two, we could run into cases where some of the computed offsets are out of bounds for the tensor&rsquo;s memory allocation. Therefore, when loading the values, we must ensure to use only valid offsets. We can easily achieve this using a mask, like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>n_elements</span>
</span></span></code></pre></div><p>This will generate a boolean mask where each value in the mask that corresponds to the same position in the offsets tensor has value 1 if it&rsquo;s valid or 0 if it&rsquo;s not. This mask can be used in the <code>tl.load</code> function as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>a_ptr</span> <span class=o>+</span> <span class=n>offset</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span></code></pre></div><p>and the function will automatically load the values in valid positions, while setting to the <code>other</code> value all of the positions that are set to <code>0</code> in the mask. In this case, we simply set the <code>other</code> parameter to 0.</p><p>So, we can load both input tensor values as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>a_ptr</span> <span class=o>+</span> <span class=n>offset</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>b_ptr</span> <span class=o>+</span> <span class=n>offset</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span></code></pre></div><p>Once in memory, we can simply sum the loaded values:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span></code></pre></div><p>Finally, we store the result using the <code>tl.store</code> function. Notice how the parameters are very similar: we specify where to store, which values to store, and which mask to use.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>c_ptr</span> <span class=o>+</span> <span class=n>offset</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span></code></pre></div><p>And that&rsquo;s it!</p><h2 class="relative group">Wrapping Up: Your First Steps into GPU Programming<div id=wrapping-up-your-first-steps-into-gpu-programming class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#wrapping-up-your-first-steps-into-gpu-programming aria-label=Anchor>#</a></span></h2><p>And there you have it! We&rsquo;ve just walked through your first Triton kernel, and honestly, it&rsquo;s pretty amazing how straightforward it becomes once you break it down step by step.</p><p>What we&rsquo;ve accomplished here might seem simple on the surface, adding two vectors together, but you&rsquo;ve actually crossed a significant threshold. You&rsquo;ve moved from being someone who uses GPU-accelerated libraries to someone who can write GPU code directly. That&rsquo;s a pretty big deal!</p><p>The beauty of what we just did lies in how Triton removes so much of the traditional GPU programming headaches. No more worrying about thread synchronization, no more manual shared memory management, no more getting lost in the maze of CUDA&rsquo;s complexity. Just clean, readable Python that gets compiled into efficient GPU code.</p><p>This vector addition kernel is your &ldquo;Hello World&rdquo; moment, but it&rsquo;s also your foundation. The same patterns we used here (calculating offsets, creating masks, loading data, performing operations, and storing results) these will show up in practically every kernel you write from now on. Whether you&rsquo;re implementing matrix multiplication, convolutions, or custom attention mechanisms, you&rsquo;ll be using these same building blocks.</p><p>The real game-changer is having platforms like LeetGPU.com where you can actually practice this stuff without needing your own GPU setup. I can&rsquo;t stress enough how valuable that is for learning. Back in the day, getting started with GPU programming meant either having expensive hardware or begging for cluster time. Now you can just open a browser and start coding.</p><p>So what&rsquo;s next? Go play around! Try the other puzzles on LeetGPU, experiment with different block sizes, see how your implementations compare to others. The more you practice, the more intuitive this parallel thinking becomes.</p><p>Welcome to the world of GPU programming, you&rsquo;re going to love it here!</p></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://dwarez.github.io/posts/intro-triton/&amp;title=From%20Sequential%20to%20Parallel:%20Your%20Journey%20into%20GPU%20Programming%20with%20Triton" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://t.me/share/url?url=https://dwarez.github.io/posts/intro-triton/&amp;resubmit=true&amp;title=From%20Sequential%20to%20Parallel:%20Your%20Journey%20into%20GPU%20Programming%20with%20Triton" title="Share via Telegram" aria-label="Share via Telegram"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M248 8C111.033 8 0 119.033.0 256S111.033 504 248 504 496 392.967 496 256 384.967 8 248 8zM362.952 176.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452.0 013.53 6.716A43.765 43.765.0 01362.952 176.66z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://dwarez.github.io/posts/intro-triton/&amp;resubmit=true&amp;title=From%20Sequential%20to%20Parallel:%20Your%20Journey%20into%20GPU%20Programming%20with%20Triton" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://dwarez.github.io/posts/intro-triton/&amp;subject=From%20Sequential%20to%20Parallel:%20Your%20Journey%20into%20GPU%20Programming%20with%20Triton" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/posts/hello-cuda/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/hello-cuda/>Hello CUDA: A Surgical Dissection</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">13 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/gpu-programming/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Gpu-Programming
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/cuda/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cuda
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/basics/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Basics</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/transformers-anatomy/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/transformers-anatomy/>The Transformer's Anatomy: A Deep Dive into the Architecture that Revolutionized Machine Learning</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">17 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/transformers/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Transformers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/dl/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Dl</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/speed-or-die/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/speed-or-die/>Move Fast or Die Slow</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">6 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/strategy/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Strategy
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/business/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Business</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section></div><script>var oid="views_posts/intro_triton/index.md",oid_likes="likes_posts/intro_triton/index.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/transformers-anatomy/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">The Transformer's Anatomy: A Deep Dive into the Architecture that Revolutionized Machine Learning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"></span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/authors/ title=Authors>Authors</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Dario Salvati</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script><a rel=me href=https://masto.ai/@blowfish></a></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://dwarez.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>