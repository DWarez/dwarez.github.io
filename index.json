[{"content":"During the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.\nThat project was crucial for me because it revealed an inconvenient truth: matrix multiplication is the most fundamental aspect of Machine Learning. Hence, I named my project \u0026ldquo;ML is ML\u0026rdquo;: Machine Learning is Matrix Multiplication. Although the course professor didn\u0026rsquo;t fully appreciate the title, it still earned me the highest grade. Because deep down, they knew it was true. It was, indeed, an inconvenient truth.\nNOTE: Yes, I\u0026rsquo;m well aware that tensor contraction is not the same as matrix multiplication. Still, the \u0026ldquo;ML is ML\u0026rdquo; joke only works when talking about matrices, so stick with it.\nHowever, this is not the place to explain why matrix multiplication is so fundamental to modern AI. Besides, if you\u0026rsquo;re reading this blog, you probably already know.\nInstead, as Machine Learning Surgeons, we should ask ourselves how such an important operation is implemented to fully utilize the power of GPUs. It\u0026rsquo;s easy to implement something, but it\u0026rsquo;s much harder to make it run fast! Just like humans need to train their reaction times to do tasks like driving an F1 car, we Machine Learning Surgeons must operate on the muscle fibres of kernels in order to make them fast and powerful!\nSo, put on your gloves, wield your scalpels and let\u0026rsquo;s start the operation!\nThe First Cut # Since you\u0026rsquo;re still a pratictioner, I\u0026rsquo;ll walk you through the simplest matrix multiplication kernel. But first, there is a basic concept that you have to grasp before proceeding with the code.\nMatrix Linearization # As you may have learned from the Hello CUDA article, when writing CUDA kernels, we typically use block and thread indices to select elements for computation. But what happens when dealing with higher-dimensional data structures, such as matrices? Moreover, how is memory organized on GPUs when dealing with such data structures?\nIn CUDA, memory on the device is managed linearly, meaning it is stored as a single, contiguous block of memory. Therefore, matrices are stored in this contiguous block of memory in a row-major order. Linearization involves mapping the multi-dimensional indices of a matrix to a single-dimensional index. It\u0026rsquo;s much easier to do than to explain.\nConsider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{\\mathrm{N} x \\mathrm{N}}\\) stored in a row-major order, which is the representation used by CUDA. The linear index of the element \\(\\mathrm{A}_{i, j}\\) (i-th row and j-th column of \\(\\mathrm{A}\\)) is simply given by \\(i * \\mathrm{N} + j\\). This is because in the row-major representation, the array that represents the 2D matrix is a continuous sequence of the rows of the matrix. Therefore, to acces the item \\(\\mathrm{A}_{i, j}\\) we must skip \\(i\\) rows, doing \\(i * \\mathrm{N}\\) and sum to it the column index \\(j\\).\nThis is a fundamental concept to understand before proceeding, as we will be performing numerous memory accesses using this technique.\nA Naive Kernel # Great! Now you are ready to fully understand the simplest matrix multiplication kernel. Keep in mind that I\u0026rsquo;ll avoid writing all the usual boilerplate code for instantiating variables on the host, moving data to the device, and printing results. If you have any doubts, you can check the full code here 1 2 3 4 5 6 7 8 9 10 11 12 __global__ void matMulKernel(float* C, float* A, float* B, int N) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; N \u0026amp;\u0026amp; col \u0026lt; N) { float value = 0; for (int k = 0; k \u0026lt; N; ++k) { value += A[row * N + k] * B[k * N + col]; } C[row * N + col] = value; } } You should be able to understand most of this code, but let\u0026rsquo;s quickly walk through it.\nIn lines 2-3, we define the index of the row and column of the matrix element that the thread will use for the dot product computation. Remember that the mathematical notation and the CUDA representation are inverted. While in mathematical notation we use the x-axis for referring to the rows and the y-axis for referring to the columns, in CUDA we use the y-components to compute the row index and the x-components to compute the column index.\nLine 5 simply checks the index boundaries. We must check this condition because we will likely spawn more threads than there are elements in the matrix, so some threads may compute out-of-bounds indices.\nLines 6-10 perform the actual dot product computation. We start by initializing a local variable to accumulate the result. Given that this is a local scalar variable, it will be stored in a register, the fastest type of memory available. This approach works well because the dot product, which involves summing the products of corresponding elements, can be computed incrementally. Remember this, as it will be essential for the upcoming optimization steps!\nLine 7 defines the loop that allows the thread to iterate over all elements of the rows of A and the columns of B. In each iteration, we accumulate the value by adding the product of the linearized row element from A and the linearized column element from B. If you find it difficult to visualize the linearizations, I suggest writing out a few loop iterations on a piece of paper.\nLastly, line 10 stores the computed dot product into the linearized element of C.\nFor completeness, I will also include the kernel invocation. Keep in mind that you can use the ceil() function for the gridSize\ndim3 blockSize(16, 16); dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (N + blockSize.y - 1) / blockSize.y); matMulKernel\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(d_C, d_A, d_B, N); Advanced Surgical Techniques # Now that we have a basic implementation, let\u0026rsquo;s pause to consider the kernel\u0026rsquo;s behavior.\nA subtle issue to notice is that all memory accesses we perform are done using Global Memory. If you need an explanation about memory types in CUDA, please read this blog post. In brief, Global Memory is much slower compared to other types of memory, such as Shared Memory. As a result, we are wasting time with these accesses.\nFurthermore, the kernel is actually performing more data loads than necessary. Consider the first thread of the grid, let\u0026rsquo;s call it \\(thread_{0,0}\\). This thread will compute the indexes \\((0,0)\\), therefore it will load the elements \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,0}\\) in its first iteration. Then, given that we perform a loop over the elements of the matrices, \\(thread_{0,0}\\) will also load \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,0}\\), \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,0}\\) and so on.\nLet\u0026rsquo;s now consider the second thread of the grid which we will call \\(thread_{0,1}\\). This kernel will load \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,1}\\), then \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,1}\\), then \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,1}\\) and so on.\nNotice how element \\(\\mathrm{A}_{0,0}\\) is loaded by both kernels. Since Global Memory access is not shared between threads, each thread must perform a separate loading operation to obtain the value of the matrix element. In fact, in this example, it is easy to see how all elements of \\(\\mathrm{A}\\) are loaded by both kernels. I encourage you to continue this analysis on your own so that you can see for yourself that the same is true for all elements of \\(\\mathrm{B}\\) as well.\nIt\u0026rsquo;s now clear what we can do to optimize this kernel: use Shared Memory to improve the speed of loading operations, while also avoiding redundant loading operations within a block. Specifically, we will use tiling to manage the memory traffic reduction factor. If we assume 16x16 tiles, we can reduce the memory traffic by a factor of 1/16 compared to the naive implementation, because all threads will cooperate for the memory accesses. In general, given a tile size \\(T_N\\), we reduce the memory traffic by \\(1/T_N\\). If we take this idea to the extreme, by setting \\(N = T_N\\)we could load the entire matrix into a single tile in Shared Memory, thereby maximizing the reduction in memory traffic. However, this is rarely feasible. In real-world scenarios, the size of matrices is usually so large that the entire matrix cannot fit into the Shared Memory.\nUnderstanding the balance between tile size and Shared Memory usage is crucial for optimizing performance. While larger tiles can reduce memory traffic, they are limited by the available shared memory, and practical implementations must account for this constraint.\nAdvanced Incision Techniques: Tiling # NOTE: A bit of linear algebra ahead, proceed with caution.\nAs mentioned before, the dot product can be computed incrementally. We can leverage this property to split the computation of a single dot product into phases, which correspond directly to tiles. Let\u0026rsquo;s consider two matrices \\(\\mathrm{A} \\in \\mathrm{R}^{16x16}\\) and \\(\\mathrm{B} \\in \\mathrm{R}^{16x16}\\). We want to perform a matrix multiplication which result will be represented by \\(\\mathrm{C} \\in \\mathrm{R}^{16x16}\\).\nLet\u0026rsquo;s split \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\) into 4 tiles of size 4x4 each, let\u0026rsquo;s call them \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\) and \\(\\mathrm{T_B}_{i} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\).\nAssume we want to compute the first element \\(\\mathrm{C}_{0,0}\\). In the previous kernel code, we fully loaded the first row of \\(\\mathrm{A}\\) and the first column of \\(\\mathrm{B}\\) and performed the dot product. Now, instead, we will use tiles.\nSince the dot product can be computed incrementally, we can procede as follows:\nLoad two tiles into memory. We load tiles based on the current phase. At the start, we are in phase 0, so we load \\(\\mathrm{T_A}_{0}\\) and \\(\\mathrm{T_B}_{0}\\).\nCompute all elements of the dot product that can be computed with the loaded tiles. Note that these are partial computations! The remaining parts of the computations will be performed in subsequent phases. In phase 0, we would compute:\n\\(\\mathrm{C}^{0}_{0,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{0,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,1}\\) \\(\\mathrm{C}^{0}_{1,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{1,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,1}\\) Notice that the notation \\(\\mathrm{C}^{0}\\) indicates the computation of and element of \\(\\mathrm{C}\\) for the first phase, phase 0. Subsequent phases will sum upon this initial computation to obtain the final result.\nThen, increase the phase counter and repeat the process until all computational phases are complete. Note that the number of phases is \\(N/size\\_of\\_tile\\), where \\(N\\) is the dimension of the square input matrices.\nAfter all the computations for a point of \\(\\mathrm{C}\\) are done, store the result in the actual output matrix.\nIf you\u0026rsquo;ve made it through this notation hell, you\u0026rsquo;re ready to see the actual kernel code!\nBrain Cells Cooperating # An attentive reader might ask: How is this algorithm faster? From a mathematical and algorithmic perspective, the benefits of splitting computations and using tiling might not be immediately clear and can even seem to complicate matters. However, tiling is crucial for optimizing memory access patterns, as we discussed earlier. Let\u0026rsquo;s examine the code to understand how tiling enhances performance in practice:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 __global__ void matMulKernel(float* C, float* A, float* B, int N){ __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; Width/TILE_WIDTH; ++ph) { Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } C[row * Width + col] = Cvalue; } In lines 2-3, we define the two data structures used to load the tiles from \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\). We use the __shared__ keyword to specify that this memory allocation will be performed in Shared Memory.\nIn lines 5-8, we define some variables to store the block and thread IDs for both the x and y dimensions. This is done for convenience, as writing these values explicitly each time would make the code less readable. Since we are not even close to register saturation, it’s a luxury we can afford. In lines 10-11, we simply compute the row and column indexes, since the kernel assumes that each thread will compute one element of \\(\\mathrm{C}\\).\nLine 15 starts the for loop that iterates over the computational phases. As mentioned before, there are \\(N/size\\_of\\_tile\\) phases in total. For simplicity, we assume that the dimension of the matrices is divisible by the tile size, so the division yields a whole number.\nFinally, lines 16-18 demonstrate thread cooperation. Here, the threads use the shared variables Ads and Bds to load the elements into the tiles for the current phase, ph. If you look closely at the index computations, you’ll see they are almost identical to those used in the naive implementation. The key difference is that here we skip elements by using the contribution ph * TILE_WIDTH. This adjustment is necessary because, at each phase, we have already performed the required computations using elements from the previous phases (and therefore the previous tiles). The picture below illustrates this behavior perfectly.\nVisualization of indexes computation for tiled matrix multiplication. Credits to the PMPP book Furthermore, line 18 introduces a barrier synchronization. Since threads are loading elements into memory asynchronously, we must ensure that all elements of the tiles for the current phase have been loaded before proceeding with the computation of the partial dot product.\nLines 20-22 perform exactly the operation listed as step 2 in the previous section, which is the computation of the partial dot product using the previously loaded tiled elements. This step also requires a barrier synchronization, as the elements stored in Ads and Bds will be overwritten in the next phase. We must ensure that all threads have finished the computation of the partial dot product for the current phase before proceeding to the next phase.\nFinally, in line 26, we write the computed value to the output matrix C.\nFlexible Incisions: Arbitrary Tile Size # In our previous kernel code, we assumed that the matrix size is divisible by the tile size. However, this is a strong assumption that may not always hold true. Is there a way to relax this assumption? Yes, through boundary checking.\nFirst, let\u0026rsquo;s examine what happens when the matrix size is not divisible by the tile size.\nThe first observation we make is that issues arise only with the last tile. For instance, consider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{16,16}\\) and tiles \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{3x3}, i = 0,1,2,3,4,5 \\land j=0,1,2,3,4,5\\). For example, the tile \\(\\mathrm{T_A}_{0,0}\\) is within the bounds of the matrix, so it does not present any issues. However, a tile like \\(\\mathrm{T_A}_{5,5}\\) is problematic because it extends beyond the matrix boundaries. This issue occurs because 16 is not divisible by 6, therefore some tiles will go out of bounds.\nWhat exactly happens when we go out of bounds? If we access an element beyond the last row, we will end up accessing elements from subsequent rows due to the row-major linearization of the matrix. This will lead to incorrect results. When it comes to columns, accessing elements outside the allocated memory of the matrix can have various outcomes. We might load random values, load zeros, or even cause a kernel crash, depending on the system and its handling of such memory accesses.\nWe cannot solve this problem by simply not using the aforementioned tiles, as this would leave some elements unprocessed.\nTo address this, we can perform boundary checks to ensure that the computed x and y indexes for the thread are responsible for loading valid elements. The same checks apply to the output indexes. For rows, we check that \\(row \\text{\\textless} Width \\land (ph * TILE\\_WIDTH) \\text{\\textless} Width\\). For columns we check that \\((ph * TILE\\_WIDTH + ty) \\text{\\textless} Width \\land Col \\text{\\textless} Width\\). If these conditions are not met, we load a neutral value, such as 0.0, into shared memory for the operation, so that such elements will not effect the computations.\nLastly, we will store the value only if both the row and column indexes are within the bounds of the output matrix. Therefore, the kernel code is modified as follows:\n__global__ void matMulKernel(float* A, float* B, float* C, int Width) { __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; ceil(Width/float(TILE_WIDTH)); ++ph) { if(row \u0026lt; Width \u0026amp;\u0026amp; ph * TILE_WIDTH + tx \u0026lt; Width) Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; else Ads[ty][tx] = 0; if (ph * TILE_WIDTH + ty \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; else Bds[ty][tx] = 0; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } if (row \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) C[row * Width + col] = Cvalue; } Closing the Incision # We’ve come a long way in our exploration of matrix multiplication in CUDA, haven\u0026rsquo;t we? We began with the basics—our \u0026ldquo;first cut\u0026rdquo;—where we learned how to implement a straightforward matrix multiplication kernel. From there, we moved on to the exciting phase of optimization and refinement, focusing on making our GPU computations faster and more efficient. By leveraging shared memory with tiling, we achieved significant performance improvements. We also incorporated boundary checking to allow for flexible tile sizes.\nFun fact: adapting the boundary-checked, tiled version of the kernel to support rectangular matrices is surprisingly straightforward! Just follow these steps:\nReplace Width with unsigned integer arguments: j, k, l. Update Width for Matrix Heights and Widths: Replace Width with j where it refers to the height of matrix A or the height of matrix C. Replace Width with k where it refers to the width of matrix A or the height of matrix B. Replace Width with l where it refers to the width of matrix B or the width of matrix C. That\u0026rsquo;s it, you just wrote a general matrix multiplication CUDA kernel!\nWhat? You’re not convinced? You don\u0026rsquo;t trust your brilliant doctor?!\nI see that you want proof. You want some empirical data that proves that the optimizations we saw today actually worked! Fear not! In the next blog post, we’ll dive into the art of profiling CUDA kernels. I’ll guide you through the process of using tools like NVIDIA’s Nsight Compute to measure performance improvements, analyze memory usage, and make data-driven decisions about further optimizations. We’ll get down to the nitty-gritty details of profiling, so you can see exactly how your changes impact performance. It’s going to be a thrilling ride through the land of performance metrics and profiling!\nUntil then, happy coding, and remember to wash your hands!\n","date":"10 July 2024","externalUrl":null,"permalink":"/posts/matrix-multiplication/","section":"Posts","summary":"During the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.","title":"A Machine Learning Surgeon’s Toolkit: Advanced Matrix Multiplication in CUDA","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"Cuda","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/","section":"The ML Surgeon","summary":"","title":"The ML Surgeon","type":"page"},{"content":"Would you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function. Knowing the philosophy behind their architectural design, the problems they aim to solve, and the reasons for their specific construction is essential for leveraging their full potential.\nExploring such a vast and complex topic can indeed be challenging. Fortunately, we don\u0026rsquo;t need to be hardware engineers. We just need to firmly grasp the basics to understand how to fully utilize these processors in our machine learning endeavors.\nThe Battle of the Brains: CPU vs. GPU # I bet my scalpels that you are familiar with programming using a CPU. Indeed, every machine learning engineer (or software engineer) must be able to do so. However, many of you may not be fully aware of the CPU\u0026rsquo;s architectural design, perhaps because you never studied it during your academic journey. Still, I\u0026rsquo;m sure you can write perfectly functioning code.\nNowadays, a programmer can effectively write code that accomplishes tasks and solves problems without a deep understanding of the underlying CPU architecture. Modern programming practices and tools abstract away much of the complexity associated with hardware specifics. High-level programming languages and comprehensive development environments allow programmers to focus on algorithm design and functionality rather than hardware details. Additionally, compilers and interpreters perform automatic code optimizations, translating high-level code into efficient machine code tailored to the specific architecture. These optimizations ensure that the code runs efficiently on various CPUs, further diminishing the need for programmers to have in-depth knowledge of the hardware. Consequently, the ability to produce functional and efficient software relies more on logical and conceptual skills than on hardware-specific expertise.\nIn contrast to CPU programming, GPU programming necessitates a more intimate understanding of the underlying hardware to achieve optimal performance. This is because GPUs are designed with a fundamentally different architecture, emphasizing parallel processing capabilities. To leverage these capabilities effectively, programmers need to be aware of concepts such as thread hierarchies, memory coalescing, and warp execution. Unlike CPUs, where compilers can often perform extensive optimizations automatically, GPU programming requires manual optimization to fully exploit the hardware\u0026rsquo;s potential. The programmer must write code that efficiently manages thousands of parallel threads and minimizes memory access latency. Consequently, achieving high performance in GPU programming involves a deep understanding of the GPU\u0026rsquo;s architecture and the intricacies of its operation, making it significantly different from the more abstracted approach possible in CPU programming.\nThe figure below shows a comparison between the CPU and GPU architecture design, which is very helpful in understanding the purpose of both processors.\nFigure 1.1 of the PMPP book The CPU is designed for general-purpose processing and optimized for single-thread performance, enabling it to run a wide range of applications, from operating systems to specialized software. As illustrated in the figure, the CPU features a large cache memory and a few powerful cores. Additionally, it employs a hierarchical memory system, consisting of registers, multiple levels of cache, and main memory, ensuring rapid data access. To enhance its general-purpose processing capabilities, the CPU includes a complex instruction set, allowing it to perform a broad spectrum of operations.\nOn the other hand, the GPU is designed for parallel computing, following the Single Instruction Multiple Data ( SIMD) paradigm. The GPU design aims to maximize throughput by utilizing thousands of cores that compute simple operations in parallel. For memory, GPUs use High Bandwidth Memory (HBM) to facilitate very fast data flows in and out of the processor, while maintaining unified memory to enable communication between threads.\nEnough with these generic notions. Put on your gloves, and let\u0026rsquo;s cut open a GPU!\nCerebral Cortex # The figure below illustrates the fundamental design of a modern GPU, primarily consisting of an array of Streaming Multiprocessors (SMs) and Global Memory.\nFigure 4.1 of the PMPP book Each SM contains a grid of CUDA cores, local memory, and a control unit. The distinction between local and global memory is crucial and must be considered when writing kernels, as loading and storing data between SMs and Global Memory can introduce significant overhead.\nBlock Scheduling # Recall that in CUDA we call kernels as follows (example from HelloCuda):\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); This results in the CUDA runtime spawning blocks of threads, which are assigned to individual SMs. In practice, each SM is responsible for executing multiple blocks. However, since each SM has a limited number of CUDA cores and memory, there is a limit to how many blocks can be assigned to it at a given time. Consequently, there is also a limit to how many threads can be executed simultaneously on a CUDA device. To manage this limitation, the CUDA runtime keeps track of all the spawned blocks and assigns new blocks to the SMs as they complete the execution of previous blocks.\nThe effort of assigning entire blocks to a single SM is well worth it. Since threads within the same block often need to communicate with each other, having them share the same SM allows them to use the SM\u0026rsquo;s internal memory. This approach avoids the need for global memory access, which requires more time-consuming load and store operations.\nAnother key advantage of dispatching entire blocks to a single SM is synchronization. Kernel logic often requires synchronization to ensure that all threads reach a certain execution point before continuing with the computation. Even if threads are started simultaneously, their execution times can vary, and perfect timing synchronization is practically impossible. In practice, the threads\u0026rsquo; execution order is random.\nFortunately, the CUDA APIs provide a straightforward way to perform barrier synchronization with a single command:\n__syncthreads() This command causes all threads in the block to wait at that point until every thread has reached the instruction. This ensures that all threads have completed the code preceding the barrier, allowing safe continuation with the subsequent code execution.\nWarps # As mentioned earlier, each SM contains a certain number of CUDA cores. Suppose that each core individually performes an instruction at a given time; in that case, the design would need to provide an instruction fetch/dispatch unit for each core, which would be inefficient given the large number of cores per SM. Additionally, to achieve significant throughput, the device must implement the SIMD paradigm.\nTo address these requirements, each SM is divided into processing blocks, each containing a certain number of cores and a single instruction fetch/dispatch unit to instruct the group of cores. The figure below illustrates an SM equipped with 16 CUDA cores, divided into 2 processing blocks.\nFigure 4.8 of the PMPP book To align the execution of kernels with this architectural implementation, warps were conceived. Whenever a block is assigned to an SM, it is further divided into warps, which are typically 32-thread units for NVIDIA GPUs. Note that the size of warps is implementation-specific and can vary between different architectures.\nA warp is essentially the unit of thread scheduling in SMs. The 32 threads in a warp are contiguous, meaning they have a contiguous range of thread IDs. For example, if a block generates only one warp during execution, that warp will contain thread IDs ranging from 0 to 31.\nWe must consider how thread IDs are assigned within a warp. For a one-dimensional grid, the partitioning is straightforward: the array of threads is simply split into N contiguous parts, each consisting of 32 threads. In the case of a two-dimensional grid, which forms a matrix, the array is first flattened into a one-dimensional array in a row-major order (i.e., first all elements of the first row, then all elements of the second row, and so on) and then split as the one-dimensional case. For three-dimensional grids, we fix the value for the z-axis and then flatten the array as we would for the two-dimensional case. This process is repeated for each value of the z-axis.\nControl Divergence # While the use of processing blocks and warps provides execution optimization, it also introduces a hidden problem.\nAs mentioned earlier, all cores in a processing block share the same fetch/dispatch unit. Consequently, all threads in a warp execute the same instruction at a given time. But what happens with branches?\nConsider a piece of code like:\nif (thread_id%2 == 0) { ... } else { ... } In this case, the execution of operations depends on the thread ID, so not all threads in a warp will be executing the same code simultaneously. Specifically, when threads with even thread IDs are computing the code inside the if brackets, threads with odd thread IDs will be idle, and vice versa.\nThis phenomenon is called control divergence, and it generally occurs whenever there is a control structure in the code that depends on the thread ID.\nControl divergence silently suggests that we should try to avoid such control structures when writing a kernel to prevent code execution from diverging and wasting time on idle threads. However, it\u0026rsquo;s not always possible to avoid these control structures, such as when checking the portion of data on which the thread must perform computations (similar to what we did in HelloCuda).\nHippocampus # When optimizing GPU kernels, it\u0026rsquo;s crucial to remember that performance improvements from processing optimizations are only part of the overall picture. A significant contribution also comes from efficient memory usage and effectively leveraging the GPU memory hierarchy.\nWith that in mind, let\u0026rsquo;s explore a high-level overview of the GPU memory architecture design.\nHierarchical Memory Design # Just as for the CPU design, the GPU architecture also specifies a hierarchical memory design, composed by several levels. As always, the size of the memory is inversely proportional to its speed and cost.\nGlobal Memory is the largest and slowest type of memory on a GPU. If you followed the Hello CUDA tutorial, you have already used it without even knowing. Both the host and the device have read and write access to this memory. The long access latencies and relatively low access bandwidth of the Global Memory derives from the fact that it is not an on-chip memory and it is also implemented using the DRAM technology. Constant Memory is similar to Global Memory but with one key difference: it supports high-bandwidth, read-only access by the device.\nGlobal Memory is also used to create Local Memory, which shares the same properties in terms of latency and access. However, Local Memory is reserved for a single thread, meaning it is not shared among threads. Each thread has its own portion of dedicated Global Memory, typically used to allocate static arrays, spilled registers, and other elements of the thread’s call stack.\nShared Memory is an on-chip memory that allows threads within the same block to share data efficiently. It is much faster than Global Memory and helps minimize latency by reducing the number of accesses to Global Memory. Its primary purpose is to facilitate high-speed data sharing and cooperation among threads.\nRegisters are another type of on-chip memory. They are the fastest memory type and are assigned to individual threads, storing the most frequently accessed data for each thread.\nLast but not least, Texture Memory is a specialized type of read-only memory in GPUs, optimized for two-dimensional spatial locality and specific access patterns commonly found in graphics rendering and image processing tasks. Its design provides significant performance benefits in various computational scenarios, utilizing techniques such as spatial locality and hardware interpolation/filtering. However, we will not delve too deeply into this type of memory here, as it is beyond the scope of this article.\nIt\u0026rsquo;s crucial to remember this hierarchy when writing a kernel, as the speed differences between these memory types are highly noticeable and have a significant impact on the overall performance of the kernel. In modern GPUs, the combined access bandwidth of all the register files across all Streaming Multiprocessors (SMs) is at least two orders of magnitude higher than that of global memory!\nMemory Types in CUDA # Now that we have a clear high-level overview of the different types of device memory, the next logical step is to understand how to utilize them effectively when writing kernels. In CUDA, variables are declared with specific properties that dictate the type of memory they will use. These declarations also define the scope and lifetime of each variable, allowing for precise control over memory usage and performance optimization.\nHere\u0026rsquo;s a quick overview of the syntax and the resulting behaviour.\nDeclaration Memory Type Scope Lifetime Automatic Variables (arrays excluded) Register Thread Grid Automatic Array Variable Local Thread Grid __device__ __shared__ int SharedVariable Shared Block Grid __device__ int GlobalVariable Global Grid Application __device__ __constant__ int ConstantVariable Constant Grid Application Let\u0026rsquo;s explain in detail each row of this table.\nSaying non-array automatic variables is just a fancy way of referring to scalars. Every variable of this type defined in a kernel or device function is automatically placed inside a thread register, meaning each thread will have its own version of the variable. Consequently, their lifespan is limited to the execution time of the thread. You already know that registers are extremely fast, but be careful not to allocate too many scalar variables, as this can exceed the maximum capacity of the register storage. Using too many registers can negatively affect the occupancy of each Streaming Multiprocessor (SM).\nArray variables are typically instead allocated in the Local Memory, however their scope and lifetime are the same of scalar variables.\nUsing the syntax __shared__, we specify that the variable must be shared among threads in the same block. Typically, a shared variable is declared inside the kernel function and used by threads within the same block to cooperate in computation. This is a convenient approach because Shared Memory is very fast.\nBy specifying only __device__, the variable will be allocated in Global Memory, making it a global variable. This means the variable is visible to all threads and persists throughout the entire application execution. Be cautious when dealing with global variables: the only robust way to ensure data consistency and synchronization across different blocks is to use atomic instructions.\nLastly, by using the __constant__ syntax, we can define a constant variable, which will be allocated in Constant Memory. These variables are defined outside any function body, are visible to all threads in the grid, and their lifespan matches that of the application. This type of variable is typically used to provide constant inputs or data to kernels, as the variables\u0026rsquo; values cannot be modified by the kernels.\nConclusion # I hope you enjoyed this dissection! By now, you should have gained a comprehensive high-level understanding of both the Cerebral Cortex and the Hippocampus of a GPU.\nThroughout our exploration, we delved into the intricacies of Streaming Multiprocessors (SMs), warps, control divergence, and various memory types inherent to a GPU\u0026rsquo;s architecture. These components collectively form the brain of a GPU, crucial for its processing power and efficiency.\nIt\u0026rsquo;s important to recognize the profound interconnection between these two brain regions. When developing GPU code, one must consider both the Cerebral Cortex and the Hippocampus to ensure optimal performance and functionality. Neglecting either could lead to inefficiencies or errors in your code—something no true surgeon of GPU programming would permit!\nThat wraps up today\u0026rsquo;s session! Remember to remove your gloves and wash your hands! Clear your mind and prepare for the next fascinating dive into the GPU programming world!\n","date":"14 June 2024","externalUrl":null,"permalink":"/posts/cpu-gpu-architecture/","section":"Posts","summary":"Would you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function.","title":"Cerebral Cortex and Hippocampus: Understanding the Computational and Memory Design of GPUs","type":"posts"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/basics/","section":"Tags","summary":"","title":"Basics","type":"tags"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/gpu-programming/","section":"Tags","summary":"","title":"Gpu-Programming","type":"tags"},{"content":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\nWith the introduction of CUDA, everything changed. Its integration with languages like C, C++, or Python, along with the elimination of the need for knowledge in graphics programming, made GPU programming much more accessible.\nHowever, many individuals today still feel daunted by the complexity of GPU programming, both in terms of the language (which is too low-level compared to the most popular programming languages today) and the architecture and development knowledge required to write this kind of software.\nThis article aims to introduce you to this realm through an in-depth technical analysis of a HelloWorld, which is famously the starting point for every programmer who ventures into a new programming language. Just as a surgeon carefully dissects to reveal the inner workings of the human body, we\u0026rsquo;ll break down each component of CUDA\u0026rsquo;s basic architecture and functionality.\nSince we\u0026rsquo;re discussing GPU programming, the classic print(\u0026quot;hello world\u0026quot;) won\u0026rsquo;t be applicable. Instead, we\u0026rsquo;ll analyze a complete program that adds two vectors, using the GPU.\nPrerequisites # I will briefly present some prerequisites necessary to fully understand the remaining content of this post. Even if you don\u0026rsquo;t meet all of them, I still suggest you keep reading. I\u0026rsquo;m confident you can still enjoy the content and find it useful.\nBasic C/C++ knowledge. Given that CUDA is originally a superset of C/C++, it comes natural that you have to know the basics of these languages. Basic knowledge of a GPU architecture. I won\u0026rsquo;t examine deeply this topic here, but understanding how a GPU works and its computational paradigm is essential for grasping CUDA programming. Installation of CUDA on your machine to try out the code yourself. That\u0026rsquo;s about it.\nCode Dissection # In the link below, you will find the full code snippet for our HelloWorld program. I believe that reading the entire code first piques your interest, as you will likely have questions you want answered.\nIf this code seems intimidating to you, please don\u0026rsquo;t be afraid. I assure you that by the end of your reading, you will fully understand it and realize its simplicity.\nHere\u0026rsquo;s the code We are now ready to dissect this code, top to bottom.\nCUDA Error Handling # The code example begins with the definition of a macro, which will be utilized throughout the program.\n#define CUDA_CHECK_ERROR(err) \\ if (err != cudaSuccess) { \\ printf(\u0026#34;CUDA err: %s at line %d\\n\u0026#34;, cudaGetErrorString(err), __LINE__); \\ exit(EXIT_FAILURE); \\ } Within this macro, we utilize several CUDA-defined functions and symbols, such as cudaSuccess and cudaGetErrorString(cudaError_t). While these are fairly self-explanatory, let\u0026rsquo;s delve deeper into their significance.\nWe consider the err variable a cudaError_t type variable, intended to store errors encountered during execution. Within the macro, we check if the error status is cudaSuccess, indicating successful code execution. If this condition is not met, we invoke the cudaGetErrorString(cudaError_t) function to retrieve the error string, providing clarity on the encountered issue, and then we exit the code execution.\nWhile this is obviously the most basic way of doing error handling, it\u0026rsquo;s crucial to remember its necessity whenever utilizing CUDA APIs, as errors may arise during their invocation.\nThe Kernel # Now, we\u0026rsquo;ll delve into what is arguably the most intriguing segment of the code, which will unveil some pivotal technical insights into CUDA programming and hardware.\nTo begin, it\u0026rsquo;s essential to understand that the term \u0026ldquo;kernel\u0026rdquo; typically denotes functions that can be computed by the device. In this context, we\u0026rsquo;re referring to the code responsible for orchestrating the GPU\u0026rsquo;s addition of the two input vectors.\nLet\u0026rsquo;s now examine the function\u0026rsquo;s prototype.\n__global__ void vecAddKernel(float* A, float* B, float* C, int n) If you are familiar with the C programming language, you may have noticed that the keyword __global__ doesn\u0026rsquo;t come from the standard. It is in fact a keyword implemented by the CUDA programming language, which specifies the visibility of the function.\nFunction Execution Space # In the GPU programming paradigm, we distinguish between two entities: the host and the device. The host refers to the CPU along with its memory, while the device pertains to the GPU. Consequently, the CUDA programming language incorporates three function execution specifiers:\n__global__ denotes a kernel. The function is executed on the device, and is callable both by the host and the device (only for devices of compute capability 5.0 or higher). It\u0026rsquo;s crucial to note that functions of this type must return void. __device__ signifies code executed exclusively on the device. It can only be called by another device execution and not from the host. __host__ denotes code that can be called and executed solely by the host. This is the default execution space if no execution space is specified. Now that we understand what a function execution space entails, let\u0026rsquo;s briefly delve into the remainder of the prototype. As mentioned earlier, given that this function serves as a kernel, it must inherently return void. Consequently, we\u0026rsquo;ll need both the input (A, B) and output (C) vectors, as well as explicitly specify the length of the vectors (n).\nBlocks and threads # The subsequent lines of code may appear cryptic at first glance, as they encompasses several complexities. To unravel their intricacies, let\u0026rsquo;s begin with the fundamentals.\nint i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; n) { C[i] = A[i] + B[i]; } This article doesn\u0026rsquo;t delve deeply into GPU architecture, so I won\u0026rsquo;t discuss its overall structure here. However, it\u0026rsquo;s crucial to understand that GPUs operate on the SIMD (Single Instruction Multiple Data) paradigm. This allows GPUs to process multiple data in parallel, executing a single instruction.\nIn essence, the heart of GPU processing lies in threads. The kernel we\u0026rsquo;re defining will be executed by the device, which will use a certain number of threads to execute the kernel instructions.\nLet\u0026rsquo;s illustrate this with a practical example using the code snippet above. Suppose the length of both vectors is 100. We\u0026rsquo;ll write code to execute the kernel, allocating 100 threads. Each thread will be responsible for summing a specific position in the vectors.\nIn the code snippet, the position is indicated by the variable i. It\u0026rsquo;s crucial for each thread to have a distinct i value; otherwise, all threads would operate on the same position in the vectors. Fortunately, the CUDA API provides built-in thread-specific variables that help us compute the correct positional index. Specifically, we\u0026rsquo;re utilizing the following built-ins:\nthreadIdx: Represents the index of the thread currently executing the code. blockIdx: Denotes the index of the block containing the executing thread. blockDim: Specifies the size of each spawned block. A block is a group of threads that collaborate and communicate through shared memory. These blocks are scheduled and executed together by the hardware. Moreover, all blocks together form what is termed as a grid. This implies that the structure is inherently multi-dimensional: a grid has three dimensions—x, y, and z. Hence, when referencing the built-in variables threadIdx, blockIdx, and blockDim, we utilize their x attribute, which corresponds to the x-dimension of the grid. The rationale behind this will become clearer when we examine the kernel invocation.\nThe final piece to explain is the if statement preceding the actual sum computation. This if statement verifies that the computed index falls within the bounds of the input vectors. While this check may initially seem unnecessary, its significance will become apparent later on.\nHost code # Following the discussion on the kernel, let\u0026rsquo;s shift our attention to the host code of the program. Here, we\u0026rsquo;re tasked with memory allocation, kernel invocation, and error checking.\nDevice Memory # A crucial consideration is that device memory is distinct from host memory. Consequently, the device cannot directly operate on data residing in host memory. This necessitates the allocation of device memory and the transfer of data from the host to the device. While this may appear trivial, it\u0026rsquo;s one of the most significant bottlenecks in modern software, particularly when handling large amounts of data, leading to substantial communication overhead between the host and the device. Given this distinction, as a best practice, we use the notation V_h to indicate a variable used by the host and V_d to denote a variable used by the device.\nGiven the simplicity of our case study, we\u0026rsquo;ll overlook these technical intricacies and focus on accomplishing the bare minimum to ensure functionality.\nvoid vecAdd(float* A_h, float* B_h, float* C_h, int n) The vecAdd function accepts three host-vectors as arguments: A_h and B_h, which are to be summed, and C_h, which will store the result. It\u0026rsquo;s important to note that an output vector C is necessary since the kernel cannot directly return anything. Finally, n represents the length of the input vectors.\nOur initial step is to determine the size of the device-vectors. This computation follows a standard C approach:\nint size = n * sizeof(float) Next, we proceed to allocate memory on the device to store the device-vectors:\nfloat *A_d, *B_d, *C_d; cudaMalloc((void**)\u0026amp;A_d, size); cudaMalloc((void**)\u0026amp;B_d, size); cudaMalloc((void**)\u0026amp;C_d, size); Fortunately, the CUDA programming language provides us with the cudaMalloc function, which functions similarly to the malloc function but operates on the device.\nBefore invoking the kernel, the final step is to transfer data from the host to the device. At this stage, we have:\nOn the host, three vectors containing the data for computation On the device, three memory allocations initialized but lacking of the necessary data. To address this, we leverage another CUDA programming function, cudaMemcpy:\ncudaError_t error = cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); error = cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); We invoke the function with the following parameters: the destination pointer, the source pointer, the size to copy, and the direction. The parameter cudaMemcpyHostToDevice specifies that we intend to copy data from the host to the device. Additionally, observe how we utilize the previously defined macro to check for errors in case the memory copying operation fails.\nKernel Invocation # Assuming everything executes correctly, we\u0026rsquo;ve successfully copied the input vectors to the device memory. Hence, we can proceed to call the kernel.\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); First, we define the number of threads we want to allocate per block. Next, we define two dim3 type variables, which represent 3-dimensional vectors typically used to specify grid and block sizes. Since we are dealing with a 1-dimensional problem, we only need to use the x-dimension.\nWe compute the grid size as the number of elements in the input vectors, n, divided by the number of threads per block. To ensure the grid dimension can accommodate all threads, even if n is not perfectly divisible by the number of threads per block, we use the ceil operator, which will round-up the division. The block dimension is simply set to the number of threads we want to allocate per block.\nLastly, we invoke the kernel, as specified in the previous sections.\nNotice two things:\nWe can now fully understand why we need to use an if statement in the kernel, as explained in the Blocks and threads. Let\u0026rsquo;s illustrate with a practical example. Suppose n = 100 and number_of_threads = 16. When computing the x-dimension of the grid size, the exact result is 6.25. However, if we set the grid x-size to 6, we won\u0026rsquo;t allocate enough threads because we\u0026rsquo;ll have 6 blocks of 16 elements each, totaling 96 threads, leaving out 4 positions of the vectors. On the other hand, if we set the grid size to 7, we\u0026rsquo;ll allocate more threads than the number of positions to compute because 7x16=112. We can\u0026rsquo;t allow the extra threads to access memory indicated by their int i = threadIdx.x + blockIdx.x * blockDim.x; because it would be out of bounds. Therefore, the necessity of using an if statement to check such boundaries.\nOften, tutorials do not explicitly allocate the dimGrid and dimBlock variables, but instead invoke the kernel directly like this: vecAddKernel\u0026lt;\u0026lt;\u0026lt;ceil(n/number_of_threads), number_of_threads\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n). This syntax is valid because the dim3 data type automatically defaults unspecified dimensions to 1. However, I chose to specify all dimensions to clarify the explanation provided in the section Blocks and threads.\nStoring and cleaning up # After the kernel invocation, we need to retrieve the computation result. Since the kernel cannot return anything, it\u0026rsquo;s the programmer\u0026rsquo;s responsibility to fetch the output of the computation. We can easily accomplish this as follows:\nerror = cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost); CUDA_CHECK_ERROR(error); cudaFree(A_d); cudaFree(B_d); cudaFree(C_d); Once again, we utilize the cudaMemcpy function, but this time we specify cudaMemcpyDeviceToHost, indicating that we are transferring data from the device to the host. Notice also how the order of the host and device variables has switched in the function call compared to previous occurrences.\nFinally, we must free the memory that was used. This isn\u0026rsquo;t just a best practice—it\u0026rsquo;s an absolute necessity!\nMain # Just for completeness, let\u0026rsquo;s quickly go through the main function:\nint n = 512; float A_h[n], B_h[n], C_h[n]; for (int i = 0; i \u0026lt; n; i++) { A_h[i] = i; B_h[i] = i; } vecAdd(A_h, B_h, C_h, n); for (int i = 0; i \u0026lt; n; i++) { printf(\u0026#34;%g\\n\u0026#34;, C_h[i]); } return 0; There isn\u0026rsquo;t much to say here. We define the length of the vectors n, then allocate and initialize the input vectors A_h and B_h with a range from 0 to n-1. Next, we call the host code, which also invokes the kernel, and finally, we print the result stored in C_h.\nCompile and run # Compiling and running this example is straightforward. First, we compile the source as we would for a C file, but we use the CUDA compiler, called nvcc. Assuming the source is named hello_world.cu and we are in its parent directory, we can build the executable like this:\nnvcc hello_world.cu -o hello_world Then, we simply run the executable:\n./hello_world And we will receive the output (truncated for readability):\n0 2 4 ... 1018 1020 1022 Wrapping Up # In summary, we\u0026rsquo;ve explored the fundamentals of CUDA programming, which enables us to harness the power of GPUs for parallel computing tasks. We began by understanding the distinction between the host (CPU) and the device (GPU) and delved into CUDA\u0026rsquo;s function execution specifiers (__global__, __device__, __host__) that define where functions can be executed.\nWe discussed the importance of memory management, highlighting how data must be transferred between host and device memories using functions like cudaMalloc and cudaMemcpy. We also saw the significance of error handling throughout the CUDA programming process, utilizing macros to check for errors and ensure smooth execution.\nThe heart of CUDA programming lies in kernels, functions executed on the GPU, typically invoked in a grid of threads. We learned about grid and block dimensions, and the necessity of boundary checks within kernels to avoid out-of-bounds memory access.\nWe then examined a practical example of vector addition, illustrating how to define and invoke kernels, manage memory, and handle errors.\nLastly, we discussed compiling and running CUDA programs using the nvcc compiler and executing the resulting executables.\nOverall, CUDA provides a powerful framework for parallel programming on GPUs, offering immense computational capabilities for a wide range of applications. With a solid understanding of its principles and techniques, developers can unlock the full potential of GPU-accelerated computing.\nTime to close the operating theater! You\u0026rsquo;ve witnessed the basic intricacies of CUDA programming under the scalpel today. Keep honing those skills, and you\u0026rsquo;ll soon be performing GPU surgeries with finesse. Until our next exploration, stay sharp and happy coding! 🤖\n","date":"6 May 2024","externalUrl":null,"permalink":"/posts/hello-cuda/","section":"Posts","summary":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.","title":"Hello CUDA: A Surgical Dissection","type":"posts"},{"content":" NOTE: This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\nLarge Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\nWhen I first started learning about neural networks -about 5 years ago-, one of the key questions I had was if it would be possible to know, a priori, the minimum number of parameters that a network must implement to achieve a certain metric value when solving a specific problem. Unfortunately, as far as I know, there is no such theorem. Surely, we know about convergence theorems -like the Universal Approximation Theorem-, but nothing tells us the optimal number of parameters, given an architecture and a problem to solve.\nA very interesting methodology is the Cascade Correlation, which is a constructive approach for obtaining a network that solves a specific problem. However, this methodology is impractical in modern times: the size of neural networks is just too big, especially when talking about LLMs, which typically span from 7 to 70 billion parameters.\nTherefore, a follow-up question: given a trained, large network, can we reduce its size while maintaining the same accuracy?\nThat’s the key idea behind Pruning.\nPruning for Sparsity # Let’s start from a biological point of view: humans drastically reduce the number of synapses per neuron from early childhood to adolescence. This has been shown in various papers (e.g. Synapse elimination accompanies functional plasticity in hippocampal neurons) and it is a way for the brain to optimize its knowledge and remove redundancy. As often happens, this concept can also be applied to learning architectures.\nAssume to have a relatively large neural network that has been trained to approximate the solving function for a given problem, which was described by a dataset. This means that, by training, we minimized the loss function defined as:\n\\(\\mathrm{L}(\\theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{(x,y)\\in(X,Y)}{L(\\theta, x, y)}\\)\nwhere \\(\\theta\\) represents the weights of the network and \\(\\mathcal{D}\\) symbolizes the data used to train it.\nLet’s now assume that we are capable of pruning the weights \\(\\theta\\), therefore obtaining \\(\\theta_{pruned}\\). Our objective is to get approximately the same loss value when using the pruned weights. In math:\nThe advantage here is obvious: \\(\\theta_{pruned}\\) is a smaller, more efficient model that is capable of replicating the accuracy of a bigger, slower model.\nTake a look at the picture below: by pruning the red neuron, we remove 8 connections and the activation of the neuron itself. If both the networks had the same accuracy, we would prefer the right one, because its inference is more efficient.\ncredits to Nvidia blog Choosing what to prune # The pruning criterion is a very important factor to take into account, and research is still very active in this specific field. The common idea that unites all the pruning criteria is to prune parameters that are less useful in solving the problem. Therefore, we need a metric that describes the importance of a neuron in the overall network.\nIn this post, I’ll only talk about the Magnitude-based criterion, but I encourage interested readers to research other methodologies, e.g. Scaling-based pruning, Regression-based pruning, and Percentage-of-zero-based pruning.\nThe Magnitude-based criterion selects parameters to prune based on, guess what, their magnitude, therefore using an L_p norm. For example, we could choose to use an L_1 norm (a.k.a. the absolute value) as the importance metric for the parameter:\n\\(importance(\\theta_{i}) = ||\\theta_{i}||\\)\nThe rationale behind this is that the smaller the parameter, the less its influence is in the activation of the neuron.\nSo, if you assume to have a neural network with 100 parameters, and you want to prune 80% of them, you should first define an importance metric, then compute it for each parameter, sort the result, and pick the top-k parameters for that importance.\nPruning Granularity # Another key concept for pruning is granularity. We can proceed either by selecting parameters in an unstructured way or using a pattern. Let’s just see two examples to grasp this concept.\nAssume to have a matrix that represents the parameters we want to prune:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n} \\)\nand also assume that we picked the L_2norm as the importance metric for the Magnitude-based criterion.\nThen, we can perform either:\nunstructured pruning: we iterate over each parameter, compute its L_2 norm, and prune it if it’s not in the top-k percentage of neurons per importance.\nstructured pruning: instead of computing the L_2 norm for the single parameter, we first select a pattern. Let’s keep things simple and assume that our pattern corresponds to the row of the matrix P. Therefore, we will not prune the single parameter, but instead, we will prune the rows that do not belong to the top-k percentage of the highest-importance rows in the matrix.\nThe picture below depicts the difference between the two approaches.\ncredits to efficientml.ai This picture also foretells the advantages of pattern-based pruning, which I will explain in a later section.\nPruning Ratio # The pruning ratio is the last key concept we need to understand. Luckily for us, it’s quite straightforward. The ratio indicates how much sparsity we want in our neural network.\nFor example, if we pick a uniform pruning ratio of 80%, we will remove 80% of the parameters of the network, which will be selected based on the criterion and importance measure, defined before.\nThe word “uniform” is not randomly put. In fact, we can also provide a different pruning ratio for different levels of the network architecture. For example, we could select different pruning ratios for different layers of the architecture, or different channels.\nThere are several methodologies to find the optimal pruning ratio, but I will not cover them in this introduction.\nDoes pruning work? # Yes, and incredibly, if done correctly.\nThe first thing to notice is that we should always retrain the network after pruning. This is done because, when pruning aggressively (typically \u0026gt; 90%), the accuracy of the network drops significantly. Performing a fine-tuning step is crucial to retain information in the parameters that survived the pruning phase, and therefore stabilize the accuracy of the model.\nThis excellent picture from efficientml.ai very clearly depicts this concept\ncredits to efficientml.ai Computations with sparsity # When talking about the pruning granularity, we saw the conceptual difference between unstructured and structured pruning. Let’s consider again the matrix:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n}\\)\nLet’s now assume another matrix, which will be multiplied with matrix \\(\\mathcal{P}\\):\n\\(\\mathsf{L} \\in \\mathcal{R}^{n,l}\\)\nComputing \\(\\mathsf{P}\\) x \\(\\mathsf{L}\\) in the most straightforward way possible, we perform a number of MACs equal to:\n\\(MACs = m \\times n \\times l\\)\nIf we manage to reduce the number of rows of \\(\\mathsf{P}\\) -therefore reducing \\(m\\)-, it’s easy to see how the number of computations is reduced, as the size of the matrix \\(\\mathsf{P}\\) itself.\nIn the unstructured case, instead, we did not shrink the dimension of the \\(\\mathsf{P}\\) matrix, but we just made it sparse, by setting some elements to 0. How are we getting a benefit from that?\nThat’s the catch: we don’t. If we don’t have the availability of an efficient system for sparsity, we will not get any benefit from having a sparse \\(\\mathsf{P}\\) matrix. This applies to both the computational and memory point of view.\nThe topic of efficient systems for sparsity computations is just too vast to be discussed here. Let me know if you are interested in the topic and would like to read a post that delves into it!\nConclusions # In this brief introduction to pruning for efficient neural networks’ inference, we learned about the three key concepts for this methodology: selecting the parameters to prune, the pruning granularity, and the pruning ratio.\nWe also saw how much benefit can be obtained by pruning a model, both in terms of computational and memory complexity.\nThere’s still much to say about this technique, so the learning doesn’t stop here! I hope this post was a good introduction, and that it conveyed well to you these basic concepts.\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/pruning-intro/","section":"Posts","summary":"NOTE: This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\nLarge Language Model. How many times did you read that term?","title":"An Introduction to Sparsity for Efficient Neural Network Inference","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/inference/","section":"Tags","summary":"","title":"Inference","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/pruning/","section":"Tags","summary":"","title":"Pruning","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]