[{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/business/","section":"Tags","summary":"","title":"Business","type":"tags"},{"content":" Today\u0026rsquo;s article steps back from our usual technical deep-dives to examine the strategic importance of ML optimization. While we\u0026rsquo;ll touch on engineering concepts, we\u0026rsquo;ll focus more on their business impact.\nThe AI landscape is evolving at a breakneck pace, and conventional wisdom suggests that competitive advantage is simply a function of computing power - the more GPUs you can afford, the better positioned you are to win. However, recent events in the market tell a different, more nuanced story.\nConsider what happened when DeepSeek released their R1-Zero and R1 models. The announcement sent shockwaves through the US stock market, affecting even giants like NVIDIA. Why would the release of two models from a relatively smaller player have such a dramatic impact? The answer lies in what these models represented: a fundamental challenge to the \u0026ldquo;more hardware equals better results\u0026rdquo; paradigm.\nOperating under China\u0026rsquo;s GPU restrictions, DeepSeek demonstrated something remarkable - they achieved results comparable to industry leaders while reportedly using significantly less computational resources. While the full technical details and production costs remain a subject of debate (and I encourage readers to check Semianalysis\u0026rsquo;s detailed breakdown for a fact-based perspective), the key takeaway isn\u0026rsquo;t about DeepSeek specifically. It\u0026rsquo;s about what their approach represents: a shift from brute-force scaling to intelligent optimization.\nThis paradigm shift reveals a crucial truth about today\u0026rsquo;s AI market: raw computational power alone isn\u0026rsquo;t enough. The real competitive edge comes from how effectively companies can optimize their AI infrastructure and development pipeline. The winners in this space won\u0026rsquo;t necessarily be those with the biggest GPU farms, but those who can:\nExecute more meaningful experiments in less time Deliver faster inference speeds for better user experiences Maximize the efficiency of their infrastructure investments In this article, we\u0026rsquo;ll explore how these factors interplay and why they matter more than ever in today\u0026rsquo;s hyper-competitive AI landscape.\nThe Dual Benefits of Optimization # In the AI industry, optimization isn\u0026rsquo;t just a technical nicety—it\u0026rsquo;s a strategic imperative that creates value for both companies and users. Let\u0026rsquo;s break down why this matters from both perspectives.\nFor users, an AI product\u0026rsquo;s value proposition is remarkably straightforward: it needs to be both intelligent and fast. Intelligence is non-negotiable, it\u0026rsquo;s the core promise of AI, especially in generative applications. However, raw intelligence isn\u0026rsquo;t enough. A brilliantly capable model that takes too long to respond creates a frustrating user experience. In fact, there\u0026rsquo;s likely a critical response-time threshold (though its exact value remains debatable) beyond which users will abandon a smarter but slower solution in favor of a faster, less capable alternative.\nFor companies, developing such products presents a complex challenge. While the definition of a \u0026ldquo;good\u0026rdquo; AI product might be simple, intelligent and fast, achieving this balance within real-world constraints is extraordinarily difficult. Companies must navigate limited budgets, finite computational resources, market timing pressures, and funding constraints.\nHere\u0026rsquo;s where it gets interesting: these constraints, rather than being purely limiting factors, often drive innovation. Consider this paradox: companies with unlimited resources often innovate less in terms of efficiency. When you can solve any problem by throwing more hardware at it, optimization becomes an afterthought. Similarly, companies with massive funding might neglect infrastructure optimization, believing they can simply buy their way out of performance bottlenecks.\nBut this approach has a critical flaw: it eventually hits a wall. The recent market dynamics between OpenAI and DeepSeek illustrate this perfectly. When scaling through raw computing power reaches its limits, companies that haven\u0026rsquo;t invested in optimization find themselves vulnerable to more efficient and innovative competitors. The ability to do more with less becomes a crucial competitive advantage.\nBreaking Down DeepSeek\u0026rsquo;s Success: Speed in Training and Inference # DeepSeek\u0026rsquo;s approach demonstrates two critical areas where optimization creates competitive advantage: accelerating experimentation through efficient training, and delivering better user experience through optimized inference. Let\u0026rsquo;s examine both.\nThe Experimentation Advantage # In AI development, there\u0026rsquo;s no substitute for experimentation. Despite machine learning\u0026rsquo;s mathematical foundations, the behavior of models at scale often surprises even the most experienced experts. Success comes through systematic trial and error, making the speed of experimentation a crucial competitive factor. Simply put: the company that can run more meaningful experiments faster has a higher probability of achieving the next breakthrough.\nDeepSeek exemplified this by optimizing their training pipeline to maximize experiments per unit of infrastructure. Their strategic choice to explore Reinforcement Learning over traditional Supervised Fine Tuning (SFT) wasn\u0026rsquo;t just about building a smarter model—it was about finding a more efficient path to innovation. By focusing on training workflow efficiency, they could conduct more experiments despite infrastructure constraints, ultimately leading to novel methodologies.\nMaking Intelligence Fast # However, developing a smart model is only half the battle. DeepSeek\u0026rsquo;s R1 model uses Chain of Thought reasoning (DeepThink) to achieve high accuracy, but this approach requires generating significantly more tokens than models like Claude Sonnet. Without optimization, this would result in unacceptably long response times—potentially minutes of waiting for users.\nDeepSeek tackled this challenge through several innovative approaches. At the core of their optimization strategy is the Multi-head Latent Attention, which achieved a remarkable 93.3% reduction in KV Cache requirements per query. They further enhanced performance through multi-token prediction and quantization techniques. The choice of a Mixture of Experts (MoE) architecture means fewer parameters need to be activated during inference, making the model more efficient to run.\nThese optimizations transform directly into business value. The reduced computational requirements mean lower infrastructure costs and more competitive API pricing. Most importantly, they enable DeepSeek to deliver what users truly want: intelligent responses that arrive quickly. The optimization work ensures users aren\u0026rsquo;t forced to choose between smart answers and fast ones—they get both.\nThe Bottom Line: Speed is Strategy # The AI landscape has reached a fascinating inflection point. While the narrative has long centered on raw computational power and model size, companies like DeepSeek are showing that the path to innovation isn\u0026rsquo;t just about having more GPUs—it\u0026rsquo;s about using them smarter.\nOptimization isn\u0026rsquo;t merely a technical consideration; it\u0026rsquo;s a strategic imperative that touches every aspect of an AI company\u0026rsquo;s success. Through efficient training pipelines, companies can experiment more rapidly and discover breakthroughs faster than competitors. Through optimized inference, they can deliver superior user experiences while keeping costs manageable. The result is a virtuous cycle where technical excellence enables business success, which in turn funds further innovation.\nThe message for AI companies is clear: in a market where everyone is chasing the next breakthrough, the winners won\u0026rsquo;t necessarily be those with the biggest infrastructure budgets. They\u0026rsquo;ll be the ones who can innovate efficiently, experiment rapidly, and deliver solutions that are both smart and fast. In the AI race, optimization isn\u0026rsquo;t just about saving money—it\u0026rsquo;s about surviving and thriving in an increasingly competitive landscape.\n","date":"1 February 2025","externalUrl":null,"permalink":"/posts/speed-or-die/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003eToday\u0026rsquo;s article steps back from our usual technical deep-dives to examine the strategic importance of ML optimization. While we\u0026rsquo;ll touch on engineering concepts, we\u0026rsquo;ll focus more on their business impact.\u003c/p\u003e","title":"Move Fast or Die Slow","type":"posts"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/strategy/","section":"Tags","summary":"","title":"Strategy","type":"tags"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/","section":"The ML Surgeon","summary":"","title":"The ML Surgeon","type":"page"},{"content":"","date":"10 November 2024","externalUrl":null,"permalink":"/tags/inference/","section":"Tags","summary":"","title":"Inference","type":"tags"},{"content":"","date":"10 November 2024","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":"","date":"10 November 2024","externalUrl":null,"permalink":"/tags/quantization/","section":"Tags","summary":"","title":"Quantization","type":"tags"},{"content":"As humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, string theory posits that the universe\u0026rsquo;s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, loop quantum gravity suggests that space itself is made up of discrete \u0026ldquo;grains,\u0026rdquo; implying that reality might not be as continuous as it appears.\nWhy should this intrigue us, Machine Learning Surgeons? Because while we may experience continuity, machines do not. Machines operate strictly within the realm of discrete and finite data types, from bits and bytes to floating-point numbers. This disconnect raises a crucial question: How can machines represent signals that seem continuous to us?\nConsider the spikes generated by neurons in the brain. While neuronal activity appears continuous from a biological perspective, it can be interpreted as a series of discrete events—spikes or pulses. To model this activity on a machine, we sample these spikes at fixed intervals, effectively converting a seemingly continuous signal into a discrete representation. The choice of how finely we sample (and which data types we use) impacts the fidelity of our representation, affecting everything from signal reconstruction to downstream machine learning models.\nThis brings us to the core challenge: quantization—the process of mapping continuous values to discrete ones. Let\u0026rsquo;s explore how this translation from the continuous to the discrete world shapes our data types and impacts machine learning performance!\nWhat is Quantization: The Anatomy of Precision # In the natural world, many signals—such as sound waves, light intensity, or even time itself—are (perceived) continuous. However, machines, by their very nature, operate in a discrete framework. They deal with bits, bytes, and finite representations of data. This fundamental limitation means that to store and process real-world signals, we must first translate them into a form that machines can understand. Enter quantization.\nQuantization is the process of mapping a continuous range of values into a finite set of discrete levels. Think of it as breaking down a flowing river of data into buckets that can be cataloged and stored. For example:\nAn audio signal, with its infinite variations in amplitude, must be sampled at specific intervals and its amplitude mapped to discrete levels. An image, representing continuous changes in light and color, is pixelated into finite, numerical values. This conversion is essential for computation but comes with trade-offs. Quantization introduces approximations; a continuous signal can only be represented with a finite precision, leading to quantization errors. In fact, measure theory states that infinite precision is not achievable, not even in a theoretical setting.\nThe following image illustrates the process of quantization. First, the signal is sampled at a specific rate, meaning values are selected along the x-axis at regular intervals. Here, we\u0026rsquo;ll assume the x-axis represents time. On the y-axis, we have the amplitude of the signal, which is then mapped to a finite set of discrete levels.\nThe difference between the actual signal value and its closest quantized level is known as the quantization error. This error is an unavoidable artifact of the process, stemming from the approximation required to fit continuous values into a discrete framework:\nCredits to HanLab At first glance, you might think this discussion has little to do with machine learning, especially since we\u0026rsquo;re not directly talking about models. Why should we care about the quantization of real-valued, continuous signals like audio or images? And you\u0026rsquo;d be partly correct—our primary concern isn\u0026rsquo;t the raw quantization of these signals.\nInstead, the point here is to emphasize the tradeoff between the true value of a signal and its representation within hardware. Neural networks aim to mimic the inner workings of the human brain, which constantly produces electrical impulses and signals. In machine learning, these impulses are represented as neuron activations. However, these activations must ultimately exist within a machine, and machines operate within the constraints of discreteness and finiteness. This means that the continuous signals we\u0026rsquo;re trying to emulate must be mapped into a discrete, finite set of values.\nThis is where data types come into play. The choice of data types for representing weights and activations in neural networks is absolutely critical. It impacts not only the precision and accuracy of the computations but also the efficiency of the entire system. And, as you\u0026rsquo;ll see shortly, the requirements for data representation often differ significantly between the training and inference phases of a model.\nBefore diving into those differences, let\u0026rsquo;s take a moment to refresh our understanding of numeric data types and their implications.\nNumeric Data Types: The Skeleton of Computation # Integers: The Bones of Simplicity # Let\u0026rsquo;s start with the simplest data type you can think of: the integer.\nRepresenting an unsigned integer is straightforward. Given n, the number of bits used for representation, we simply use the binary representation of the number. Here\u0026rsquo;s a quick refresher for those who might need it:\nAnother good opportunity to checkout my great drawings! In this case, the range of the representation is \\([0, 2^n - 1]\\).\nBut what about signed integers? These require a way to handle both positive and negative numbers, and there are two common approaches for this:\nOne possible approach is called Sign-Magnitude Representation. In this method, the leftmost bit (most significant bit) represents the sign of the number: \\(0\\) for positive and \\(1\\) for negative. The remaining bits represent the magnitude. For example:\nIn this representation, the range of values is \\([-2^{n-1} + 1, 2^{n-1} - 1]\\).\nAlternatively, the Two\u0026rsquo;s Complement Representation can be used. Here, the leftmost bit is treated as having a negative value, allowing for a more elegant way to represent signed numbers. This method is widely used in modern computing because it simplifies arithmetic operations. For example:\nWith two\u0026rsquo;s complement, the range of values becomes \\([-2^{n-1}, 2^{n-1} - 1]\\).\nFloating Point Numbers: The Lifeblood of Precision # Floating-point numbers are where things get interesting—and a little tricky. Unlike integers, they need to represent both whole numbers and fractions, which means we need more complex ways to store them. There are several standards for this, so let\u0026rsquo;s dive in!\nThe most common one you\u0026rsquo;ve probably heard of is IEEE 754, which defines the famous 64-bit and 32-bit floating-point formats, also called FP64 and FP32.\nFP64 splits 64 bits into three parts: 1 sign bit, 11 exponent bits and 53 fraction bits (often called the mantissa or significant).\nLikewise, FP32 splits its 32 bits into three parts: 1 bit for the sign, 8 bits for the exponent, and 23 bits for the fraction.\nHere\u0026rsquo;s what an FP32 number looks like:\nThe value of an FP32 number is calculated using this formula: \\((-1)^{sign} \\times (1+ fraction) \\times 2^{exponent-127}\\)\nNote: Don\u0026rsquo;t stress about the formula! It\u0026rsquo;s slightly different for subnormal numbers, but we can skip those for now.\nNow, what\u0026rsquo;s the point of splitting numbers this way? Each part has a job:\nThe sign bit is obvious—it tells you whether the number is positive or negative. The exponent determines how big or small the number can get, controlling the numeric range. The fraction determines the numeric precision. The kicker is that there\u0026rsquo;s always a tradeoff between range and precision. With a fixed number of bits, you have to decide what\u0026rsquo;s more important for your task. Do you need pinpoint accuracy, or do you need to handle really large (or tiny) numbers?\nBeyond FP32: Cutting into Smaller Tissue Layers # FP32 offers a solid balance between precision, speed, and memory efficiency, especially compared to its big sibling, FP64. But here\u0026rsquo;s the question: do we really need 32 bits for deep learning? As it turns out, the answer is often no. Experiments have shown we can go smaller, and that\u0026rsquo;s where 16-bit data types come into play.\nLet\u0026rsquo;s start with FP16, or \u0026ldquo;half-precision\u0026rdquo;. This format uses 1 bit for the sign, 5 bits for the exponent, and 10 bits for the fraction.\nThis smaller footprint makes FP16 faster and more memory-efficient, which is a big deal for inference tasks where every millisecond counts. However, there\u0026rsquo;s a catch: those 5 exponent bits don\u0026rsquo;t provide enough dynamic range to handle the wild swings in gradient values during training. Training a model in pure FP16 often leads to a drop in accuracy because the format struggles to represent very small or very large numbers accurately.\nBut don\u0026rsquo;t lose hope! There\u0026rsquo;s a clever workaround called mixed precision training. The idea is simple: keep the heavy-lifting math (like gradient calculations) in FP32 while using FP16 for everything else. This way, you get the efficiency of FP16 without sacrificing accuracy. We\u0026rsquo;ll dive into the details of this technique soon.\nTo address FP16\u0026rsquo;s limitations for training, Google introduced BF16 (Brain Float 16). BF16 keeps the 8-bit exponent from FP32 but reduces the fraction to 7 bits. This clever tweak gives BF16 nearly the same range as FP32, making it suitable for both training and inference. The reduced precision in the fraction doesn\u0026rsquo;t hurt much in practice, especially for tasks like training deep neural networks where range matters more than pinpoint precision.\nA visual difference between FP16 and BF16 With BF16, you get a sweet spot: less memory usage than FP32 but enough dynamic range to handle training. That\u0026rsquo;s why it\u0026rsquo;s become a favorite in the deep learning community.\nEven Smaller? Dissecting FP16 and Below # Why stop at 16 bits? When it comes to efficiency, smaller is better, and researchers are pushing the limits with even tinier formats.\nOne of the boldest moves is FP8. NVIDIA introduced FP8 to squeeze even more efficiency out of deep learning models. FP8 comes in two flavors: E4M3, using 4 bits for the exponent and 3 for the fraction; E5M2, using 5 bits for the exponent and 2 for the fraction.\nIf we extend the range to the maximum, we obtain the INT8 data type, which has been a popular choice in the last years.\nBoth versions focus on maximizing throughput and minimizing memory usage, making them perfect for large-scale inference tasks.\nBut wait—why stop at 8 bits? Let\u0026rsquo;s talk about 4-bit formats.\nINT4 is a simple 4-bit signed integer. Nothing fancy, but incredibly efficient. This get a bit more interesting with FP4, which comes in variants like E1M2, E2M1, and E3M0, where you can decide how to split the few available bits between the exponent and fraction. These formats are still experimental but hold promise for ultra-fast inference on edge devices and other resource-constrained environments.\nWhy Do We Care? The Doctor\u0026rsquo;s Case for Optimization # All this talk about data types isn\u0026rsquo;t just academic—it\u0026rsquo;s mission-critical for machine learning. The data type you choose affects everything: speed, memory usage, and even the accuracy of your models. The key point is that for training, you need formats with enough precision and range (like FP32 or BF16) to capture subtle gradients and updates. For inference, though, you can often afford to go lower (FP16, FP8, or even FP4), with the compromise of losing in terms of accuracy.\nPicking the right data type is like choosing the right scalpel in an operating room—you need the perfect balance of precision and efficiency for the task at hand. And trust me, as Machine Learning Surgeons, we always care about the tools we use.\nQuantization in Practice: Performing Surgery on Model Precision # Alright, now that we\u0026rsquo;ve covered the basics of numeric data types, it\u0026rsquo;s time to roll up our sleeves and dive into quantization!\nIn this section, we\u0026rsquo;ll explore the how, when, what, and why of quantization, starting with a simple explanation and building up from there.\nHow: Naïve Quantization Algorithms for Quick Fixes # There are many techniques for quantization, so it would be impossible to cover all of them here. However, let\u0026rsquo;s focus on building a basic understanding of the process from a mathematical perspective. First and foremost, let\u0026rsquo;s clarify something: when we quantize a tensor, we aim to reduce its values. Naturally, the values of the quantized tensor will exist in the same dimensional space but within a subset of its co-domain. To put it simply, we shrink the tensor\u0026rsquo;s values (not its shape!) so that it can be represented using a data type that requires fewer bits.\nFor example, let\u0026rsquo;s say we want to quantize a tensor in FP32 format, called \\(\\mathrm{X}\\) into INT8 format—a significant leap! Since we\u0026rsquo;re targeting the INT8 data type, all the values of the FP32 tensor must be mapped to integers in the range \\([-128, 127]\\).\nLet\u0026rsquo;s implement a very simple quantization technique called absolute maximum quantization.\nAbsolute Maximum Quantization: A Straightforward Incision # The first step is to compute a scaling factor. This factor is calculated by dividing the absolute maximum value of the tensor \\(\\mathrm{X}\\) by the largest representable number in the target data type, which in this case is 127:\n\\(\\mathrm{S}= \\frac{max|\\mathrm{X}|}{127}\\)\nOnce we have the scaling factor \\(\\mathrm{S}\\), we scale all the values in \\(\\mathrm{X}\\), which we\u0026rsquo;ll call \\(\\mathrm{x_i}\\) and then round them to get the quantized values:\n\\(\\mathrm{q}_{i}= round(\\frac{\\mathrm{x_i}}{\\mathrm{S}})\\)\nIf any values end up outside the representable range, we clamp them to fall within the range of the target data type.\nReconstructing the original values is straightforward. We just multiply the quantized values by the scaling factor:\n\\(\\mathrm{\\hat{\\mathrm{x_i}}}= \\mathrm{q_i} * \\mathrm{S}\\)\nOf course, the reconstructed values won\u0026rsquo;t be identical to the original ones due to approximations during the process. This difference is what we call the quantization error, something we touched on earlier.\nOne final thing to note about absolute maximum quantization: it\u0026rsquo;s symmetric. This means the resulting tensor values are centered around zero, which is a nice property for certain applications.\nZero-Point Quantization: Accounting for Asymmetric Anatomy # For certain types of tensors, we can reduce the reconstruction error from quantization by using asymmetric quantization methods, such as zero-point quantization. This approach introduces an offset to account for the asymmetric ranges between the source and target data types, making it particularly useful when the data isn\u0026rsquo;t centered around zero.\nZero-point quantization is still very straightforward to implement. To begin with, we compute the scaling factor. This factor represents the ratio between the range of values in the source tensor and the range of values that can be represented by the target data type. The computation adjusts for the difference in these ranges:\n\\(\\mathrm{S}= \\frac{max(\\mathrm{X}) - min(\\mathrm{X})}{max\\_int - min\\_int}\\)\nOnce we have the scaling factor, the next step is to calculate the zero-point, which acts as the offset. This offset ensures that the minimum value of the source tensor aligns correctly with the minimum value of the target data type. The zero-point is determined based on the scaling factor and the range of values in the source tensor:\n\\(\\mathrm{Z}= round(min\\_int - \\frac{min(\\mathrm{X})}{\\mathrm{S}})\\)\nWith both the scaling factor and the zero-point ready, we quantize the tensor. Each original value is scaled and then shifted by the zero-point offset to ensure it fits properly into the target data type:\n\\(\\mathrm{q}_{i}= round(\\frac{\\mathrm{x_i}}{\\mathrm{S}}) + \\mathrm{Z}\\)\nDequantization is just as simple. To reconstruct the original values, you reverse the process: first, shift the quantized values back by removing the zero-point offset, and then re-scale them to their original range. A word of caution here—make sure you shift back the values before applying the scaling factor, not the other way around!\n\\(\\mathrm{\\hat{\\mathrm{x_i}}}= (\\mathrm{q_i} - \\mathrm{Z}) * \\mathrm{S}\\)\nBy using zero-point quantization, we can better preserve the original tensor\u0026rsquo;s characteristics, especially when dealing with data that isn\u0026rsquo;t symmetrically distributed. This method is a handy tool for minimizing quantization errors in such scenarios.\nWhat: Identifying the Organs to Quantize # Now that we\u0026rsquo;ve explored a naïve way to perform quantization, let\u0026rsquo;s discuss what to quantize.\nWeights and Activations: The Heart and Brain of Deep Learning # To begin with, let\u0026rsquo;s start at a very low level. Most of the computations in deep learning models involve matrix (tensor) multiplications. So, let\u0026rsquo;s focus on the matrix multiplications between the input tensor \\(\\mathrm{X}\\) and the weights \\(\\mathrm{W}\\). A simple approach to quantization is to quantize the entire matrices. However, remember that the scaling factor depends on the values within the tensors. This means that using a finer granularity can help avoid or limit some potential numerical instability issues.\nConsider a scenario where a tensor\u0026rsquo;s values are sampled from a uniform distribution, but the minimum and maximum values are actually out-of-distribution (i.e., extreme values that don\u0026rsquo;t represent the typical data). This can cause numerical instability during quantization because the scaling factor relies heavily on these two values.\nTo address this, we can use per-token and per-channel quantization. This approach involves quantizing a row of \\(\\mathrm{X}\\) and its corresponding column in \\(\\mathrm{W}\\) independently, so that the min and max values are computed at the token or channel level. By doing so, we mitigate the influence of out-of-distribution extreme values, leading to a more stable and accurate quantization process.\nUp until now, we\u0026rsquo;ve discussed quantizing tensors in general terms. However, in the context of deep learning, we must decide whether to quantize the weights, the activations, or both. This decision is important because it affects both the precision of the model and its computational speed. Therefore, it\u0026rsquo;s crucial to make this choice thoughtfully and with a clear understanding of the trade-offs involved.\nWhen: Choosing the Right Time for Precision Surgery # Deciding what to quantize is also closely related to when we want to perform quantization.\nStatic Quantization: Prepping the Patient Before Surgery # The most straightforward approach is post-training quantization. This involves taking a trained model and applying a chosen quantization algorithm.\nLet\u0026rsquo;s stop for a second and think about what we can quantize using this approach. We don\u0026rsquo;t have any particular limitation in this case: we surely can quantize the weights and the model when loading it in memory. However, if we quantize only the weights, the activations of the model will keep their original data type (let\u0026rsquo;s say BF16). This means that in order to perform the multiplication between the weights and the activations, we must dequantize the weights before that. This adds a significant overhead. At the same time, statically quantize the activations of the model seems kinda impossible, since activations depends on the input, which is only known at runtime, unlike the weights of the model that do not change overtime.\nWe can actually statically quantize the model\u0026rsquo;s activations by using a calibration dataset, which will be used to observe the activations of the model and compute their distributions. These distributions are then used to determine how the specifically the different activations should be quantized at inference time. In this way, we avoid the drawback of dequantization for allowing the matrix multiplication.\nHere\u0026rsquo;s a visualization of both cases:\nIf only the weights are quantized If both weights and activations are quantized I know what you\u0026rsquo;re thinking. The first case doesn\u0026rsquo;t make any sense: why would be quantizing the weights if then we are dequantizing them before doing the GEMM? Trust me, there\u0026rsquo;s a reason and I\u0026rsquo;ll explain it in the next section.\nDynamic Quantization: On-the-Fly Adjustments # This approach applies to static quantization. However, we could also choose dynamic quantization, where activations are quantized on the fly during inference. In this case, there\u0026rsquo;s no need for a calibration dataset, and the activation quantization becomes more accurate since it isn\u0026rsquo;t constrained by a pre-computed, limited distribution derived from the calibration data. The tradeoff remains the same: increased latency in exchange for better accuracy.\nQuantization Aware Training: Training with Surgical Foresight # But what if post-training quantization lowers the model\u0026rsquo;s performance too much? In such cases, we can turn to a more sophisticated technique called Quantization-Aware Training (QAT). This method simulates quantized computations during the forward pass while retaining higher-precision weights during backpropagation. By making the training process aware of the quantization, QAT helps the model adapt to the lower precision of its parameters, resulting in significantly better accuracy retention.\nMixed Precision Training: Balancing Precision and Efficiency # Since I brought it up earlier, let me briefly explain mixed precision training. As mentioned before, FP16\u0026rsquo;s dynamic range isn\u0026rsquo;t large enough to store training gradients with sufficient precision. This limitation means that if we were to train entirely in FP16, we\u0026rsquo;d likely end up with a poorly performing model. This is where mixed precision training comes in. The idea is straightforward: we start with FP32 weights and then quantize them to FP16 to speed up the inference during the forward pass. This results in FP16 gradients, which are then dequantized back to FP32. By doing this, we maintain numerical stability and avoid troublesome issues like vanishing or exploding gradients—fascinating phenomena to study but undesirable in practice. After that, the process is business as usual. The optimizer step remains unchanged, wrapping up the training cycle with the stability of FP32 and the speed benefits of FP16.\nWhy: Quantization as the Life-Saving Operation # Let\u0026rsquo;s dive into the significant advantages offered by the various quantization techniques we\u0026rsquo;ve explored. Quantization isn\u0026rsquo;t just about reducing a model\u0026rsquo;s memory footprint—though that\u0026rsquo;s certainly a key benefit. It also unlocks improvements tailored to specific use cases. Understanding these unique advantages allows us to make informed choices about the best quantization strategy for a given scenario.\nDynamic Quantization: Tackling Compute-Bound Emergencies # Picture this: you\u0026rsquo;re working with a massive language model (LLM) boasting 70 billion parameters. In such cases, memory becomes the limiting factor, as loading the model\u0026rsquo;s weights into memory dominates the inference process.\nEnter weights-only static quantization. By reducing the precision of the weights, the memory requirements shrink dramatically, enabling faster weight loading. Although there\u0026rsquo;s some overhead from the dequantization step (necessary for precise matrix multiplications between weights and activations), this tradeoff is well worth it in memory-bound applications.\nIn essence, static quantization relieves the memory bottleneck, leading to substantial improvements in overall inference performance. See? I told you it\u0026rsquo;d make sense!\nDynamic Quantization: Tackling Compute-Bound Bottlenecks # Now, let\u0026rsquo;s shift focus to dynamic quantization. While it also reduces the memory footprint by utilizing lower-bit precision data types, its real strength lies in speeding up computations. Dynamic quantization dynamically converts weights or activations into integers for matrix multiplications during inference.\nWhy is this impactful? Integer matrix multiplications are inherently faster than their floating-point counterparts. As a result, dynamic quantization is a game-changer for compute-bound scenarios, where the primary bottleneck is the sheer number of floating-point operations (FLOPs). By quantizing activations and leveraging the speed of integer arithmetic, we can significantly boost performance with only a minor hit to accuracy.\nAdditional Benefits: Energy Efficiency and Scalability in Healthcare Systems # Quantization\u0026rsquo;s benefits extend beyond speed and memory. From an energy efficiency perspective, lower-precision computations consume considerably less power. For instance, a 32-bit floating-point multiplication requires about 3.7 picojoules (pJ), while an 8-bit integer multiplication demands just 0.2 pJ—a more than 18-fold reduction in power consumption! This is particularly advantageous for edge devices and large-scale deployments where energy efficiency is critical.\nScalability also comes into play. Smaller models are inherently easier to handle due to their reduced memory and computational demands. This makes them more suitable for resource-constrained environments and simplifies scaling in production systems.\nThese considerations are crucial in real-world settings, where the performance of a machine learning system is evaluated not just by its metrics but also by its cost and return on investment. Not every organization can afford to operate like OpenAI, burning millions of dollars daily. For most companies, quantization offers a practical way to achieve high performance while keeping costs manageable.\nQuantization in Frameworks: The Surgeon\u0026rsquo;s Toolkit # Alright, I\u0026rsquo;ll stop rambling now. By this point, you should have a solid grasp of what quantization is, the technical details of how it works, the methodologies we can apply, and the scenarios where each approach excels.\nNow, let\u0026rsquo;s roll up our sleeves and dive into how to implement quantization in different frameworks.\nPytorch: The Scalpel for Precise Optimization # Let\u0026rsquo;s start simple. PyTorch, as always, is amazing. One of its recent innovations is Torch AO (Architecture Optimization), a framework designed to streamline optimization techniques like quantization. While it\u0026rsquo;s still in beta, it offers a wide range of quantization options that are already incredibly powerful.\nPyTorch supports three primary modes for quantization: Eager Mode, FX Graph Mode, and PyTorch 2 Export. I won\u0026rsquo;t go into the details here, but you can check out the official documentation for a deeper dive.\nQuantizing an nn.Module using Torch AO is straightforward. The library abstracts much of the complexity involved in different quantization methodologies, and most approaches boil down to using the quantize_ function.\nThis function takes several arguments, with the key ones being the nn.Module to be quantized and a callable apply_tensor_subclass that specifies the quantization type to perform.\nFor example, if we want to quantize weights to A16W4 (activations in 16-bit and weights in 4-bit), we\u0026rsquo;d use:\nquantize_(model, int4_weight_only(group_size=32, use_hqq=False)) Here, int4_weight_only is the function that applies quantization to the weights. Notice that the function takes two arguments, namely group_size and use_hqq. The Group Size is a nifty trick for improving quantization precision. Smaller group sizes lead to better precision because, as we\u0026rsquo;ve discussed, scaling factors rely on statistical properties like the minimum and maximum values of the tensor. If the tensor values are widely spread, this can introduce significant de-quantization errors. By dividing the tensor into smaller groups and calculating scaling factors for each, we reduce this error. HQQ instead stands for Half-Quadratic Quantization, and it\u0026rsquo;s a method for performing weight-only quantization without requiring a calibration dataset. It\u0026rsquo;s particularly useful for quick deployment. If you\u0026rsquo;re curious, you can read more in this this blogpost.\nBut how does quantize_ know which modules to quantize? That\u0026rsquo;s where the filter_fn argument comes into play. This callable determines, module by module, whether quantization should be applied. By default, quantize_ only quantizes linear layers using the internal _is_linear function.\nWant to try dynamic quantization? It\u0026rsquo;s just as simple:\nquantize_(model, int8_dynamic_activation_int8_weight()) This applies int8 dynamic symmetric per-token activation quantization and per-channel weight quantization.\nTorch AO offers many other quantization options, but you get the idea. It\u0026rsquo;s a versatile and powerful tool that makes implementing quantization in PyTorch easier than ever.\nGPTQ: Surgical Innovation in Post-Training Quantization # GPTQ is a post-training quantization technique that leverages INT4 weights alongside FP16 activations. It builds upon the Optimal Brain Quantization (OBQ) framework, which originally introduced per-channel quantization. However, GPTQ incorporates key optimizations that make the process more efficient and scalable.\nFor instance, using GPTQ, Bloom 176B can be quantized in under 4 GPU-hours. That\u0026rsquo;s a remarkable achievement, especially when compared to the original OBQ framework, which required 2 GPU-hours just to quantize a 336M parameter BERT model.\nSo, how can you apply GPTQ in practice? It\u0026rsquo;s quite straightforward, especially with the auto-gptq library. Here\u0026rsquo;s how you can get started:\nFirst, define a GPTQ configuration:\ngptq_config = GPTQConfig(bits=4, dataset=\u0026#34;c4\u0026#34;, tokenizer=tokenizer) In this example, we specify: the number of bits for weight quantization (e.g., bits=4 for int4) and the calibration dataset (e.g., \u0026ldquo;c4\u0026rdquo;).\nThe configuration can include many other parameters, so I highly recommend checking the official documentation.\nNext, load your model with this configuration:\nquantized_model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\u0026#34;auto\u0026#34;, quantization_config=gptq_config ) And voilà! Your model is ready for deployment with efficient post-training quantization.\nTransformers\u0026rsquo; LLM.int8(): Mitigating Outliers with Precision Cuts # By now, you\u0026rsquo;re likely familiar with the main tradeoff in quantization: dequantization error. This error largely stems from the scaling factor, which is derived from the statistical properties of the tensor being quantized. While techniques like group quantization aim to minimize this error by using multiple scaling factors, Hugging Face\u0026rsquo;s Transformers library offers another innovative approach: LLM.int8(). The core idea behind LLM.int8() is to tackle the dequantization error caused by outliers. Instead of attempting to quantize every parameter in the tensor, this method separates the outliers—parameters with unusually large magnitudes—and processes them differently.\nHere\u0026rsquo;s how it works. First, outliers are identified based on their magnitude. Anything above a threshold of 6 is considered an outlier. Instead of forcing these extreme values into the quantization process, the method handles them separately in FP16, while the rest of the tensor is processed in INT8. Afterward, the two results are combined—dequantized INT8 data merged with FP16 outliers—to produce the final output.\nThis approach works brilliantly for large models, especially those with more than 6 billion parameters, where cumulative dequantization errors can wreak havoc as they propagate through layers. By dealing with outliers differently, LLM.int8() ensures much better precision.\nThe threshold is statically set to 6, therefore parameters with a magnitude larger than 6 will be considered outliers and will not be quantized. Is it a coincidence that the threshold is the same as for ReLU6? If you think about the motivation behind ReLU6, I\u0026rsquo;d say it\u0026rsquo;s probably not.\nOf course, there\u0026rsquo;s a catch. Since it performs two separate matrix multiplications, one for the outliers and another for the rest, the method isn\u0026rsquo;t as fast as FP16. In fact, it\u0026rsquo;s about 15-23% slower right now, though optimizations are in the works.\nSo, how do we actually quantize a model to LLM.int8()? It\u0026rsquo;s pretty straightforward. First, you load the original FP16 or BF16 model as usual. Next, you create a replica using the Linear8bitLt class provided by bitsandbytes. After that, you simply copy the state dictionary from the original model to the quantized one. The magic happens when you move the quantized module to the device—that\u0026rsquo;s when the actual quantization kicks in. Here\u0026rsquo;s a quick example pulled straight from the official documentation:\nimport torch import torch.nn as nn import bitsandbytes as bnb from bnb.nn import Linear8bitLt fp16_model = nn.Sequential( nn.Linear(64, 64), nn.Linear(64, 64) ) int8_model = nn.Sequential( Linear8bitLt(64, 64, has_fp16_weights=False), Linear8bitLt(64, 64, has_fp16_weights=False) ) int8_model.load_state_dict(fp16_model.state_dict()) int8_model = int8_model.to(0) # Quantization happens here Hardware Support: The Operating Table for Modern Quantization # At the end of the day, all these methodologies and techniques are only as valuable as the practical benefits they deliver. While they might work beautifully in theory, the real-world performance hinges on hardware capabilities. Let\u0026rsquo;s briefly go over the hardware requirements needed to truly leverage these quantization techniques, and then we\u0026rsquo;ll wrap this up—this article has gone on long enough!\nTensor Cores, introduced by NVIDIA in 2017 with the Volta architecture, are a game-changer for deep learning. These specialized cores were designed to accelerate matrix operations, the backbone of most deep learning workloads. Over the years, NVIDIA has iterated on Tensor Core technology, adding support for an expanding range of data types with each new GPU generation.\nThe first generation of Tensor Cores, debuting with the Volta architecture (e.g., the V100), supported FP16 operations. This innovation helped popularize mixed-precision training by making it both efficient and practical.\nNext came the Turing architecture, which introduced a second generation of Tensor Cores capable of handling integer operations, including INT8 and INT4.\nThe Ampere architecture (think A100) further extended support to BF16, enabling even more flexibility for precision-constrained computations.\nWith the Hopper architecture (e.g., the H100), FP8 was added to the mix, further improving performance for workloads requiring reduced precision. And with NVIDIA\u0026rsquo;s latest Blackwell architecture, we now have support for FP4, pushing the boundaries even further.\nAnother critical factor to consider is memory bandwidth. Since quantization reduces the memory footprint of tensors, we need faster cache hierarchies and increased memory bandwidth to efficiently feed Tensor Cores and other low-latency hardware. Without these enhancements, the performance gains from quantization could be bottlenecked by data transfer speeds.\nConclusion: The Final Diagnosis # Quantization isn\u0026rsquo;t just a buzzword in the operating theater of machine learning; it\u0026rsquo;s a surgical technique that enables us to enhance efficiency, scalability, and sustainability in the field. Whether reducing the memory footprint of a sprawling language model or improving computational speed for edge devices, quantization holds the scalpel for optimizing deep learning systems without sacrificing too much accuracy.\nYet, as any seasoned surgeon knows, the tools are only as good as the hands that wield them. Understanding the anatomy of numeric data types, the timing of precision adjustments, and the capabilities of hardware ensures that every cut is calculated. Frameworks like PyTorch and techniques like GPTQ empower practitioners to bring these theoretical innovations to life in their practice.\nAs we leave the operating room, the prescription is clear: quantization is no longer an optional refinement—it\u0026rsquo;s a fundamental step in building systems that are not only powerful but also efficient, affordable, and environmentally conscious. For every aspiring surgeon of machine learning, this technique is not just a tool—it\u0026rsquo;s the future.\n","date":"10 November 2024","externalUrl":null,"permalink":"/posts/quantization/","section":"Posts","summary":"\u003cp\u003eAs humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, \u003cstrong\u003estring theory\u003c/strong\u003e posits that the universe\u0026rsquo;s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, \u003cstrong\u003eloop quantum gravity\u003c/strong\u003e suggests that space itself is made up of discrete \u0026ldquo;grains,\u0026rdquo; implying that reality might not be as continuous as it appears.\u003c/p\u003e","title":"The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models","type":"posts"},{"content":"","date":"21 October 2024","externalUrl":null,"permalink":"/tags/cpp/","section":"Tags","summary":"","title":"Cpp","type":"tags"},{"content":"","date":"21 October 2024","externalUrl":null,"permalink":"/tags/cuda/","section":"Tags","summary":"","title":"Cuda","type":"tags"},{"content":"","date":"21 October 2024","externalUrl":null,"permalink":"/tags/libtorch/","section":"Tags","summary":"","title":"Libtorch","type":"tags"},{"content":"","date":"21 October 2024","externalUrl":null,"permalink":"/tags/setup/","section":"Tags","summary":"","title":"Setup","type":"tags"},{"content":"Undoubtedly, one of the most critical aspects of machine learning is understanding the theory—without grasping how machines learn, you\u0026rsquo;ll never excel as an ML Surgeon! But being a surgeon isn\u0026rsquo;t just about theory; it’s about getting your hands dirty—writing code, setting up infrastructures, and operating on the intricacies of data. That’s why having a tailored, efficient, and functional development setup is essential to stay productive and ensure everything gets done right.\nIn this brief article, I’ll walk you through my personalized setup for coding in Python, C/C++, and most importantly, CUDA. Keep in mind, this is a highly opinionated guide—what works for me might not work for everyone, so feel free to adapt it to your own needs.\nThis article will be over before you know it, so no need to put on your gloves!\nPrepping for Surgery: My Development Setup # In general, I’m not a fan of abstraction—in fact, I despise it. I want full control and awareness of everything happening under the hood. This principle guides me across many domains, including software development. So, as you explore my setup, remember that this desire for control is often the reason behind the decisions I’ve made.\nThe OS: Choosing the Right Operating Room # ⚠️Note: When I refer to \u0026ldquo;Linux,\u0026rdquo; I mean a Linux distribution, which serves as an operating system. I understand that Linux itself is just the kernel.\nFor the past 13 years, I’ve relied on Linux as my operating system of choice. I believe it’s the ideal platform for developers because it offers complete control over your machine, and the open-source community continuously creates and supports an array of incredible tools that you can use freely for personal projects or professional work. Unless you’re aiming to develop for specific environments, such as Apple software, I see no reason not to use Linux.\nThroughout my extensive experience with Linux, I’ve experimented with various distributions, desktop environments, tools, and much more. Ultimately, I always gravitate toward an Arch-based distribution. Currently, I’m using Arch Linux itself, but I also find myself comfortable with EndeavourOS and similar alternatives.\nI prefer to use a tiling window manager, specifically Hyprland. I believe that tiling window managers enhance productivity by optimizing screen space and reducing excessive mouse usage, which is often unnecessary for many tasks.\nThe Terminal: Your Scalpel # The terminal is an essential tool for a developer, much like a scalpel is for a surgeon. You can accomplish nearly everything in it: writing code, managing files and Git repositories, compiling or interpreting code, and launching commands for all kinds of purposes. Because of its importance, it’s one of the tools I configure most thoroughly—it must seamlessly align with my workflow.\nMy current terminal emulator of choice is Kitty, though any terminal that suits your needs will do. As for the shell, I use zsh, which I find superior to **bash in many ways. I’ve experimented with fish and nushell, but I always return to zsh. While zsh lacks certain features like syntax highlighting and command suggestions, these can easily be added through plugins managed by a plugin manager (I use OhMyZsh).\nIn combination with zsh, I rely on tmux for handling multiple windows and panes. This is invaluable when working with several terminal panes simultaneously. With tmux, I can avoid opening multiple instances of Kitty and instead efficiently navigate between panes, split views, detach commands, and more. If you haven’t used tmux yet, I highly recommend giving it a try. I also use TPM (Tmux Plugin Manager) to manage external plugins, which further enhance tmux’s functionality.\nTo streamline my workflow, I’ve customized my .zshrc file to automatically launch a tmux session whenever I open a terminal, which saves time and keeps everything organized.\nThe Editor: Precision Tools for Code Surgery # If you use an IDE, I don’t blame you. Setting up a proper development environment can be challenging, and IDEs simplify that process by offering a suite of tools and configurations right out of the box. However, this convenience clashes with my need for total control, which is why I stopped using them a long time ago. For years, I relied on Visual Studio Code, which, at its core, is just an editor that can be transformed into an IDE with extensions and straightforward configuration. But recently, I’ve grown frustrated with some of its limitations—its sluggishness (mainly due to its background trackers), and the often confusing and overlapping configurations, which make it difficult to pinpoint the cause of specific behaviors. That’s when I decided to give Neovim a try, and I was genuinely impressed by its speed and workflow—once you get past the steep learning curve. Learning Vim motions transforms your workflow completely, reducing your dependency on the mouse. This approach aligns closely with the philosophy of tiling window managers and provides a significant boost to both productivity and speed.\nNeovim is highly customizable and extensible, much like how you can extend VS Code with plugins. However, the key difference is control. With Neovim, every aspect of the configuration is in your hands. You must be deliberate about which plugins you install, how you configure them, and their specific purposes. This results in a clean, optimized setup that is far faster and more efficient than any traditional IDE. If the process of configuring Neovim from scratch seems daunting, you can start with pre-built distributions like AstroVim or LunarVim, which handle most of the initial setup for you. However, I still recommend understanding their inner workings—it will be crucial if you want to further customize and optimize your environment down the road.\nPerforming the Procedure: A CUDA + Libtorch Case Study # In this part, I’ll give you a glimpse of how I use my environment to write a CUDA kernel while also utilizing Libtorch. For those unfamiliar, Libtorch is the C++ distribution of PyTorch, offering C++ APIs based on the ATen library. To start, head over to the Get Started Page and download the appropriate distribution. In my case, I’m using the version with CUDA 12.4 and cxx11 ABI support, but for our example, the exact version isn’t crucial.\nOnce downloaded, extract the library into your home folder—that’s about it for the \u0026ldquo;installation\u0026rdquo; process.\nNext, I create a project folder and open it in Neovim. Since I’ll be working with both C++ and CUDA, I’ve configured clang, a language server that supports both languages, ensuring smooth linting and formatting. I start by writing the CUDA kernel and immediately notice linting and formatting in action, courtesy of clang. Pretty cool!\nHowever, clang doesn’t seem to recognize Libtorch’s imports—this is expected since we simply have the library sitting in our home directory, and clang isn’t aware of it. Clang relies on a compilation database to inform it about external libraries, languages, and the specific commands needed to compile the project and generate binaries. We need to provide this information to clang so it can lint the Libtorch imports correctly!\nTo achieve this, we can generate a compile_commands.json file using Bear, a simple command-line tool that generates the necessary compile commands. Just run it like this:\nbear - [[command for compiling]] That’s all! Alternatively, you could use CMake to handle this, but personally, I avoid CMake whenever possible.\nGreat! Now I can start programming. But wait—I forgot the parameters for the torch::rand function! Normally, I’d have to leave my editor, open a browser, navigate through results, and waste time hunting down the documentation. Or, thanks to my Neovim setup, I can simply hover over the method name, press Shift+K, and voilà—a documentation pop-up appears directly in my editor!\nThis pop-up displays useful information like the library providing the method, input and output parameters, and the function signature. When available, it even shows the method’s docstring!\nBut that’s not all—if I want to check the definition of something, I can hover over it (easily done with Vim motions), press gD, and jump straight to its definition. Pretty handy!\nI can already hear you saying, \u0026ldquo;But Dario, how am I supposed to remember all these keybindings?\u0026rdquo; Well, Neovim can be configured to display suggestions as you type key sequences. For example, if I hover over the torch.rand method and press g, this suggestion view will pop up:\nI could go on all day about the editor and its capabilities, but that’s not the focus of this post. Let’s move on to compiling and running our program!\nCompiling and Running a CUDA + C++ Project: Suturing Together CUDA and C++ with Libtorch # Let’s start with the compilation command, which we’ll also use with Bear to generate the compile_commands.json file. Since we’re working with C++, CUDA, and Libtorch, we need to be mindful of the compilation flags, ensuring that all necessary information is correctly specified to compile the program.\nFirst, we’ll use nvcc as the compiler, since g++ can’t handle the CUDA syntax in cu or cpp files. Additionally, because Libtorch requires the C++17 standard, we must explicitly specify that during compilation. And since we haven’t installed Libtorch in a system-wide directory (meaning it’s not in a path that the linker will automatically search), we need to provide the correct path to the Libtorch library.\nWe also want to enable CUDA acceleration, so we’ll include the relevant CUDA flags. It’s possible that nvcc already includes these by default, so this part might not be strictly necessary—feel free to experiment and see what works for you!\nFinally, we’ll add flags for cross-compilation and linking, position-independent code, and use the \u0026ndash;no-as-needed flag with the linker to ensure all required libraries are included.\nHere’s the resulting command:\nnvcc -std=c++17 -I/path/to/libtorch/include -I/path/to/libtorch/include/torch/csrc/api/include \\ -L/path/to/libtorch/lib -ltorch -ltorch_cpu -lc10 -lcuda -lcudart \\ -Xcompiler -fPIC -Xlinker --no-as-needed -o my_program csrc/main.cu Running this command will generate the my_program executable. Let’s try running it:\n$ ./my_program ./my_program: error while loading shared libraries: libtorch.so: cannot open shared object file: No such file or directory Well, that doesn’t look like a tensor at all! The issue here is familiar: since we didn’t install the Libtorch library system-wide, the operating system doesn’t know where to find libtorch.so. We need to explicitly tell it by exporting the library path:\nexport LD_LIBRARY_PATH=/path/to/libtorch/lib:$LD_LIBRARY_PATH` Running the program again will now execute successfully. Keep in mind, though, that this export is only valid for the current terminal session. If you want to make it permanent, you can add this export command to your .zshrc (or equivalent file for your shell).\nPost-Op: Final Thoughts and Recovery # In conclusion, setting up a development environment tailored for CUDA, C++, and Libtorch may feel like stitching together a complex surgery—but once you get the hang of it, it’s smooth sailing (or should I say, smooth compiling?). With the right tools, a bit of configuration magic, and a trusty terminal scalpel, you’ll be slicing through code like a seasoned ML Surgeon in no time. Happy coding, and remember—always keep your tools sharp!\n","date":"21 October 2024","externalUrl":null,"permalink":"/posts/dev-setup/","section":"Posts","summary":"\u003cp\u003eUndoubtedly, one of the most critical aspects of machine learning is understanding the theory—without grasping how machines learn, you\u0026rsquo;ll never excel as an ML Surgeon! But being a surgeon isn\u0026rsquo;t just about theory; it’s about getting your hands dirty—writing code, setting up infrastructures, and operating on the intricacies of data. That’s why having a tailored, efficient, and \u003cstrong\u003efunctional\u003c/strong\u003e development setup is essential to stay productive and ensure everything gets done right.\u003c/p\u003e","title":"The Operating Room Setup","type":"posts"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/compiler/","section":"Tags","summary":"","title":"Compiler","type":"tags"},{"content":" You can take a look at the GitHub repository of this blogpost at this link Remember when machine learning was done using Caffe? The ML Surgeon remembers that. If you didn’t catch the reference, too bad for you!\nIn the last few days, I\u0026rsquo;ve been reflecting on how much easier machine learning has become in recent years. Not only do we now have a larger and higher-quality plethora of tools and frameworks—ones we could only dream of a few years ago—but the fact that these frameworks are so user-friendly is mind-boggling!\nThis got me thinking: are practitioners truly aware of the extreme complexity behind modern machine learning tools? Probably not. That’s why today, I want to dissect Torch Compile.\nThis article will be quite complex and lengthy, so gear up. But first, sterilize your hands.\nDissecting torch.compile: A Surgeon’s Approach to PyTorch Optimization # At the end of 2022, PyTorch 2.0 was released, bringing with it a host of improvements and new features. Among them, the standout addition was undoubtedly torch.compile, a method designed to speed up PyTorch code. Its usage is quite straightforward: pass either a torch.nn.Module or a function to the method, and you’ll get an optimized version of it. For example:\nclass MyModel(torch.nn.Module) ... model = MyModel() optimized_model = torch.compile(model) The optimized_model will, hopefully, run faster than the originally instantiated model. Later on, we’ll conduct some benchmarks to demonstrate these speedups. So far, so simple, right?\nBut what actually happens when we use torch.compile? How can a single line of code optimize a model and achieve up to 3x speedups compared to classic, eager-mode PyTorch?\nTo understand that, we’ll need to cut deeper—time to get some blood on our hands (or, ideally, gloves).\nThings to know before cutting deep # Before we dive into the complexities hidden within torch.compile, it’s essential to cover some foundational concepts. These basics are crucial for understanding the roles of the tools involved in the compilation pipeline. Trust me, the intricacies behind torch.compile are quite convoluted, and there’s a lot to grasp before you can see the full picture. So, please be patient and make sure you fully understand each step before proceeding to the next.\nSlicing Through the Layers: Dissecting PyTorch’s Computational Graphs # Have you ever wondered what really happens when you execute PyTorch code? If not, shame on you! Don’t take for granted the incredible features right at your fingertips!\nLet’s start from the beginning. Suppose you have code like this pseudocode:\nfn(x op y) where, x and y are two tensors, op is an operation (like the product * or sum +), and fn is a function (like log or sin).\nThis syntax results in a computational graph, which represents the operations that will be performed on the tensors. Below is a simple sketch of the computational graph that would result from the code above:\nIs this crooked? I can\u0026rsquo;t really tell. In Pytorch, the graph is built dynamically as operations are applied to tensors, which is often referred as define-by-run.\nBut wait! That\u0026rsquo;s only the forward step! We can’t train our models with just that! Luckily for us, Pytorch provides autograd, a system responsible for automatic differentiation. It records operations on tensors to form an autograd graph. Long story short, PyTorch automatically computes gradients for tensors. A computational graph for the backward pass looks something like this:\nIt\u0026rsquo;s definitely crooked ⚠️ Note: I forgot to add arrows from op and Z to the Derivative Magic block!\nDamn, I\u0026rsquo;m good at drawing. Anyway, in case you missed your calculus classes, the notation \\(\\frac{\\partial Z}{\\partial X}\\) and \\(\\frac{\\partial Z}{\\partial Y}\\) stands for the partial derivative of Z with respect to X (or Y).\nAs you can see, there are a lot of graphs involved. So, guess what torch.compile does to these graphs? That’s right—it optimizes them to make the overall computation faster.\nProbing the Depths: Surgical Insights into PyTorch’s FX Graphs # Let’s dive deeper into the world of graphs. In the previous section, we explored the critical role of computational graphs. But how do we go about optimizing them? I’m not referring to techniques or methodologies—I\u0026rsquo;m talking about the nuts and bolts of how we can technically modify and optimize PyTorch\u0026rsquo;s computational graphs.\nTo do that, we need a specialized toolkit. Fortunately, PyTorch equips us with just what we need: the FX toolkit.\nThe FX toolkit allows to modify torch.nn.Modules by implementing a pipeline consisting of a symbolic tracer, an intermediate representation (IR) and a Python code generator. This makes FX a powerful Python-to-Python transformation toolkit.\nThe symbolic tracer constructs a torch.fx.GraphModule by recording the operations that occur when the nn.Module is fed with fake data, called proxies.\nA GraphModule is essentially a nn.Module generated from a torch.fx.Graph, which serves as the core data structure for FX’s internal representation.\nWith just a few lines of code, we can observe how the symbolic tracer and intermediate representation function:\nimport torch import torch.fx import torch.nn as nn class MyModule(nn.Module): def __init__(self): super().__init__() self.weights = torch.nn.Parameter(torch.rand(4, 4)) self.linear = torch.nn.Linear(4, 5) def forward(self, x): return (self.linear(x) + x).relu() m = MyModule() gm = torch.fx.symbolic_trace(m) print(gm.graph) This script outputs the following graph representation:\ngraph(): %x : [num_users=2] = placeholder[target=x] %linear : [num_users=1] = call_module[target=linear](args = (%x,), kwargs = {}) %add : [num_users=1] = call_function[target=operator.add](args = (%linear, %x), kwargs = {}) %relu : [num_users=1] = call_method[target=relu](args = (%add,), kwargs = {}) return relu This output shows the graph’s intermediate representation, which is made up of Nodes. Without going too deep into the details, you can see references to module calls (e.g. linear), function calls (e.g. add), and method calls (e.g. relu). Each node also specifies the args for the operation, which are other nodes within the graph.\nOnce we have this graph, we can modify it as needed. Afterward, the code generator component takes over, creating a new GraphModule from the modified Graph data structure. I won’t dive into the specific techniques for modifying a graph here—this article is already long enough!\nStitching Together Efficiency: Introducing CUDA Graphs # While we\u0026rsquo;re on the subject of graphs, it’s worth highlighting another important feature: CUDA Graphs. Introduced in 2021, CUDA Graphs are a relatively new addition to the PyTorch ecosystem, specifically available for NVIDIA GPUs with CUDA version 10 or higher.\nTypically, when operations are executed on the GPU, each kernel launch must be initiated from the CPU—a process that introduces noticeable overhead, especially when dealing with thousands of operations. Each individual launch might be small, but when accumulated, this overhead can impact performance.\nCUDA Graphs address this by representing GPU operations as a single, cohesive graph. While building and launching this graph may initially be slower, the advantage lies in the fact that all subsequent operations remain on the GPU, significantly reducing the overhead caused by CPU-GPU communication.\nThe image below illustrates this concept perfectly:\nCredits to this blogpost Into the Operating Room: Dissecting the Mechanics of Torch Compile # After all this talk about graphs, it\u0026rsquo;s finally time to get down to business. Now, we’re ready to make the incision and dive deep into torch.compile to explore its inner workings. Armed with the knowledge we’ve gained in the previous sections, this should feel like a well-prepared field trip into the body of PyTorch, right? I certainly hope so—my head’s already spinning from the sheer complexity of it all!\nOperating on the Fly: Torch Dynamo’s JIT Bytecode Transformation # Let\u0026rsquo;s start with a definition: Torch Dynamo is a Python JIT compiler that uses CPython\u0026rsquo;s frame evaluation API to dynamically modify the bytecode generated from Pytorch source. That sentence might sound a bit overwhelming, so let’s take it step by step.\nFirst, what is Just-in-time (JIT) compilation? It’s a compilation process that occurs during the execution of a program, rather than before (as with languages like C). In Python, this means that while the program is running, its bytecode is translated into machine code, which the system then executes. Here’s a simple diagram to illustrate:\nAs you can see, the original Python source code is parsed into bytecode, which is easier to manage during execution. Now, thanks to the frame evaluation API, we can insert a middleware between the bytecode and the interpreter, as shown in the diagram below:\nThis is where Torch Dynamo comes in. It acts as a middleware, intercepting the bytecode to rewrite it and extract FX graphs from the PyTorch operations defined in the source code.\nSince Dynamo operates just-in-time, it dynamically intercepts bytecode during execution and extracts graphs based on the current state of the code. This allows us to work with dynamic graphs, adapting to the changing flow of execution. However, for the sake of performance, we want to avoid re-capturing graphs every time the same code runs—doing so repeatedly, as seen in frameworks like JAX, would result in unnecessary overhead.\nTo address this, Dynamo uses guards. These guards are conditions that check whether the graph needs to be re-captured. If nothing significant has changed since the last run, Dynamo will use the previously captured graph, avoiding the need to reconstruct it from scratch.\nHere’s a code snippet to illustrate how guards work:\nfrom typing import Callable, List import torch from torch import _dynamo as torchdynamo def custom_compiler(graph_module: torch.fx.GraphModule, dummy_inputs: List[torch.Tensor]) -\u0026gt; Callable: graph_module.graph.print_tabular() return graph_module.forward @torchdynamo.optimize(custom_compiler) def example(a: torch.Tensor, b: torch.Tensor) -\u0026gt; torch.Tensor: x = a / (torch.abs(a) + 1) return x * b for _ in range(100): example(torch.randn(10), torch.randn(10)) To observe Dynamo in action, run the script with the following command to enable the appropriate logging level:\nTORCH_LOGS=guards uv run src/dynamo.py Here’s a snippet of the output:\n[__guards] | +- GuardManager: source=L[\u0026#39;a\u0026#39;], accessed_by=DictGetItemGuardAccessor(a) [__guards] | | +- TENSOR_MATCH: check_tensor(L[\u0026#39;a\u0026#39;], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1]) # x = a / (torch.abs(a) + 1) # src/dynamo.py:14 in example As you can see, the output shows guards being used. These are essentially assertions that determine whether the graph should be reused or re-captured. For example, the check_tensor guard verifies properties of the torch.Tensor, such as dtype, device, requires_grad, and size. If any of these properties change, the guard triggers a re-capture of the graph, ensuring that it remains accurate for the current execution.\nHandling Dynamic Flow with Surgical Precision: Torch Dynamo vs. Static Tracing Tools # One of the standout features of Torch Dynamo, compared to other tracing tools like TorchScript or FX tracing, is its ability to trace dynamic graphs that involve data-dependent control flow. In simpler terms, this means that the execution path of the code depends on a dynamic value, which makes it impossible to capture within a static graph.\nHere’s a simple example:\ndef function(x: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: return y if x.sum() \u0026gt; 0 else -y In this case, the returned value depends on the sum of the tensor x. Because of this dynamic condition, the function can’t be traced by a tool that only works with static graphs.\nIf we attempt to trace this function using TorchScript, it will fail silently, producing a static graph. This means that even if the condition x.sum() \u0026gt; 0 is false, the traced function will still return y, which is incorrect.\nWith FX tracing, however, we would get an exception like:\nraise TraceError('symbolically traced variables cannot be used as inputs to control flow') torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\nTrying to bypass this by providing concrete input arguments won’t work either, as FX tracing will still generate a static graph—leading to the same issues as TorchScript.\nAlthough TorchScript can technically support data-dependent control flow, this requires significant changes to the codebase. Torch Dynamo, on the other hand, handles data-dependent control flow seamlessly, with no need for code modifications. When Dynamo encounters unsupported Python code (like control flow dependent on dynamic data), it breaks the computation graph, allows Python’s interpreter to process the unsupported code, and then resumes graph capture.\nThis feature also allows Dynamo to trace and optimize non-PyTorch code—another major limitation of both TorchScript and FX tracing.\nTorch Inductor: The Final Scalpel for Optimizing Computational Graphs # The last crucial tool in our arsenal is a compiler that knows how to transform the computational graph into highly efficient machine code. In this realm, this is typically called deep learning compiler. This is where TorchInductor comes in.\nTorchInductor acts as the key performance engine for PyTorch’s deep learning models by compiling and optimizing the graph for both inference and training modes—something that many traditional backends struggle with. Most alternative backends are limited to inference-only optimizations, leaving a significant gap in performance during the training phase. TorchInductor fills this gap by targeting both modes of operation, offering a unified solution that performs well across different stages of machine learning pipelines.\nTorchInductor supports both CPU and GPU architectures, adapting its optimizations based on the target hardware:\nFor CPUs, TorchInductor generates highly efficient C++/OpenMP code. OpenMP is a popular framework for parallel computing on CPUs, making it ideal for distributing workloads across multiple cores in modern processors. By leveraging OpenMP, Inductor ensures that the compiled code scales with CPU architectures for tasks like training and inference.\nFor GPUs, the generated code is written in Triton, a high-level programming language designed for ease of use and speed, serving as a flexible alternative to CUDA. Triton is designed to simplify GPU programming by providing Python-like syntax, making it accessible to a broader range of developers. It supports NVIDIA GPUs with compute capability 7.0 or greater, as well as AMD GPUs with ROCm 5.2 or later. Though Triton’s support for CPUs is still evolving, its GPU capabilities make it a powerful ally in the quest for optimization.\nKey Optimization Techniques with TorchInductor # Once the computational graph is fed into TorchInductor, a number of advanced optimization techniques are applied to significantly speed up the execution of your machine learning models.\nOperation Fusion\nOne of the primary optimizations that TorchInductor performs is operation fusion. This technique merges multiple operations into a single kernel, reducing the overhead associated with launching separate kernels and minimizing memory bandwidth usage. By combining operations, the system executes more tasks with fewer memory transactions, leading to noticeable performance boosts. This is particularly effective for GPU optimization, where kernel launch overhead can become a bottleneck.\nMemory Layout Transformations\nAnother key technique involves memory layout transformations. The layout of tensors in memory can have a substantial impact on performance, especially when accessing data in parallel. TorchInductor reorders tensors to match the access patterns of the target hardware, ensuring that memory accesses are efficient. This process helps reduce cache misses, improve memory locality, and maximize the performance of both CPU and GPU systems.\nLoop Unrolling\nFor CPU-bound operations, loop unrolling is a critical optimization. It involves transforming loops to execute multiple iterations in one go, reducing the overhead associated with loop control and improving cache utilization. By expanding loops, TorchInductor increases the efficiency of the CPU\u0026rsquo;s instruction pipeline, making it better suited to handle parallel workloads and improving overall throughput.\nParallelism and Hardware Utilization\nTorchInductor doesn\u0026rsquo;t just focus on memory and execution optimizations—it also maximizes hardware utilization through enhanced parallelism. For GPUs, this means leveraging more cores simultaneously, while for CPUs, this means distributing tasks efficiently across multiple cores. The overall effect is that models run faster and scale better across different hardware setups.\nScalpel in Hand: A Practical Dive into torch.compile # Enough theory—let’s get our hands dirty! As any good surgeon knows, the best way to master a technique is through practice. In this section, we’ll explore the magic of torch.compile by dissecting its output and performance. We’ll first analyze what happens under the hood when we use torch.compile, and then we’ll run a benchmark to determine if the compiled function is truly faster.\nLet’s start with a straightforward PyTorch function to keep things simple. This will allow us to focus on understanding how the compilation process works without being overwhelmed by complexity:\ndef simple_fn(x: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: z = torch.matmul(x, y) return torch.nn.functional.softmax(z, dim=1) Why this function?\nReadability: It’s easier to understand and debug the logs with a minimal example compared to a full nn.Module. Interesting Output: This function performs a matrix multiplication (matmul) followed by a softmax operation. These are common operations in deep learning, and their transformation during compilation will give us insights into PyTorch’s lower-level optimizations. Now, let’s compile the function using torch.compile with the TorchInductor backend, and then run it with some random input tensors:\n# Create input tensors x = torch.rand((100, 100)) y = torch.rand((100, 100)) # Compile the function using torch.compile() with TorchInductor backend compiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;) # Call the compiled function result = compiled_fn(x, y) If we run this script with debugging enabled (TORCH_COMPILE_DEBUG=1), a folder named torch_compile_debug will appear in the directory where the script was executed. This folder contains artifacts from the compilation process, such as:\nfx_graph_readable.py fx_graph_runnable.py fx_graph_transformed.py ir_post_fusion.txt ir_pre_fusion.txt output_code.py Let’s break down the most interesting files:\nIntermediate Representation (IR) # The Intermediate Representation (IR) files provide snapshots of the computational graph before and after fusion. These files contain a low-level, abstracted view of the operations PyTorch will perform, allowing for optimizations to be applied.\nPre-Fusion IR: Shows the state of the computational graph before any optimizations have been applied. Post-Fusion IR: Displays the graph after key optimizations, such as operation fusion, have been performed. FX Graph Artifacts # The FX graph files offer another layer of insight into the internal workings of torch.compile. Let’s open fx_graph_readable.py to examine how PyTorch translates the original function into an intermediate, traceable format:\n1 2 3 4 5 6 7 8 9 10 11 12 class \u0026lt;lambda\u0026gt;(torch.nn.Module): def forward(self, arg0_1: \u0026#34;f32[100, 100]\u0026#34;, arg1_1: \u0026#34;f32[100, 100]\u0026#34;): # File: /home/dwarez/Documents/workspace/torch_compile/inductor.py:6 in simple_fn, code: z = torch.matmul(x, y) mm: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.mm.default(arg1_1, arg0_1); arg1_1 = arg0_1 = None # File: /home/dwarez/Documents/workspace/torch_compile/inductor.py:7 in simple_fn, code: return torch.nn.functional.softmax(z, dim=1) amax: \u0026#34;f32[100, 1]\u0026#34; = torch.ops.aten.amax.default(mm, [1], True) sub: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.sub.Tensor(mm, amax); mm = amax = None exp: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.exp.default(sub); sub = None sum_1: \u0026#34;f32[100, 1]\u0026#34; = torch.ops.aten.sum.dim_IntList(exp, [1], True) div: \u0026#34;f32[100, 100]\u0026#34; = torch.ops.aten.div.Tensor(exp, sum_1); exp = sum_1 = None return (div,) Here:\nThe graph is structured as a PyTorch nn.Module with a forward method. Each operation (such as matmul and softmax) is translated into lower-level ATen operations (torch.ops.aten), which represent the core tensor computations that PyTorch relies on. The code clearly shows how PyTorch’s softmax function has been decomposed into its mathematical components:\namax: Maximum value extraction across the specified dimension. sub: Subtraction of the maximum value from each element in the matrix. exp: Exponentiation of the result. sum: Summation across the same dimension. div: Division to normalize the output, resulting in the final softmax. This low-level breakdown reveals the granular steps involved in even simple operations, showing how TorchInductor prepares the graph for optimization.\nWhen you see torch.ops.aten, it refers to ATen, the backend engine that powers PyTorch’s tensor computations. ATen is responsible for handling fundamental operations like matrix multiplication, element-wise functions, and reductions, ensuring that these operations are executed efficiently on both CPU and GPU.\nLet’s dive into the contents of fx_graph_runnable.py, which provides another view of the computational graph. This file is generated as part of the artifact collection during the compilation process, and it’s crucial for understanding how PyTorch turns your high-level code into something that can actually run on your hardware.\nHere’s what the file looks like:\nimport torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf import torch._inductor.inductor_prims import torch._dynamo.config import torch._inductor.config import torch._functorch.config import torch.fx.experimental._config torch._functorch.config.debug_partitioner = True torch._functorch.config.unlift_effect_tokens = True isolate_fails_code_str = None # torch version: 2.4.1+cu121 # torch cuda version: 12.1 # torch git version: 38b96d3399a695e704ed39b60dac733c3fbf20e2 # CUDA Info: # nvcc: NVIDIA (R) Cuda compiler driver # Copyright (c) 2005-2024 NVIDIA Corporation # Built on Wed_Aug_14_10:10:22_PDT_2024 # Cuda compilation tools, release 12.6, V12.6.68 # Build cuda_12.6.r12.6/compiler.34714021_0 # GPU Hardware Info: # NVIDIA GeForce RTX 4060 : 1 from torch.nn import * class Repro(torch.nn.Module): def __init__(self): super().__init__() def forward(self, arg0_1, arg1_1): mm = torch.ops.aten.mm.default(arg1_1, arg0_1); arg1_1 = arg0_1 = None amax = torch.ops.aten.amax.default(mm, [1], True) sub = torch.ops.aten.sub.Tensor(mm, amax); mm = amax = None exp = torch.ops.aten.exp.default(sub); sub = None sum_1 = torch.ops.aten.sum.dim_IntList(exp, [1], True) div = torch.ops.aten.div.Tensor(exp, sum_1); exp = sum_1 = None return (div,) def load_args(reader): buf0 = reader.storage(None, 40000) reader.tensor(buf0, (100, 100), is_leaf=True) # arg0_1 buf1 = reader.storage(None, 40000) reader.tensor(buf1, (100, 100), is_leaf=True) # arg1_1 load_args._version = 0 mod = Repro() if __name__ == \u0026#39;__main__\u0026#39;: from torch._dynamo.repro.after_aot import run_repro with torch.no_grad(): run_repro(mod, load_args, accuracy=False, command=\u0026#39;run\u0026#39;, save_dir=None, tracing_mode=\u0026#39;real\u0026#39;, check_str=None) # To run it separately, do # mod, args = run_repro(mod, load_args, accuracy=False, command=\u0026#39;get_args\u0026#39;, save_dir=None, tracing_mode=\u0026#39;real\u0026#39;, check_str=None) # mod(*args) By now you should be understaing this quite easily. Additionally to the Repro class (which is simply a runnable nn.Module of the computational graph), there\u0026rsquo;s an argument loader function, called load_args and some useful debugging information in the comments, like CUDA version and underlying hardware.\nNow, let’s take a look at the output code file (output_code.py), which contains the actual machine code generated by TorchInductor. Although the file is quite long, here’s a snippet that highlights a key portion:\ncpp_fused__softmax_0 = async_compile.cpp_pybinding([\u0026#39;const float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;, \u0026#39;float*\u0026#39;], \u0026#39;\u0026#39;\u0026#39; #include \u0026#34;/tmp/torchinductor_dwarez/sk/cskh5dx62fglpphcrl6723dnmowdabouerrzy3dmqcngbxwfa7bv.h\u0026#34; extern \u0026#34;C\u0026#34; void kernel(const float* in_ptr0, ... What’s Happening Here?\nC++ to Python Binding: This snippet shows the generation of a C++ binding for the fused softmax operation. Here, PyTorch has automatically compiled the softmax function into efficient C++ code, and this function will be called directly during execution.\nAsynchronous Compilation: The async_compile.cpp_pybinding function allows this C++ kernel to be invoked asynchronously from Python, ensuring that the CPU or GPU isn’t sitting idle waiting for Python’s GIL (Global Interpreter Lock).\nFused Operations: Notice that the softmax operation has been fused into a single C++ function. Operation fusion is one of the key optimizations that TorchInductor applies to improve performance by reducing memory bandwidth and kernel launch overhead.\nBenchmarking: Is the Compiled Function Actually Faster? # Let\u0026rsquo;s put theory into practice and check if the compiled function outperforms the eager-mode function. We’ll use PyTorch\u0026rsquo;s torch.cuda utilities to measure the execution time of both. Here’s the code:\nimport numpy as np import torch N_ITERS = 10 def timed(fn): start = torch.cuda.Event(enable_timing=True) end = torch.cuda.Event(enable_timing=True) start.record() result = fn() end.record() torch.cuda.synchronize() return result, start.elapsed_time(end) / 1000 # Define a simple function def simple_fn(x, y): z = torch.matmul(x, y) return torch.nn.functional.softmax(z, dim=1) def generate_data(): return ( torch.randn((100, 100)).to(torch.float32).cuda(), torch.randn(100, 100).to(torch.float32).cuda(), ) # Compile the function using torch.compile() with TorchInductor backend compiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;) eager_times = [] for i in range(N_ITERS): inp = generate_data() with torch.no_grad(): _, eager_time = timed(lambda: simple_fn(inp[0], inp[1])) eager_times.append(eager_time) print(f\u0026#34;eager eval time {i}: {eager_time}\u0026#34;) print(\u0026#34;~\u0026#34; * 10) compile_times = [] for i in range(N_ITERS): inp = generate_data() with torch.no_grad(): _, compile_time = timed(lambda: compiled_fn(inp[0], inp[1])) compile_times.append(compile_time) print(f\u0026#34;compile eval time {i}: {compile_time}\u0026#34;) print(\u0026#34;~\u0026#34; * 10) eager_med = np.median(eager_times) compile_med = np.median(compile_times) speedup = eager_med / compile_med print( f\u0026#34;(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\u0026#34; ) print(\u0026#34;~\u0026#34; * 10) When running this script, you might see an output like this:\neager eval time 0: 0.018083839416503905 eager eval time 1: 6.585600227117538e-05 eager eval time 2: 2.332800067961216e-05 eager eval time 3: 1.8239999189972877e-05 eager eval time 4: 1.744000054895878e-05 eager eval time 5: 1.6383999958634378e-05 eager eval time 6: 1.6383999958634378e-05 eager eval time 7: 1.711999997496605e-05 eager eval time 8: 1.5231999568641186e-05 eager eval time 9: 1.535999961197376e-05 ~~~~~~~~~~ compile eval time 0: 0.8755618286132812 compile eval time 1: 0.00010204800218343735 compile eval time 2: 5.8816000819206235e-05 compile eval time 3: 4.831999912858009e-05 compile eval time 4: 4.790399968624115e-05 compile eval time 5: 4.1280001401901244e-05 compile eval time 6: 3.82080003619194e-05 compile eval time 7: 3.683200106024742e-05 compile eval time 8: 3.686400130391121e-05 compile eval time 9: 3.481600061058998e-05 ~~~~~~~~~~ (eval) eager median: 1.7280000261962412e-05, compile median: 4.45920005440712e-05, speedup: 0.3875134564748722x ~~~~~~~~~~ Surprisingly, the compiled function was slower than the eager-mode one, with a speedup less than 1 (0.39x). Let\u0026rsquo;s break down why this happens:\nLow Computational Complexity: The benchmarked function is too simple: it performs just a matrix multiplication followed by a softmax. These operations are lightweight, meaning they don’t provide enough work for the compilation optimizations to shine. The overhead introduced by the compilation process outweighs the benefits of the optimizations, resulting in slower execution.\nIn this case, there\u0026rsquo;s no significant performance gain from calling a fused softmax function because there’s nothing substantial to fuse—just a basic matrix operation and activation function.\nCompilation Overhead: Notice that the first couple of iterations of the compiled function are much slower than the rest. This is due to the overhead introduced by the compilation step. The TorchInductor backend needs time to analyze and compile the computational graph into optimized code. Once the graph is compiled, subsequent iterations are much faster because PyTorch uses the cached version of the graph, avoiding recompilation. So, if you run many iterations, the compilation time becomes less of a factor, and you’ll see the true performance gains.\nEager Mode’s Instant Execution: Eager mode is designed to run PyTorch operations immediately without any ahead-of-time optimizations, which is ideal for small, one-off operations like this. While compiled execution becomes faster over time, eager mode benefits from its simplicity and immediacy for lightweight tasks.\nWhile this example shows a slowdown, that’s because the workload is too small to benefit from TorchInductor’s optimizations. Compiled execution shines when dealing with larger models or heavier workloads. For a more substantial example where the compiled function outperforms eager mode, check out this link.\nGPU-Poor and Proud: Tweaking torch.compile Parameters for Fun (and Slower Functions) # Let\u0026rsquo;s try to tweak some torch.compile parameters and see what happens, shall we?\nFirst up, let\u0026rsquo;s play around with CUDA graphs. While CUDA graphs are great at reducing host-to-device communication overhead, that’s not exactly helpful when our function is so simple it might as well be considered an atomic operation. But hey, let\u0026rsquo;s try it anyway!\nWe can try this backend by simply specifying it in torch.compile\u0026rsquo;s arguments, like this:\ncompiled_fn = torch.compile(simple_fn, backend=\u0026#34;cudagraphs\u0026#34;) And here’s the \u0026ldquo;amazing\u0026rdquo; speedup result:\n(eval) eager median: 1.635199971497059e-05, compile median: 0.00019219200313091278, speedup: 0.08508158221251444x Oh no! It\u0026rsquo;s slower\u0026hellip; again! Turns out wrapping a super-simple function in a CUDA graph is like putting racing tires on a tricycle—you’re not going anywhere faster.\nAnother thing that we could try is to change the mode parameter. This is a way of specifying what should be the focus of the compilation. The default mode is a good balance between performance and overhead, but there are other modalities like reduce-overhead which relies on CUDA graphs or max-autotune, which also relies on CUDA graphs and Triton based matrix multiplications and convolutions.\nSo naturally, let\u0026rsquo;s crank it up to \u0026ldquo;max\u0026rdquo; with:\ncompiled_fn = torch.compile(simple_fn, backend=\u0026#34;inductor\u0026#34;, mode=\u0026#34;max-autotune\u0026#34;) Drumroll, please\u0026hellip; and the results:\n(eval) eager median: 1.5343999955803157e-05, compile median: 4.190399870276451e-05, speedup: 0.36617030428627984x Once again, we\u0026rsquo;ve hit a slowdown. But wait, what\u0026rsquo;s this warning?\nW0924 11:37:01.192000 123584511351680 torch/_inductor/utils.py:977] [0/0] Not enough SMs to use max_autotune_gemm mode Uh-oh. PyTorch is warning me that I don’t have enough SMs. Let’s check the code that triggered this passive-aggressive insult from PyTorch:\n@functools.lru_cache(None) def is_big_gpu(index) -\u0026gt; bool: min_sms = 68 # 3080 avail_sms = torch.cuda.get_device_properties(index).multi_processor_count if avail_sms \u0026lt; min_sms: log.warning( \u0026#34;Not enough SMs to use max_autotune_gemm mode\u0026#34;, extra={\u0026#34;min_sms\u0026#34;: min_sms, \u0026#34;avail_sms\u0026#34;: avail_sms}, ) return False return True So, PyTorch basically looked at my hardware and said, \u0026ldquo;Sorry, but you\u0026rsquo;re too GPU-poor to hang with the big boys.\u0026rdquo; Not cool, PyTorch, not cool at all.\nAnd with that, I’m officially done with this article!\nConclusion: Closing the Incision on torch.compile # After a deep surgical dive into the world of torch.compile, we\u0026rsquo;ve explored key components like Torch Dynamo, Torch Inductor, CUDA graphs, and more—each acting like specialized tools in the operating room of PyTorch optimization. Dynamo handles the \u0026ldquo;diagnosis,\u0026rdquo; tracing and transforming eager-mode operations into a computational graph, while Inductor steps in like a skilled surgeon to optimize that graph and generate fast machine code for CPUs and GPUs. CUDA graphs reduce the back-and-forth between the host and the GPU, making the \u0026ldquo;patient\u0026rdquo; function more efficient by minimizing communication.\nHowever, as our benchmarks revealed, not every function needs major surgery. Sometimes, a simple bandage is better than complex procedures. In fact, when we tried to optimize a lightweight function, the compilation overhead actually made it slower. And yes, it turns out being GPU-poor can hold you back, as our GPU didn\u0026rsquo;t have the strength for max-autotune—PyTorch’s gentle reminder that sometimes, you just need a more powerful toolkit.\nIn the end, optimization is like surgery—use the right tool for the right task, and remember, not every patient needs to be on the table!\nNow, if you’ll excuse me, I’m off to cry over my GPU specs. Bye!\n","date":"1 September 2024","externalUrl":null,"permalink":"/posts/torch-compile/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003eYou can take a look at the GitHub repository of this blogpost \u003ca href=\"https://github.com/DWarez/torch_compile_blogpost\" target=\"_blank\"\u003e\u003cstrong\u003eat this link\u003c/strong\u003e\u003c/a\u003e   \n\n  \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\u003c/p\u003e","title":"Dissecting torch.compile: Surgical Precision in PyTorch Optimization","type":"posts"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/torch-compile/","section":"Tags","summary":"","title":"Torch-Compile","type":"tags"},{"content":"Hello, fellow surgeons! How is life treating you? I hope you\u0026rsquo;ve spent your vacation relaxing, far, far away from the tools of our trade. After all, a good surgeon needs to rest after a long year of work and learning, right? With that in mind, I\u0026rsquo;ve chosen a simple yet useful topic to discuss today, so you can stay relaxed and not worry about the tremendous complexity of our field—at least for now.\nI\u0026rsquo;m sure you\u0026rsquo;ve come across the term RAG at least once. It stands for Retrieval-Augmented Generation, a technique that\u0026rsquo;s become quite popular these days. Its popularity stems from the fact that RAG systems are relatively simple to implement yet highly effective in terms of performance, all while keeping infrastructure costs reasonably low.\nDon\u0026rsquo;t worry, you won\u0026rsquo;t need your gloves today. This incision will be quick and easy, and we won\u0026rsquo;t go too deep—no risk of getting blood on you!\nPrepping the Instrument: Understanding RAG # Imagine this scenario: you\u0026rsquo;re a med student who has, over the years, compiled a vast set of notes from your courses, covering all the topics you\u0026rsquo;ve studied. You’re confident that this material will help you become a great surgeon! But there\u0026rsquo;s a problem: the sheer volume of this knowledge base makes it difficult to query. Even with well-organized notes, pinpointing the exact information you need can be a long and tedious task.\nPressed for time and with many questions that need answering, you turn to a large language model (LLM) for help. You start a conversation with a free conversational agent, but soon realize that the answers are not precise enough. Maybe the responses are too generic, or in some cases, they’re completely off the mark.\nThen an idea hits you: wouldn\u0026rsquo;t it be nice if you could incorporate your own information into the model?\nSure, one way to do it is through fine-tuning: all you need to do is prepare your data into a dataset, select a pretrained, open-weights model, set up the training and evaluation scripts, pay for the infrastructure and computing power, and finally—after a few days and several thousand dollars—you’ll have your fine-tuned model! Easy, right? What do you mean you don’t want to spend thousands of dollars fine-tuning a model?! Don’t you know AI is just for the rich?\nWell, if you’re GPU-poor, you can try building a RAG system. It’s quite straightforward. First, take your knowledge base and embed it into a vector database. Then, when querying your LLM, simply add the top-k most similar documents of your knowledge base to the prompt, based on the input question. By doing this, you’re injecting knowledge that the LLM might not be aware of, helping it reason better and providing a more accurate answer.\nI like to define a RAG system as a component in a conversational pipeline that extends the knowledge base of an LLM by using the prompt, all while leaving the model’s weights unchanged. As you can imagine, this technique is much cheaper and faster to prototype compared to model fine-tuning.\nOf course, we don’t expect the full answer to a question to be contained within the retrieved documents. Instead, we assume that these documents will provide relevant information about the question, thereby aiding the agent’s reasoning process.\nSuturing the Code: Implementing RAG # Now that we have a solid understanding of what a RAG system is, let’s dive into some code that demonstrates this technique. It might seem a bit unconventional, but this time I’ll be using Python instead of CUDA! Remember, this is a relaxed article!\n🚀 I started using uv as my go-to Python package manager. I highly suggest you to try it, it\u0026rsquo;s blazing fast!\nStoring the Memories # It may seem incredible, but a vector database is exactly what it sounds like—a database for vectors. Yes, I know, take a moment to let that sink in. What do you mean it was obvious? Well, I guess you\u0026rsquo;re right. Anyhow, we’ll be using Qdrant, a blazing-fast vector database, written in Rust. It’s very easy to set up, thanks to their Docker image. Simply pull the image and make sure to install their Python client. For example:\nuv add qdrant-client The great thing about vector databases is that you can associate each vector with a payload, which is essentially a set of information about that data point. In our RAG case, we’ll map the text embedding to the text itself, allowing us to quickly find texts similar to the user’s prompt.\nSince I’m not solving a specific problem here, I didn’t have any particular data to put in the vector database. So, I created a character named Mr. Fat Raccoon and generated some sentences about him. Here’s our example knowledge base:\nimport pandas as pd data = pd.DataFrame({ \u0026#39;text\u0026#39;: [ \u0026#34;Mr. Fat Raccoon was born in Trash City.\u0026#34;, \u0026#34;This raccoon, known as Mr. Fluffy, is 80cm long.\u0026#34;, \u0026#34;Mr. Fat Raccoon weighs 27 kgs.\u0026#34;, \u0026#34;In Trash City, a raccoon named Mr. Fat Raccoon was born.\u0026#34;, \u0026#34;At 80cm long, Mr. Fat Raccoon is quite large.\u0026#34;, \u0026#34;Weighing 27 kgs, Mr. Fat Raccoon is one hefty raccoon.\u0026#34;, \u0026#34;Trash City is the birthplace of Mr. Fat Raccoon.\u0026#34;, \u0026#34;Mr. Fat Raccoon, a native of Trash City, is known for his 80cm length.\u0026#34;, \u0026#34;The weight of Mr. Fat Raccoon is 27 kgs.\u0026#34;, \u0026#34;Born in Trash City, Mr. Fat Raccoon is 80cm long and weighs 27 kgs.\u0026#34; ] }) Now that we have the data, it’s time to populate the database. First, we instantiate the embedding model, which will be used to generate text embeddings. Then, we create a collection in Qdrant, where we will insert our data points.\nWhen creating the collection, we must specify its name, the vector size—which corresponds to the size of the embeddings and is therefore model-dependent—and the type of distance metric we want to use. For this example, we’ll use cosine distance.\nEach data point will be represented by its embedding and will include the original text in its payload. Here’s the code to do that:\nfrom qdrant_client import QdrantClient, models from sentence_transformers import SentenceTransformer client = QdrantClient(url=\u0026#34;http://localhost:6333\u0026#34;) collection_name = \u0026#34;raccoon_info\u0026#34; embedding_model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) def create_knowledge_base(): if collection_name not in client.get_collections().collections: client.recreate_collection( collection_name=collection_name, vectors_config= models.VectorParams( size=384, distance=models.Distance.COSINE ) ) embeddings = embedding_model.encode(data[\u0026#39;text\u0026#39;].tolist()) points = [ models.PointStruct(id=idx, vector=embedding.tolist(), payload={\u0026#34;text\u0026#34;: row[\u0026#34;text\u0026#34;]}) for (idx, row), embedding in zip(data.iterrows(), embeddings) ] client.upsert( collection_name=collection_name, points=points ) print(\u0026#34;Data inserted into Qdrant collection successfully.\u0026#34;) Great, we now have our knowledge base embedded in a vector database! We can proceede with our experiment.\nAsking about the unknown # Now, let’s choose an LLM and ask it about Mr. Fat Raccoon. What do you expect the output to be? I’m fairly confident the model will hallucinate, but let’s find out.\nI decided to use Microsoft’s Phi-3.5-mini-instruct, just to ensure we’re working with a good conversational model.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\u0026#34;microsoft/Phi-3.5-mini-instruct\u0026#34;, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(\u0026#34;microsoft/Phi-3.5-mini-instruct\u0026#34;, trust_remote_code=True) Here’s the function we’ll call to generate the response:\ndef ask_question(prompt: str, inject_knowledge: bool = True) -\u0026gt; str: if inject_knowledge: prompt_vector = embedding_model.encode(prompt).tolist() # type: ignore search_result = client.search( collection_name=collection_name, query_vector=prompt_vector, limit=2, ) injection = \u0026#34;Considering that: \\n\u0026#34; + \u0026#34;\\n\u0026#34;.join([point.payload[\u0026#34;text\u0026#34;] for point in search_result]) # type: ignore prompt = injection + prompt inputs = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;) output = model.generate(**inputs, max_length=128, do_sample=True, temperature=0.1) return tokenizer.decode(output[0], skip_special_tokens=True) ⚠️ Don\u0026rsquo;t be an amateur! Be sure to always use the same model for both the embedding and retrieving phase!\nLet’s give it a try:\nprint(ask_question(\u0026#34;What is double the weight of Mr. Fat Raccoon?\u0026#34;, inject_knowledge=False)) The result is as follows:\nFirst, we need to calculate the weight of Mr. Fat Raccoon. We know that Mr. Fat Raccoon weighs 30 pounds more than Mr. Scary Raccoon. Since Mr. Scary Raccoon weighs 30 pounds, we add this to Mr. Fat Raccoon\u0026rsquo;s weight. So, Mr. Fat Raccoon\u0026rsquo;s weight = Mr. Scary Raccoon\u0026rsquo;s weight + 30 pounds\nWell, that’s disappointing—the model is just spitting out nonsense! First of all, Mr. Fat Raccoon weighs 27 kilos (we’re not confused Americans here; we use the metric system). And who on earth is Mr. Scary Raccoon?\nNow, let’s see what happens when we add information to the prompt using RAG.\nInjecting the Insight: Retrieving Data for the Prompt # As you can see from the previous code snippet, when setting the inject_knowledge parameter to True, the pipeline changes slightly. First, we use the embedding model to embed the input question. Then, we retrieve the top-k results from our vector database (in this case, the top two). The payload of the most similar data points is then injected into the user prompt.\nLet’s see if things change when using RAG:\nprint(ask_question(\u0026#34;What is double the weight of Mr. Fat Raccoon?\u0026#34;, inject_knowledge=True)) To find double the weight of Mr. Fat Raccoon, we simply multiply his weight by 2: 27 kgs * 2 = 54 kgs Double the weight of Mr. Fat Raccoon is 54 kgs.\nThat’s spot on! Well done, Phi-3.5! Now, let’s check the prompt that generated this response:\nConsidering that: The weight of Mr. Fat Raccoon is 27 kgs. Mr. Fat Raccoon weighs 27 kgs. What is double the weight of Mr. Fat Raccoon?\nAs you can see, our RAG system successfully inserted relevant information into the prompt, enabling the model to respond correctly. Easier said than done!\nWrapping Up the Operation: Final Thoughts # Well, there you have it, folks! We’ve successfully implemented a RAG system, enhancing our LLM’s ability to provide accurate responses by injecting relevant information into the prompt. By leveraging a vector database like Qdrant, we avoided the costly and time-consuming process of fine-tuning, all while improving the model’s performance.\nRemember, not every procedure requires a complex or expensive solution. Sometimes, a well-placed stitch—in this case, a bit of embedded knowledge—is all it takes to get the job done right. So next time you find yourself with a data-heavy problem, consider RAG as your go-to surgical tool.\nUntil next time, keep your scalpels sharp and your models smarter!\n","date":"29 August 2024","externalUrl":null,"permalink":"/posts/ten-minutes-to-rag/","section":"Posts","summary":"\u003cp\u003eHello, fellow surgeons! How is life treating you? I hope you\u0026rsquo;ve spent your vacation relaxing, far, far away from the tools of our trade. After all, a good surgeon needs to rest after a long year of work and learning, right? With that in mind, I\u0026rsquo;ve chosen a simple yet useful topic to discuss today, so you can stay relaxed and not worry about the tremendous complexity of our field—at least for now.\u003c/p\u003e","title":"A quick incision: ten minutes to RAG","type":"posts"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag","type":"tags"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/vector-db/","section":"Tags","summary":"","title":"Vector-Db","type":"tags"},{"content":"Being a Machine Learning Surgeon is not an easy life. We not only have to deal with intricate machine learning systems but also navigate the additional complexities surrounding them. To be a proficient ML Surgeon, we must develop a diverse skill set. First and foremost, we need a deep understanding of machine learning and deep learning. Additionally, we must be adept at writing software, building infrastructures to host and integrate models, managing large volumes of data, and much more. This requires familiarity with numerous tools.\nOne fundamental tool in our toolkit is the profiler. Profiling involves performing dynamic code analysis to illustrate various performance indicators, such as memory usage and computational complexity.\nWhen dealing with GPU programming, profiling becomes even more crucial. Our main objective is to maximize performance by utilizing hardware resources as efficiently as possible. Given the unique architecture of GPUs and their specific characteristics, writing a perfect kernel that leverages all possible optimizations is a challenging task.\nFortunately, tools are available to help us with this difficult task. Today, we will explore how to use NVIDIA Nsight Compute to profile kernels and understand how we can optimize them.\nThis time, you won\u0026rsquo;t need any scalpels, so sit back and relax!\nPreparing for Surgery # Before we can proceed with our profiling sessions, we need two things: the profiling tool and a kernel to profile.\nI particularly enjoy using NVIDIA Nsight Compute —a tool developed by NVIDIA— because it provides an excellent interface to the CLI programs of the NVIDIA CUDA Toolkit. It offers a wealth of information and insights about the profiled kernel or application. The tool is very easy to install; simply follow the installation guide.\nFor this introductory tutorial, I\u0026rsquo;ll profile some matrix multiplication kernels, which are slightly modified versions of the ones you can find in this previous blogpost.\nTo illustrate the profiler\u0026rsquo;s capabilities, we will analyze a poorly performing kernel. This will result in a much more interesting and insightful report!\nInitiating the Procedure # After opening our project, we can click on Open Connection Dialog to configure the profiling activity we are going to launch, as shown in the figure below:\nConnection Dialog interface As you can see, this menu is split into two parts: platform-specific settings on the top and activity specifics on the bottom.\nFor the Platform Settings, we simply specify which executable to use for profiling the kernel. An executable must be launched for the kernel to be profiled. In my case, since I\u0026rsquo;m using Windows (I know, I know, but I don\u0026rsquo;t have a Linux machine at the moment), I\u0026rsquo;ll specify the .exe filepath in the Application Executable parameter while leaving everything else unchanged.\nThe Activity Settings are more interesting and should be tweaked based on the metrics we are interested in.\nFirst things first, we specify the Output Path, which is where the .rep (report) file will be stored.\nNext, we can select the Replay Mode. Replaying parts of the code during the profiling procedure is necessary because not all metrics can be computed in a single execution. For this reason, different replay modes (or strategies) can be applied, depending on the intended metrics we want to obtain and the behavior of the application. In kernel replay, the kernel is executed multiple times, for application replay, the whole application is run multiple times, while for range replay only a specified range of CUDA APIs are replayed. You can check the exact behavior on the official NVIDIA docs. For our case, let\u0026rsquo;s set the replay mode to kernel.\nThe Graph Profiling option specifies if the CUDA execution graph should be seen as a single workload or a set of individual kernel nodes. For our example, let\u0026rsquo;s leave this option set to node.\nLastly, let\u0026rsquo;s navigate to the Metrics tab and select detailed from the checkbox menu. This will ensure that addtional metrics will be collected, including the ones relative to memory usage. With this setting, a total of 459 metrics will be collected in the report —a substantial amount!\nThere are many more options available, but we\u0026rsquo;ll focus on the essentials for now. Let\u0026rsquo;s start profiling!\nDiagnosing the Patient # Initial Assessment # When we open the report, the first thing we encounter is a summary of the profiling activity, as shown in the picture below. This summary provides a wealth of useful information. Let\u0026rsquo;s break it down.\nSummary of the report ID: an unique value given to the activity in the context of the project.\nEstimated Speedup: this predicts how much faster the kernel can run with appropriate optimizations. In our case, a potential 43.75% speedup is estimated—quite significant! This is expected since we intentionally used a slow kernel for profiling.\nFunction Name and Demangled Name: these columns show the kernel\u0026rsquo;s name and input parameters from the source code, useful when the source code isn\u0026rsquo;t readily available.\nDuration: the elapsed GPU time.\nRuntime Improvement: the absolute improvement in kernel duration corresponding to the estimated speedup.\nCompute Throughput and Memory Throughput: these metrics provide insights into the efficiency of different kernel implementations.\n# Registers: the number of registers allocated per thread, important for checking occupancy.\nThe last two columns of the summary store the Grid Size and Block Size. As you can see from the example, the kernel has a block size of (6,6,1), which is defenitely sub-optimal. Could it be one of the reasons of the kernel\u0026rsquo;s inefficiency?\nAt the top of the summary, additional information about the run is provided, such as hardware used, execution time, and number of cycles. In this example, I ran the experiment on a GTX 4060—yes, I’m GPU-poor!\nAdditional information about the profiling activity At the bottom of the page, the profiler offers suggestions for improving the kernel, along with estimated speedups for each fix. Let\u0026rsquo;s examine the first suggestion:\nNote: If something is not clear to you, please read the following article.\nThat\u0026rsquo;s excellent advice! The tool not only suggests effective ways to enhance our kernel\u0026rsquo;s performance but also educates us in the process.\nThere are two additional suggestions from the profiler, but I won\u0026rsquo;t show them here as they relate to the aforementioned block size issue. Solving the block size problem will also address the other issues.\nIn-Depth Analysis # Let\u0026rsquo;s now navigate to the Details tab. Here, we can find tables containing all the numerical values for the various metrics collected during the profiling activity. The number of metrics displayed in this tab depends on the options selected in the Connection Dialog before starting the profiling activity. If you don\u0026rsquo;t see memory metrics, ensure you selected the detailed option, as mentioned earlier.\nWe won\u0026rsquo;t go through all the details here, but let\u0026rsquo;s examine a few key tables.\nIn the GPU Speed of Light Throughput table below, we can see compute and memory throughput information. The profiler also provides analytics to help us understand what\u0026rsquo;s happening at the hardware level. In this case, the compute throughput is much higher than the memory throughput, indicating that the kernel is compute-bound. This means arithmetic operations dominate the execution time. We can actually see that this is true through the Compute Workload Analysis, which shows us insight on the compute workload:\nAt the same time, the low memory throughput suggests inefficient memory access patterns in the kernel. Spoiler alert: this inefficiency is intentional for demonstration purposes. I\u0026rsquo;m not that awful at writing kernels!\nThe Memory Workload Analysis table is great to understand what is happening at the memory level. This analysis shows us different critical aspects of the kernel. First and foremost, the kernel performs operations only using the Global Memory, which is slow! Thankfully, the L1 cache has a good hit rate, of about 87%. The L2 cache, instead, is completely wasted: less than 50% hit rate an 0% compression ratio.\nThere are many, many more metrics we could check, but that\u0026rsquo;s out of the scope of this article. Feel free to explore and learn on your own!\nExamining the Source # In the Source tab, we find the original source code of the application alongside the corresponding assembly code, available in both SASS and PTX representations.\nQuoting the official NVIDIA documentation:\nPTX is a low-level parallel-thread execution virtual machine and instruction set architecture. PTX provides a stable programming model and instruction set for general purpose parallel programming, and is designed to be efficient on NVIDIA GPUs. High-level language compilers for languages such as CUDA and C/C++ generate PTX instructions, which are optimized for and translated to native target-architecture instructions.\nSASS is the low-level assembly language that compiles to binary microcode, which executes natively on NVIDIA GPU hardware.\nFrom this interface, it\u0026rsquo;s possible to quickly check the resulting assembly code by clicking on the lines of the source code. Not only that, we also have statistics and details about memory operations for each row of the original source code, which makes it easy to check the memory implications of each instruction!\nMoreover, the profiler highlights efficiency warnings near the lines that create bottlenecks. In this case, it indicates that we are performing excessive global memory accesses. This issue is explained in detail in our previous blog post about matrix multiplication.\nPost-Op Review # As a Machine Learning Surgeon, diagnosing and treating the performance ailments of your kernels is crucial. Profiling CUDA kernels with NVIDIA Nsight Compute is like having an advanced diagnostic tool in your surgical kit. It allows you to pinpoint exactly where the bottlenecks and inefficiencies lie, so that you will be guided on how to operate on the kernel to improve its performances.\nIn this blog post, we walked through the essential features of Nsight Compute, from setting up the connection dialog to analyzing key metrics and reports. By examining a poorly performing kernel, we demonstrated how to use these insights to prescribe the right optimizations and improve performance—akin to a successful surgical intervention.\nRemember, the profiler offers an extensive array of additional metrics and features that we haven\u0026rsquo;t covered here. Just as a surgeon continually learns new techniques, I encourage you to explore these tools further, experiment with different kernels, and refine your skills to ensure your models perform at their peak.\nHappy profiling, and may your kernels be ever efficient and healthy!\n","date":"15 July 2024","externalUrl":null,"permalink":"/posts/profiling-introduction/","section":"Posts","summary":"\u003cp\u003eBeing a Machine Learning Surgeon is not an easy life. We not only have to deal with intricate machine learning systems but also navigate the additional complexities surrounding them. To be a proficient ML Surgeon, we must develop a diverse skill set. First and foremost, we need a deep understanding of machine learning and deep learning. Additionally, we must be adept at writing software, building infrastructures to host and integrate models, managing large volumes of data, and much more. This requires familiarity with numerous tools.\u003c/p\u003e","title":"Performing Kernel Surgery: Profiling CUDA Kernels with NVIDIA Nsight Compute","type":"posts"},{"content":"","date":"15 July 2024","externalUrl":null,"permalink":"/tags/profiling/","section":"Tags","summary":"","title":"Profiling","type":"tags"},{"content":"During the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.\nThat project was crucial for me because it revealed an inconvenient truth: matrix multiplication is the most fundamental aspect of Machine Learning. Hence, I named my project \u0026ldquo;ML is ML\u0026rdquo;: Machine Learning is Matrix Multiplication. Although the course professor didn\u0026rsquo;t fully appreciate the title, it still earned me the highest grade. Because deep down, they knew it was true. It was, indeed, an inconvenient truth.\nNOTE: Yes, I\u0026rsquo;m well aware that tensor contraction is not the same as matrix multiplication. Still, the \u0026ldquo;ML is ML\u0026rdquo; joke only works when talking about matrices, so stick with it.\nHowever, this is not the place to explain why matrix multiplication is so fundamental to modern AI. Besides, if you\u0026rsquo;re reading this blog, you probably already know.\nInstead, as Machine Learning Surgeons, we should ask ourselves how such an important operation is implemented to fully utilize the power of GPUs. It\u0026rsquo;s easy to implement something, but it\u0026rsquo;s much harder to make it run fast! Just like humans need to train their reaction times to do tasks like driving an F1 car, we Machine Learning Surgeons must operate on the muscle fibres of kernels in order to make them fast and powerful!\nSo, put on your gloves, wield your scalpels and let\u0026rsquo;s start the operation!\nThe First Cut # Since you\u0026rsquo;re still a pratictioner, I\u0026rsquo;ll walk you through the simplest matrix multiplication kernel. But first, there is a basic concept that you have to grasp before proceeding with the code.\nMatrix Linearization # As you may have learned from the Hello CUDA article, when writing CUDA kernels, we typically use block and thread indices to select elements for computation. But what happens when dealing with higher-dimensional data structures, such as matrices? Moreover, how is memory organized on GPUs when dealing with such data structures?\nIn CUDA, memory on the device is managed linearly, meaning it is stored as a single, contiguous block of memory. Therefore, matrices are stored in this contiguous block of memory in a row-major order. Linearization involves mapping the multi-dimensional indices of a matrix to a single-dimensional index. It\u0026rsquo;s much easier to do than to explain.\nConsider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{\\mathrm{N} x \\mathrm{N}}\\) stored in a row-major order, which is the representation used by CUDA. The linear index of the element \\(\\mathrm{A}_{i, j}\\) (i-th row and j-th column of \\(\\mathrm{A}\\)) is simply given by \\(i * \\mathrm{N} + j\\). This is because in the row-major representation, the array that represents the 2D matrix is a continuous sequence of the rows of the matrix. Therefore, to acces the item \\(\\mathrm{A}_{i, j}\\) we must skip \\(i\\) rows, doing \\(i * \\mathrm{N}\\) and sum to it the column index \\(j\\).\nThis is a fundamental concept to understand before proceeding, as we will be performing numerous memory accesses using this technique.\nA Naive Kernel # Great! Now you are ready to fully understand the simplest matrix multiplication kernel. Keep in mind that I\u0026rsquo;ll avoid writing all the usual boilerplate code for instantiating variables on the host, moving data to the device, and printing results. If you have any doubts, you can check the full code here 1 2 3 4 5 6 7 8 9 10 11 12 __global__ void matMulKernel(float* C, float* A, float* B, int N) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; N \u0026amp;\u0026amp; col \u0026lt; N) { float value = 0; for (int k = 0; k \u0026lt; N; ++k) { value += A[row * N + k] * B[k * N + col]; } C[row * N + col] = value; } } You should be able to understand most of this code, but let\u0026rsquo;s quickly walk through it.\nIn lines 2-3, we define the index of the row and column of the matrix element that the thread will use for the dot product computation. Remember that the mathematical notation and the CUDA representation are inverted. While in mathematical notation we use the x-axis for referring to the rows and the y-axis for referring to the columns, in CUDA we use the y-components to compute the row index and the x-components to compute the column index.\nLine 5 simply checks the index boundaries. We must check this condition because we will likely spawn more threads than there are elements in the matrix, so some threads may compute out-of-bounds indices.\nLines 6-10 perform the actual dot product computation. We start by initializing a local variable to accumulate the result. Given that this is a local scalar variable, it will be stored in a register, the fastest type of memory available. This approach works well because the dot product, which involves summing the products of corresponding elements, can be computed incrementally. Remember this, as it will be essential for the upcoming optimization steps!\nLine 7 defines the loop that allows the thread to iterate over all elements of the rows of A and the columns of B. In each iteration, we accumulate the value by adding the product of the linearized row element from A and the linearized column element from B. If you find it difficult to visualize the linearizations, I suggest writing out a few loop iterations on a piece of paper.\nLastly, line 10 stores the computed dot product into the linearized element of C.\nFor completeness, I will also include the kernel invocation. Keep in mind that you can use the ceil() function for the gridSize\ndim3 blockSize(16, 16); dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (N + blockSize.y - 1) / blockSize.y); matMulKernel\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(d_C, d_A, d_B, N); Advanced Surgical Techniques # Now that we have a basic implementation, let\u0026rsquo;s pause to consider the kernel\u0026rsquo;s behavior.\nA subtle issue to notice is that all memory accesses we perform are done using Global Memory. If you need an explanation about memory types in CUDA, please read this blog post. In brief, Global Memory is much slower compared to other types of memory, such as Shared Memory. As a result, we are wasting time with these accesses.\nFurthermore, the kernel is actually performing more data loads than necessary. Consider the first thread of the grid, let\u0026rsquo;s call it \\(thread_{0,0}\\). This thread will compute the indexes \\((0,0)\\), therefore it will load the elements \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,0}\\) in its first iteration. Then, given that we perform a loop over the elements of the matrices, \\(thread_{0,0}\\) will also load \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,0}\\), \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,0}\\) and so on.\nLet\u0026rsquo;s now consider the second thread of the grid which we will call \\(thread_{0,1}\\). This kernel will load \\(\\mathrm{A}_{0,0}\\) and \\(\\mathrm{B}_{0,1}\\), then \\(\\mathrm{A}_{0,1}\\) and \\(\\mathrm{B}_{1,1}\\), then \\(\\mathrm{A}_{0,2}\\) and \\(\\mathrm{B}_{2,1}\\) and so on.\nNotice how element \\(\\mathrm{A}_{0,0}\\) is loaded by both kernels. Since Global Memory access is not shared between threads, each thread must perform a separate loading operation to obtain the value of the matrix element. In fact, in this example, it is easy to see how all elements of \\(\\mathrm{A}\\) are loaded by both kernels. I encourage you to continue this analysis on your own so that you can see for yourself that the same is true for all elements of \\(\\mathrm{B}\\) as well.\nIt\u0026rsquo;s now clear what we can do to optimize this kernel: use Shared Memory to improve the speed of loading operations, while also avoiding redundant loading operations within a block. Specifically, we will use tiling to manage the memory traffic reduction factor. If we assume 16x16 tiles, we can reduce the memory traffic by a factor of 1/16 compared to the naive implementation, because all threads will cooperate for the memory accesses. In general, given a tile size \\(T_N\\), we reduce the memory traffic by \\(1/T_N\\). If we take this idea to the extreme, by setting \\(N = T_N\\)we could load the entire matrix into a single tile in Shared Memory, thereby maximizing the reduction in memory traffic. However, this is rarely feasible. In real-world scenarios, the size of matrices is usually so large that the entire matrix cannot fit into the Shared Memory.\nUnderstanding the balance between tile size and Shared Memory usage is crucial for optimizing performance. While larger tiles can reduce memory traffic, they are limited by the available shared memory, and practical implementations must account for this constraint.\nAdvanced Incision Techniques: Tiling # NOTE: A bit of linear algebra ahead, proceed with caution.\nAs mentioned before, the dot product can be computed incrementally. We can leverage this property to split the computation of a single dot product into phases, which correspond directly to tiles. Let\u0026rsquo;s consider two matrices \\(\\mathrm{A} \\in \\mathrm{R}^{16x16}\\) and \\(\\mathrm{B} \\in \\mathrm{R}^{16x16}\\). We want to perform a matrix multiplication which result will be represented by \\(\\mathrm{C} \\in \\mathrm{R}^{16x16}\\).\nLet\u0026rsquo;s split \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\) into 4 tiles of size 4x4 each, let\u0026rsquo;s call them \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\) and \\(\\mathrm{T_B}_{i} \\in \\mathrm{R}^{4x4}, i = 0, 1, 2, 3 \\land j=0,1,2,3\\).\nAssume we want to compute the first element \\(\\mathrm{C}_{0,0}\\). In the previous kernel code, we fully loaded the first row of \\(\\mathrm{A}\\) and the first column of \\(\\mathrm{B}\\) and performed the dot product. Now, instead, we will use tiles.\nSince the dot product can be computed incrementally, we can procede as follows:\nLoad two tiles into memory. We load tiles based on the current phase. At the start, we are in phase 0, so we load \\(\\mathrm{T_A}_{0}\\) and \\(\\mathrm{T_B}_{0}\\).\nCompute all elements of the dot product that can be computed with the loaded tiles. Note that these are partial computations! The remaining parts of the computations will be performed in subsequent phases. In phase 0, we would compute:\n\\(\\mathrm{C}^{0}_{0,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{0,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{0,k} * \\mathrm{T_B}_{k,1}\\) \\(\\mathrm{C}^{0}_{1,0} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,0}\\) \\(\\mathrm{C}^{0}_{1,1} = \\sum_{k=0}^{3} \\mathrm{T_A}_{1,k} * \\mathrm{T_B}_{k,1}\\) Notice that the notation \\(\\mathrm{C}^{0}\\) indicates the computation of and element of \\(\\mathrm{C}\\) for the first phase, phase 0. Subsequent phases will sum upon this initial computation to obtain the final result.\nThen, increase the phase counter and repeat the process until all computational phases are complete. Note that the number of phases is \\(N/size\\_of\\_tile\\), where \\(N\\) is the dimension of the square input matrices.\nAfter all the computations for a point of \\(\\mathrm{C}\\) are done, store the result in the actual output matrix.\nIf you\u0026rsquo;ve made it through this notation hell, you\u0026rsquo;re ready to see the actual kernel code!\nBrain Cells Cooperating # An attentive reader might ask: How is this algorithm faster? From a mathematical and algorithmic perspective, the benefits of splitting computations and using tiling might not be immediately clear and can even seem to complicate matters. However, tiling is crucial for optimizing memory access patterns, as we discussed earlier. Let\u0026rsquo;s examine the code to understand how tiling enhances performance in practice:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 __global__ void matMulKernel(float* C, float* A, float* B, int N){ __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; Width/TILE_WIDTH; ++ph) { Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } C[row * Width + col] = Cvalue; } In lines 2-3, we define the two data structures used to load the tiles from \\(\\mathrm{A}\\) and \\(\\mathrm{B}\\). We use the __shared__ keyword to specify that this memory allocation will be performed in Shared Memory.\nIn lines 5-8, we define some variables to store the block and thread IDs for both the x and y dimensions. This is done for convenience, as writing these values explicitly each time would make the code less readable. Since we are not even close to register saturation, it’s a luxury we can afford. In lines 10-11, we simply compute the row and column indexes, since the kernel assumes that each thread will compute one element of \\(\\mathrm{C}\\).\nLine 15 starts the for loop that iterates over the computational phases. As mentioned before, there are \\(N/size\\_of\\_tile\\) phases in total. For simplicity, we assume that the dimension of the matrices is divisible by the tile size, so the division yields a whole number.\nFinally, lines 16-18 demonstrate thread cooperation. Here, the threads use the shared variables Ads and Bds to load the elements into the tiles for the current phase, ph. If you look closely at the index computations, you’ll see they are almost identical to those used in the naive implementation. The key difference is that here we skip elements by using the contribution ph * TILE_WIDTH. This adjustment is necessary because, at each phase, we have already performed the required computations using elements from the previous phases (and therefore the previous tiles). The picture below illustrates this behavior perfectly.\nVisualization of indexes computation for tiled matrix multiplication. Credits to the PMPP book Furthermore, line 18 introduces a barrier synchronization. Since threads are loading elements into memory asynchronously, we must ensure that all elements of the tiles for the current phase have been loaded before proceeding with the computation of the partial dot product.\nLines 20-22 perform exactly the operation listed as step 2 in the previous section, which is the computation of the partial dot product using the previously loaded tiled elements. This step also requires a barrier synchronization, as the elements stored in Ads and Bds will be overwritten in the next phase. We must ensure that all threads have finished the computation of the partial dot product for the current phase before proceeding to the next phase.\nFinally, in line 26, we write the computed value to the output matrix C.\nFlexible Incisions: Arbitrary Tile Size # In our previous kernel code, we assumed that the matrix size is divisible by the tile size. However, this is a strong assumption that may not always hold true. Is there a way to relax this assumption? Yes, through boundary checking.\nFirst, let\u0026rsquo;s examine what happens when the matrix size is not divisible by the tile size.\nThe first observation we make is that issues arise only with the last tile. For instance, consider a matrix \\(\\mathrm{A} \\in \\mathrm{R}^{16,16}\\) and tiles \\(\\mathrm{T_A}_{i,j} \\in \\mathrm{R}^{3x3}, i = 0,1,2,3,4,5 \\land j=0,1,2,3,4,5\\). For example, the tile \\(\\mathrm{T_A}_{0,0}\\) is within the bounds of the matrix, so it does not present any issues. However, a tile like \\(\\mathrm{T_A}_{5,5}\\) is problematic because it extends beyond the matrix boundaries. This issue occurs because 16 is not divisible by 6, therefore some tiles will go out of bounds.\nWhat exactly happens when we go out of bounds? If we access an element beyond the last row, we will end up accessing elements from subsequent rows due to the row-major linearization of the matrix. This will lead to incorrect results. When it comes to columns, accessing elements outside the allocated memory of the matrix can have various outcomes. We might load random values, load zeros, or even cause a kernel crash, depending on the system and its handling of such memory accesses.\nWe cannot solve this problem by simply not using the aforementioned tiles, as this would leave some elements unprocessed.\nTo address this, we can perform boundary checks to ensure that the computed x and y indexes for the thread are responsible for loading valid elements. The same checks apply to the output indexes. For rows, we check that \\(row \\text{\\textless} Width \\land (ph * TILE\\_WIDTH) \\text{\\textless} Width\\). For columns we check that \\((ph * TILE\\_WIDTH + ty) \\text{\\textless} Width \\land Col \\text{\\textless} Width\\). If these conditions are not met, we load a neutral value, such as 0.0, into shared memory for the operation, so that such elements will not effect the computations.\nLastly, we will store the value only if both the row and column indexes are within the bounds of the output matrix. Therefore, the kernel code is modified as follows:\n__global__ void matMulKernel(float* A, float* B, float* C, int Width) { __shared__ float Ads[TILE_WIDTH][TILE_WIDTH]; __shared__ float Bds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * TILE_WIDTH + ty; int col = bx * TILE_WIDTH + tx; float Cvalue = 0; for(int ph = 0; ph \u0026lt; ceil(Width/float(TILE_WIDTH)); ++ph) { if(row \u0026lt; Width \u0026amp;\u0026amp; ph * TILE_WIDTH + tx \u0026lt; Width) Ads[ty][tx] = A[row * Width + ph * TILE_WIDTH + tx]; else Ads[ty][tx] = 0; if (ph * TILE_WIDTH + ty \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) Bds[ty][tx] = B[(ph * TILE_WIDTH + ty) * Width + col]; else Bds[ty][tx] = 0; __syncthreads(); for(int i = 0; i \u0026lt; TILE_WIDTH; ++i) { Cvalue += Ads[ty][i] * Bds[i][tx]; } __syncthreads(); } if (row \u0026lt; Width \u0026amp;\u0026amp; col \u0026lt; Width) C[row * Width + col] = Cvalue; } Closing the Incision # We’ve come a long way in our exploration of matrix multiplication in CUDA, haven\u0026rsquo;t we? We began with the basics—our \u0026ldquo;first cut\u0026rdquo;—where we learned how to implement a straightforward matrix multiplication kernel. From there, we moved on to the exciting phase of optimization and refinement, focusing on making our GPU computations faster and more efficient. By leveraging shared memory with tiling, we achieved significant performance improvements. We also incorporated boundary checking to allow for flexible tile sizes.\nFun fact: adapting the boundary-checked, tiled version of the kernel to support rectangular matrices is surprisingly straightforward! Just follow these steps:\nReplace Width with unsigned integer arguments: j, k, l. Update Width for Matrix Heights and Widths: Replace Width with j where it refers to the height of matrix A or the height of matrix C. Replace Width with k where it refers to the width of matrix A or the height of matrix B. Replace Width with l where it refers to the width of matrix B or the width of matrix C. That\u0026rsquo;s it, you just wrote a general matrix multiplication CUDA kernel!\nWhat? You’re not convinced? You don\u0026rsquo;t trust your brilliant doctor?!\nI see that you want proof. You want some empirical data that proves that the optimizations we saw today actually worked! Fear not! In the next blog post, we’ll dive into the art of profiling CUDA kernels. I’ll guide you through the process of using tools like NVIDIA’s Nsight Compute to measure performance improvements, analyze memory usage, and make data-driven decisions about further optimizations. We’ll get down to the nitty-gritty details of profiling, so you can see exactly how your changes impact performance. It’s going to be a thrilling ride through the land of performance metrics and profiling!\nUntil then, happy coding, and remember to wash your hands!\n","date":"10 July 2024","externalUrl":null,"permalink":"/posts/matrix-multiplication/","section":"Posts","summary":"\u003cp\u003eDuring the first year of my Master\u0026rsquo;s Degree in Computer Science, I had to complete a project for a Machine Learning course. It involved implementing a small feed-forward neural network framework from scratch, using only numerical libraries and coding elements such as loss functions, backpropagation, and the feed-forward step.\u003c/p\u003e","title":"A Machine Learning Surgeon’s Toolkit: Advanced Matrix Multiplication in CUDA","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/matrix-multiplication/","section":"Tags","summary":"","title":"Matrix-Multiplication","type":"tags"},{"content":"","date":"14 June 2024","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":"Would you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function. Knowing the philosophy behind their architectural design, the problems they aim to solve, and the reasons for their specific construction is essential for leveraging their full potential.\nExploring such a vast and complex topic can indeed be challenging. Fortunately, we don\u0026rsquo;t need to be hardware engineers. We just need to firmly grasp the basics to understand how to fully utilize these processors in our machine learning endeavors.\nThe Battle of the Brains: CPU vs. GPU # I bet my scalpels that you are familiar with programming using a CPU. Indeed, every machine learning engineer (or software engineer) must be able to do so. However, many of you may not be fully aware of the CPU\u0026rsquo;s architectural design, perhaps because you never studied it during your academic journey. Still, I\u0026rsquo;m sure you can write perfectly functioning code.\nNowadays, a programmer can effectively write code that accomplishes tasks and solves problems without a deep understanding of the underlying CPU architecture. Modern programming practices and tools abstract away much of the complexity associated with hardware specifics. High-level programming languages and comprehensive development environments allow programmers to focus on algorithm design and functionality rather than hardware details. Additionally, compilers and interpreters perform automatic code optimizations, translating high-level code into efficient machine code tailored to the specific architecture. These optimizations ensure that the code runs efficiently on various CPUs, further diminishing the need for programmers to have in-depth knowledge of the hardware. Consequently, the ability to produce functional and efficient software relies more on logical and conceptual skills than on hardware-specific expertise.\nIn contrast to CPU programming, GPU programming necessitates a more intimate understanding of the underlying hardware to achieve optimal performance. This is because GPUs are designed with a fundamentally different architecture, emphasizing parallel processing capabilities. To leverage these capabilities effectively, programmers need to be aware of concepts such as thread hierarchies, memory coalescing, and warp execution. Unlike CPUs, where compilers can often perform extensive optimizations automatically, GPU programming requires manual optimization to fully exploit the hardware\u0026rsquo;s potential. The programmer must write code that efficiently manages thousands of parallel threads and minimizes memory access latency. Consequently, achieving high performance in GPU programming involves a deep understanding of the GPU\u0026rsquo;s architecture and the intricacies of its operation, making it significantly different from the more abstracted approach possible in CPU programming.\nThe figure below shows a comparison between the CPU and GPU architecture design, which is very helpful in understanding the purpose of both processors.\nFigure 1.1 of the PMPP book The CPU is designed for general-purpose processing and optimized for single-thread performance, enabling it to run a wide range of applications, from operating systems to specialized software. As illustrated in the figure, the CPU features a large cache memory and a few powerful cores. Additionally, it employs a hierarchical memory system, consisting of registers, multiple levels of cache, and main memory, ensuring rapid data access. To enhance its general-purpose processing capabilities, the CPU includes a complex instruction set, allowing it to perform a broad spectrum of operations.\nOn the other hand, the GPU is designed for parallel computing, following the Single Instruction Multiple Data (SIMD) paradigm. The GPU design aims to maximize throughput by utilizing thousands of cores that compute simple operations in parallel. For memory, GPUs use High Bandwidth Memory (HBM) to facilitate very fast data flows in and out of the processor, while maintaining unified memory to enable communication between threads.\nEnough with these generic notions. Put on your gloves, and let\u0026rsquo;s cut open a GPU!\nCerebral Cortex # The figure below illustrates the fundamental design of a modern GPU, primarily consisting of an array of Streaming Multiprocessors (SMs) and Global Memory.\nFigure 4.1 of the PMPP book Each SM contains a grid of CUDA cores, local memory, and a control unit. The distinction between local and global memory is crucial and must be considered when writing kernels, as loading and storing data between SMs and Global Memory can introduce significant overhead.\nBlock Scheduling # Recall that in CUDA we call kernels as follows (example from HelloCuda):\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); This results in the CUDA runtime spawning blocks of threads, which are assigned to individual SMs. In practice, each SM is responsible for executing multiple blocks. However, since each SM has a limited number of CUDA cores and memory, there is a limit to how many blocks can be assigned to it at a given time. Consequently, there is also a limit to how many threads can be executed simultaneously on a CUDA device. To manage this limitation, the CUDA runtime keeps track of all the spawned blocks and assigns new blocks to the SMs as they complete the execution of previous blocks.\nThe effort of assigning entire blocks to a single SM is well worth it. Since threads within the same block often need to communicate with each other, having them share the same SM allows them to use the SM\u0026rsquo;s internal memory. This approach avoids the need for global memory access, which requires more time-consuming load and store operations.\nAnother key advantage of dispatching entire blocks to a single SM is synchronization. Kernel logic often requires synchronization to ensure that all threads reach a certain execution point before continuing with the computation. Even if threads are started simultaneously, their execution times can vary, and perfect timing synchronization is practically impossible. In practice, the threads\u0026rsquo; execution order is random.\nFortunately, the CUDA APIs provide a straightforward way to perform barrier synchronization with a single command:\n__syncthreads() This command causes all threads in the block to wait at that point until every thread has reached the instruction. This ensures that all threads have completed the code preceding the barrier, allowing safe continuation with the subsequent code execution.\nWarps # As mentioned earlier, each SM contains a certain number of CUDA cores. Suppose that each core individually performes an instruction at a given time; in that case, the design would need to provide an instruction fetch/dispatch unit for each core, which would be inefficient given the large number of cores per SM. Additionally, to achieve significant throughput, the device must implement the SIMD paradigm.\nTo address these requirements, each SM is divided into processing blocks, each containing a certain number of cores and a single instruction fetch/dispatch unit to instruct the group of cores. The figure below illustrates an SM equipped with 16 CUDA cores, divided into 2 processing blocks.\nFigure 4.8 of the PMPP book To align the execution of kernels with this architectural implementation, warps were conceived. Whenever a block is assigned to an SM, it is further divided into warps, which are typically 32-thread units for NVIDIA GPUs. Note that the size of warps is implementation-specific and can vary between different architectures.\nA warp is essentially the unit of thread scheduling in SMs. The 32 threads in a warp are contiguous, meaning they have a contiguous range of thread IDs. For example, if a block generates only one warp during execution, that warp will contain thread IDs ranging from 0 to 31.\nWe must consider how thread IDs are assigned within a warp. For a one-dimensional grid, the partitioning is straightforward: the array of threads is simply split into N contiguous parts, each consisting of 32 threads. In the case of a two-dimensional grid, which forms a matrix, the array is first flattened into a one-dimensional array in a row-major order (i.e., first all elements of the first row, then all elements of the second row, and so on) and then split as the one-dimensional case. For three-dimensional grids, we fix the value for the z-axis and then flatten the array as we would for the two-dimensional case. This process is repeated for each value of the z-axis.\nControl Divergence # While the use of processing blocks and warps provides execution optimization, it also introduces a hidden problem.\nAs mentioned earlier, all cores in a processing block share the same fetch/dispatch unit. Consequently, all threads in a warp execute the same instruction at a given time. But what happens with branches?\nConsider a piece of code like:\nif (thread_id%2 == 0) { ... } else { ... } In this case, the execution of operations depends on the thread ID, so not all threads in a warp will be executing the same code simultaneously. Specifically, when threads with even thread IDs are computing the code inside the if brackets, threads with odd thread IDs will be idle, and vice versa.\nThis phenomenon is called control divergence, and it generally occurs whenever there is a control structure in the code that depends on the thread ID.\nControl divergence silently suggests that we should try to avoid such control structures when writing a kernel to prevent code execution from diverging and wasting time on idle threads. However, it\u0026rsquo;s not always possible to avoid these control structures, such as when checking the portion of data on which the thread must perform computations (similar to what we did in HelloCuda).\nHippocampus # When optimizing GPU kernels, it\u0026rsquo;s crucial to remember that performance improvements from processing optimizations are only part of the overall picture. A significant contribution also comes from efficient memory usage and effectively leveraging the GPU memory hierarchy.\nWith that in mind, let\u0026rsquo;s explore a high-level overview of the GPU memory architecture design.\nHierarchical Memory Design # Just as for the CPU design, the GPU architecture also specifies a hierarchical memory design, composed by several levels. As always, the size of the memory is inversely proportional to its speed and cost.\nGlobal Memory is the largest and slowest type of memory on a GPU. If you followed the Hello CUDA tutorial, you have already used it without even knowing. Both the host and the device have read and write access to this memory. The long access latencies and relatively low access bandwidth of the Global Memory derives from the fact that it is not an on-chip memory and it is also implemented using the DRAM technology. Constant Memory is similar to Global Memory but with one key difference: it supports high-bandwidth, read-only access by the device.\nGlobal Memory is also used to create Local Memory, which shares the same properties in terms of latency and access. However, Local Memory is reserved for a single thread, meaning it is not shared among threads. Each thread has its own portion of dedicated Global Memory, typically used to allocate static arrays, spilled registers, and other elements of the thread’s call stack.\nShared Memory is an on-chip memory that allows threads within the same block to share data efficiently. It is much faster than Global Memory and helps minimize latency by reducing the number of accesses to Global Memory. Its primary purpose is to facilitate high-speed data sharing and cooperation among threads.\nRegisters are another type of on-chip memory. They are the fastest memory type and are assigned to individual threads, storing the most frequently accessed data for each thread.\nLast but not least, Texture Memory is a specialized type of read-only memory in GPUs, optimized for two-dimensional spatial locality and specific access patterns commonly found in graphics rendering and image processing tasks. Its design provides significant performance benefits in various computational scenarios, utilizing techniques such as spatial locality and hardware interpolation/filtering. However, we will not delve too deeply into this type of memory here, as it is beyond the scope of this article.\nIt\u0026rsquo;s crucial to remember this hierarchy when writing a kernel, as the speed differences between these memory types are highly noticeable and have a significant impact on the overall performance of the kernel. In modern GPUs, the combined access bandwidth of all the register files across all Streaming Multiprocessors (SMs) is at least two orders of magnitude higher than that of global memory!\nMemory Types in CUDA # Now that we have a clear high-level overview of the different types of device memory, the next logical step is to understand how to utilize them effectively when writing kernels. In CUDA, variables are declared with specific properties that dictate the type of memory they will use. These declarations also define the scope and lifetime of each variable, allowing for precise control over memory usage and performance optimization.\nHere\u0026rsquo;s a quick overview of the syntax and the resulting behaviour.\nDeclaration Memory Type Scope Lifetime Automatic Variables (arrays excluded) Register Thread Grid Automatic Array Variable Local Thread Grid __device__ __shared__ int SharedVariable Shared Block Grid __device__ int GlobalVariable Global Grid Application __device__ __constant__ int ConstantVariable Constant Grid Application Let\u0026rsquo;s explain in detail each row of this table.\nSaying non-array automatic variables is just a fancy way of referring to scalars. Every variable of this type defined in a kernel or device function is automatically placed inside a thread register, meaning each thread will have its own version of the variable. Consequently, their lifespan is limited to the execution time of the thread. You already know that registers are extremely fast, but be careful not to allocate too many scalar variables, as this can exceed the maximum capacity of the register storage. Using too many registers can negatively affect the occupancy of each Streaming Multiprocessor (SM).\nArray variables are typically instead allocated in the Local Memory, however their scope and lifetime are the same of scalar variables.\nUsing the syntax __shared__, we specify that the variable must be shared among threads in the same block. Typically, a shared variable is declared inside the kernel function and used by threads within the same block to cooperate in computation. This is a convenient approach because Shared Memory is very fast.\nBy specifying only __device__, the variable will be allocated in Global Memory, making it a global variable. This means the variable is visible to all threads and persists throughout the entire application execution. Be cautious when dealing with global variables: the only robust way to ensure data consistency and synchronization across different blocks is to use atomic instructions.\nLastly, by using the __constant__ syntax, we can define a constant variable, which will be allocated in Constant Memory. These variables are defined outside any function body, are visible to all threads in the grid, and their lifespan matches that of the application. This type of variable is typically used to provide constant inputs or data to kernels, as the variables\u0026rsquo; values cannot be modified by the kernels.\nConclusion # I hope you enjoyed this dissection! By now, you should have gained a comprehensive high-level understanding of both the Cerebral Cortex and the Hippocampus of a GPU.\nThroughout our exploration, we delved into the intricacies of Streaming Multiprocessors (SMs), warps, control divergence, and various memory types inherent to a GPU\u0026rsquo;s architecture. These components collectively form the brain of a GPU, crucial for its processing power and efficiency.\nIt\u0026rsquo;s important to recognize the profound interconnection between these two brain regions. When developing GPU code, one must consider both the Cerebral Cortex and the Hippocampus to ensure optimal performance and functionality. Neglecting either could lead to inefficiencies or errors in your code—something no true surgeon of GPU programming would permit!\nThat wraps up today\u0026rsquo;s session! Remember to remove your gloves and wash your hands! Clear your mind and prepare for the next fascinating dive into the GPU programming world!\n","date":"14 June 2024","externalUrl":null,"permalink":"/posts/cpu-gpu-architecture/","section":"Posts","summary":"\u003cp\u003eWould you operate on a human body without knowing its organs? Similarly, how can you effectively write a GPU kernel without understanding the underlying hardware? This is why it\u0026rsquo;s crucial to understand how GPUs function. Knowing the philosophy behind their architectural design, the problems they aim to solve, and the reasons for their specific construction is essential for leveraging their full potential.\u003c/p\u003e","title":"Cerebral Cortex and Hippocampus: Understanding the Computational and Memory Design of GPUs","type":"posts"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/basics/","section":"Tags","summary":"","title":"Basics","type":"tags"},{"content":"","date":"6 May 2024","externalUrl":null,"permalink":"/tags/gpu-programming/","section":"Tags","summary":"","title":"Gpu-Programming","type":"tags"},{"content":"In case you didn\u0026rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\nWith the introduction of CUDA, everything changed. Its integration with languages like C, C++, or Python, along with the elimination of the need for knowledge in graphics programming, made GPU programming much more accessible.\nHowever, many individuals today still feel daunted by the complexity of GPU programming, both in terms of the language (which is too low-level compared to the most popular programming languages today) and the architecture and development knowledge required to write this kind of software.\nThis article aims to introduce you to this realm through an in-depth technical analysis of a HelloWorld, which is famously the starting point for every programmer who ventures into a new programming language. Just as a surgeon carefully dissects to reveal the inner workings of the human body, we\u0026rsquo;ll break down each component of CUDA\u0026rsquo;s basic architecture and functionality.\nSince we\u0026rsquo;re discussing GPU programming, the classic print(\u0026quot;hello world\u0026quot;) won\u0026rsquo;t be applicable. Instead, we\u0026rsquo;ll analyze a complete program that adds two vectors, using the GPU.\nPrerequisites # I will briefly present some prerequisites necessary to fully understand the remaining content of this post. Even if you don\u0026rsquo;t meet all of them, I still suggest you keep reading. I\u0026rsquo;m confident you can still enjoy the content and find it useful.\nBasic C/C++ knowledge. Given that CUDA is originally a superset of C/C++, it comes natural that you have to know the basics of these languages. Basic knowledge of a GPU architecture. I won\u0026rsquo;t examine deeply this topic here, but understanding how a GPU works and its computational paradigm is essential for grasping CUDA programming. Installation of CUDA on your machine to try out the code yourself. That\u0026rsquo;s about it.\nCode Dissection # In the link below, you will find the full code snippet for our HelloWorld program. I believe that reading the entire code first piques your interest, as you will likely have questions you want answered.\nIf this code seems intimidating to you, please don\u0026rsquo;t be afraid. I assure you that by the end of your reading, you will fully understand it and realize its simplicity.\nHere\u0026rsquo;s the code We are now ready to dissect this code, top to bottom.\nCUDA Error Handling # The code example begins with the definition of a macro, which will be utilized throughout the program.\n#define CUDA_CHECK_ERROR(err) \\ if (err != cudaSuccess) { \\ printf(\u0026#34;CUDA err: %s at line %d\\n\u0026#34;, cudaGetErrorString(err), __LINE__); \\ exit(EXIT_FAILURE); \\ } Within this macro, we utilize several CUDA-defined functions and symbols, such as cudaSuccess and cudaGetErrorString(cudaError_t). While these are fairly self-explanatory, let\u0026rsquo;s delve deeper into their significance.\nWe consider the err variable a cudaError_t type variable, intended to store errors encountered during execution. Within the macro, we check if the error status is cudaSuccess, indicating successful code execution. If this condition is not met, we invoke the cudaGetErrorString(cudaError_t) function to retrieve the error string, providing clarity on the encountered issue, and then we exit the code execution.\nWhile this is obviously the most basic way of doing error handling, it\u0026rsquo;s crucial to remember its necessity whenever utilizing CUDA APIs, as errors may arise during their invocation.\nThe Kernel # Now, we\u0026rsquo;ll delve into what is arguably the most intriguing segment of the code, which will unveil some pivotal technical insights into CUDA programming and hardware.\nTo begin, it\u0026rsquo;s essential to understand that the term \u0026ldquo;kernel\u0026rdquo; typically denotes functions that can be computed by the device. In this context, we\u0026rsquo;re referring to the code responsible for orchestrating the GPU\u0026rsquo;s addition of the two input vectors.\nLet\u0026rsquo;s now examine the function\u0026rsquo;s prototype.\n__global__ void vecAddKernel(float* A, float* B, float* C, int n) If you are familiar with the C programming language, you may have noticed that the keyword __global__ doesn\u0026rsquo;t come from the standard. It is in fact a keyword implemented by the CUDA programming language, which specifies the visibility of the function.\nFunction Execution Space # In the GPU programming paradigm, we distinguish between two entities: the host and the device. The host refers to the CPU along with its memory, while the device pertains to the GPU. Consequently, the CUDA programming language incorporates three function execution specifiers:\n__global__ denotes a kernel. The function is executed on the device, and is callable both by the host and the device (only for devices of compute capability 5.0 or higher). It\u0026rsquo;s crucial to note that functions of this type must return void. __device__ signifies code executed exclusively on the device. It can only be called by another device execution and not from the host. __host__ denotes code that can be called and executed solely by the host. This is the default execution space if no execution space is specified. Now that we understand what a function execution space entails, let\u0026rsquo;s briefly delve into the remainder of the prototype. As mentioned earlier, given that this function serves as a kernel, it must inherently return void. Consequently, we\u0026rsquo;ll need both the input (A, B) and output (C) vectors, as well as explicitly specify the length of the vectors (n).\nBlocks and threads # The subsequent lines of code may appear cryptic at first glance, as they encompasses several complexities. To unravel their intricacies, let\u0026rsquo;s begin with the fundamentals.\nint i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; n) { C[i] = A[i] + B[i]; } This article doesn\u0026rsquo;t delve deeply into GPU architecture, so I won\u0026rsquo;t discuss its overall structure here. However, it\u0026rsquo;s crucial to understand that GPUs operate on the SIMD (Single Instruction Multiple Data) paradigm. This allows GPUs to process multiple data in parallel, executing a single instruction.\nIn essence, the heart of GPU processing lies in threads. The kernel we\u0026rsquo;re defining will be executed by the device, which will use a certain number of threads to execute the kernel instructions.\nLet\u0026rsquo;s illustrate this with a practical example using the code snippet above. Suppose the length of both vectors is 100. We\u0026rsquo;ll write code to execute the kernel, allocating 100 threads. Each thread will be responsible for summing a specific position in the vectors.\nIn the code snippet, the position is indicated by the variable i. It\u0026rsquo;s crucial for each thread to have a distinct i value; otherwise, all threads would operate on the same position in the vectors. Fortunately, the CUDA API provides built-in thread-specific variables that help us compute the correct positional index. Specifically, we\u0026rsquo;re utilizing the following built-ins:\nthreadIdx: Represents the index of the thread currently executing the code. blockIdx: Denotes the index of the block containing the executing thread. blockDim: Specifies the size of each spawned block. A block is a group of threads that collaborate and communicate through shared memory. These blocks are scheduled and executed together by the hardware. Moreover, all blocks together form what is termed as a grid. This implies that the structure is inherently multi-dimensional: a grid has three dimensions—x, y, and z. Hence, when referencing the built-in variables threadIdx, blockIdx, and blockDim, we utilize their x attribute, which corresponds to the x-dimension of the grid. The rationale behind this will become clearer when we examine the kernel invocation.\nThe final piece to explain is the if statement preceding the actual sum computation. This if statement verifies that the computed index falls within the bounds of the input vectors. While this check may initially seem unnecessary, its significance will become apparent later on.\nHost code # Following the discussion on the kernel, let\u0026rsquo;s shift our attention to the host code of the program. Here, we\u0026rsquo;re tasked with memory allocation, kernel invocation, and error checking.\nDevice Memory # A crucial consideration is that device memory is distinct from host memory. Consequently, the device cannot directly operate on data residing in host memory. This necessitates the allocation of device memory and the transfer of data from the host to the device. While this may appear trivial, it\u0026rsquo;s one of the most significant bottlenecks in modern software, particularly when handling large amounts of data, leading to substantial communication overhead between the host and the device. Given this distinction, as a best practice, we use the notation V_h to indicate a variable used by the host and V_d to denote a variable used by the device.\nGiven the simplicity of our case study, we\u0026rsquo;ll overlook these technical intricacies and focus on accomplishing the bare minimum to ensure functionality.\nvoid vecAdd(float* A_h, float* B_h, float* C_h, int n) The vecAdd function accepts three host-vectors as arguments: A_h and B_h, which are to be summed, and C_h, which will store the result. It\u0026rsquo;s important to note that an output vector C is necessary since the kernel cannot directly return anything. Finally, n represents the length of the input vectors.\nOur initial step is to determine the size of the device-vectors. This computation follows a standard C approach:\nint size = n * sizeof(float) Next, we proceed to allocate memory on the device to store the device-vectors:\nfloat *A_d, *B_d, *C_d; cudaMalloc((void**)\u0026amp;A_d, size); cudaMalloc((void**)\u0026amp;B_d, size); cudaMalloc((void**)\u0026amp;C_d, size); Fortunately, the CUDA programming language provides us with the cudaMalloc function, which functions similarly to the malloc function but operates on the device.\nBefore invoking the kernel, the final step is to transfer data from the host to the device. At this stage, we have:\nOn the host, three vectors containing the data for computation On the device, three memory allocations initialized but lacking of the necessary data. To address this, we leverage another CUDA programming function, cudaMemcpy:\ncudaError_t error = cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); error = cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice); CUDA_CHECK_ERROR(error); We invoke the function with the following parameters: the destination pointer, the source pointer, the size to copy, and the direction. The parameter cudaMemcpyHostToDevice specifies that we intend to copy data from the host to the device. Additionally, observe how we utilize the previously defined macro to check for errors in case the memory copying operation fails.\nKernel Invocation # Assuming everything executes correctly, we\u0026rsquo;ve successfully copied the input vectors to the device memory. Hence, we can proceed to call the kernel.\nint number_of_threads = 256; dim3 dimGrid(ceil(n/number_of_threads), 1, 1); dim3 dimBlock(number_of_threads, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n); First, we define the number of threads we want to allocate per block. Next, we define two dim3 type variables, which represent 3-dimensional vectors typically used to specify grid and block sizes. Since we are dealing with a 1-dimensional problem, we only need to use the x-dimension.\nWe compute the grid size as the number of elements in the input vectors, n, divided by the number of threads per block. To ensure the grid dimension can accommodate all threads, even if n is not perfectly divisible by the number of threads per block, we use the ceil operator, which will round-up the division. The block dimension is simply set to the number of threads we want to allocate per block.\nLastly, we invoke the kernel, as specified in the previous sections.\nNotice two things:\nWe can now fully understand why we need to use an if statement in the kernel, as explained in the Blocks and threads. Let\u0026rsquo;s illustrate with a practical example. Suppose n = 100 and number_of_threads = 16. When computing the x-dimension of the grid size, the exact result is 6.25. However, if we set the grid x-size to 6, we won\u0026rsquo;t allocate enough threads because we\u0026rsquo;ll have 6 blocks of 16 elements each, totaling 96 threads, leaving out 4 positions of the vectors. On the other hand, if we set the grid size to 7, we\u0026rsquo;ll allocate more threads than the number of positions to compute because 7x16=112. We can\u0026rsquo;t allow the extra threads to access memory indicated by their int i = threadIdx.x + blockIdx.x * blockDim.x; because it would be out of bounds. Therefore, the necessity of using an if statement to check such boundaries.\nOften, tutorials do not explicitly allocate the dimGrid and dimBlock variables, but instead invoke the kernel directly like this: vecAddKernel\u0026lt;\u0026lt;\u0026lt;ceil(n/number_of_threads), number_of_threads\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, n). This syntax is valid because the dim3 data type automatically defaults unspecified dimensions to 1. However, I chose to specify all dimensions to clarify the explanation provided in the section Blocks and threads.\nStoring and cleaning up # After the kernel invocation, we need to retrieve the computation result. Since the kernel cannot return anything, it\u0026rsquo;s the programmer\u0026rsquo;s responsibility to fetch the output of the computation. We can easily accomplish this as follows:\nerror = cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost); CUDA_CHECK_ERROR(error); cudaFree(A_d); cudaFree(B_d); cudaFree(C_d); Once again, we utilize the cudaMemcpy function, but this time we specify cudaMemcpyDeviceToHost, indicating that we are transferring data from the device to the host. Notice also how the order of the host and device variables has switched in the function call compared to previous occurrences.\nFinally, we must free the memory that was used. This isn\u0026rsquo;t just a best practice—it\u0026rsquo;s an absolute necessity!\nMain # Just for completeness, let\u0026rsquo;s quickly go through the main function:\nint n = 512; float A_h[n], B_h[n], C_h[n]; for (int i = 0; i \u0026lt; n; i++) { A_h[i] = i; B_h[i] = i; } vecAdd(A_h, B_h, C_h, n); for (int i = 0; i \u0026lt; n; i++) { printf(\u0026#34;%g\\n\u0026#34;, C_h[i]); } return 0; There isn\u0026rsquo;t much to say here. We define the length of the vectors n, then allocate and initialize the input vectors A_h and B_h with a range from 0 to n-1. Next, we call the host code, which also invokes the kernel, and finally, we print the result stored in C_h.\nCompile and run # Compiling and running this example is straightforward. First, we compile the source as we would for a C file, but we use the CUDA compiler, called nvcc. Assuming the source is named hello_world.cu and we are in its parent directory, we can build the executable like this:\nnvcc hello_world.cu -o hello_world Then, we simply run the executable:\n./hello_world And we will receive the output (truncated for readability):\n0 2 4 ... 1018 1020 1022 Wrapping Up # In summary, we\u0026rsquo;ve explored the fundamentals of CUDA programming, which enables us to harness the power of GPUs for parallel computing tasks. We began by understanding the distinction between the host (CPU) and the device (GPU) and delved into CUDA\u0026rsquo;s function execution specifiers (__global__, __device__, __host__) that define where functions can be executed.\nWe discussed the importance of memory management, highlighting how data must be transferred between host and device memories using functions like cudaMalloc and cudaMemcpy. We also saw the significance of error handling throughout the CUDA programming process, utilizing macros to check for errors and ensure smooth execution.\nThe heart of CUDA programming lies in kernels, functions executed on the GPU, typically invoked in a grid of threads. We learned about grid and block dimensions, and the necessity of boundary checks within kernels to avoid out-of-bounds memory access.\nWe then examined a practical example of vector addition, illustrating how to define and invoke kernels, manage memory, and handle errors.\nLastly, we discussed compiling and running CUDA programs using the nvcc compiler and executing the resulting executables.\nOverall, CUDA provides a powerful framework for parallel programming on GPUs, offering immense computational capabilities for a wide range of applications. With a solid understanding of its principles and techniques, developers can unlock the full potential of GPU-accelerated computing.\nTime to close the operating theater! You\u0026rsquo;ve witnessed the basic intricacies of CUDA programming under the scalpel today. Keep honing those skills, and you\u0026rsquo;ll soon be performing GPU surgeries with finesse. Until our next exploration, stay sharp and happy coding! 🤖\n","date":"6 May 2024","externalUrl":null,"permalink":"/posts/hello-cuda/","section":"Posts","summary":"\u003cp\u003eIn case you didn\u0026rsquo;t already know, \u003ca href=\"https://it.wikipedia.org/wiki/CUDA\" target=\"_blank\"\u003eCUDA\u003c/a\u003e is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.\u003c/p\u003e","title":"Hello CUDA: A Surgical Dissection","type":"posts"},{"content":" NOTE: This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\nLarge Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\nWhen I first started learning about neural networks -about 5 years ago-, one of the key questions I had was if it would be possible to know, a priori, the minimum number of parameters that a network must implement to achieve a certain metric value when solving a specific problem. Unfortunately, as far as I know, there is no such theorem. Surely, we know about convergence theorems -like the Universal Approximation Theorem-, but nothing tells us the optimal number of parameters, given an architecture and a problem to solve.\nA very interesting methodology is the Cascade Correlation, which is a constructive approach for obtaining a network that solves a specific problem. However, this methodology is impractical in modern times: the size of neural networks is just too big, especially when talking about LLMs, which typically span from 7 to 70 billion parameters.\nTherefore, a follow-up question: given a trained, large network, can we reduce its size while maintaining the same accuracy?\nThat’s the key idea behind Pruning.\nPruning for Sparsity # Let’s start from a biological point of view: humans drastically reduce the number of synapses per neuron from early childhood to adolescence. This has been shown in various papers (e.g. Synapse elimination accompanies functional plasticity in hippocampal neurons) and it is a way for the brain to optimize its knowledge and remove redundancy. As often happens, this concept can also be applied to learning architectures.\nAssume to have a relatively large neural network that has been trained to approximate the solving function for a given problem, which was described by a dataset. This means that, by training, we minimized the loss function defined as:\n\\(\\mathrm{L}(\\theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{(x,y)\\in(X,Y)}{L(\\theta, x, y)}\\)\nwhere \\(\\theta\\) represents the weights of the network and \\(\\mathcal{D}\\) symbolizes the data used to train it.\nLet’s now assume that we are capable of pruning the weights \\(\\theta\\), therefore obtaining \\(\\theta_{pruned}\\). Our objective is to get approximately the same loss value when using the pruned weights. In math:\nThe advantage here is obvious: \\(\\theta_{pruned}\\) is a smaller, more efficient model that is capable of replicating the accuracy of a bigger, slower model.\nTake a look at the picture below: by pruning the red neuron, we remove 8 connections and the activation of the neuron itself. If both the networks had the same accuracy, we would prefer the right one, because its inference is more efficient.\ncredits to Nvidia blog Choosing what to prune # The pruning criterion is a very important factor to take into account, and research is still very active in this specific field. The common idea that unites all the pruning criteria is to prune parameters that are less useful in solving the problem. Therefore, we need a metric that describes the importance of a neuron in the overall network.\nIn this post, I’ll only talk about the Magnitude-based criterion, but I encourage interested readers to research other methodologies, e.g. Scaling-based pruning, Regression-based pruning, and Percentage-of-zero-based pruning.\nThe Magnitude-based criterion selects parameters to prune based on, guess what, their magnitude, therefore using an L_p norm. For example, we could choose to use an L_1 norm (a.k.a. the absolute value) as the importance metric for the parameter:\n\\(importance(\\theta_{i}) = ||\\theta_{i}||\\)\nThe rationale behind this is that the smaller the parameter, the less its influence is in the activation of the neuron.\nSo, if you assume to have a neural network with 100 parameters, and you want to prune 80% of them, you should first define an importance metric, then compute it for each parameter, sort the result, and pick the top-k parameters for that importance.\nPruning Granularity # Another key concept for pruning is granularity. We can proceed either by selecting parameters in an unstructured way or using a pattern. Let’s just see two examples to grasp this concept.\nAssume to have a matrix that represents the parameters we want to prune:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n} \\)\nand also assume that we picked the L_2norm as the importance metric for the Magnitude-based criterion.\nThen, we can perform either:\nunstructured pruning: we iterate over each parameter, compute its L_2 norm, and prune it if it’s not in the top-k percentage of neurons per importance.\nstructured pruning: instead of computing the L_2 norm for the single parameter, we first select a pattern. Let’s keep things simple and assume that our pattern corresponds to the row of the matrix P. Therefore, we will not prune the single parameter, but instead, we will prune the rows that do not belong to the top-k percentage of the highest-importance rows in the matrix.\nThe picture below depicts the difference between the two approaches.\ncredits to efficientml.ai This picture also foretells the advantages of pattern-based pruning, which I will explain in a later section.\nPruning Ratio # The pruning ratio is the last key concept we need to understand. Luckily for us, it’s quite straightforward. The ratio indicates how much sparsity we want in our neural network.\nFor example, if we pick a uniform pruning ratio of 80%, we will remove 80% of the parameters of the network, which will be selected based on the criterion and importance measure, defined before.\nThe word “uniform” is not randomly put. In fact, we can also provide a different pruning ratio for different levels of the network architecture. For example, we could select different pruning ratios for different layers of the architecture, or different channels.\nThere are several methodologies to find the optimal pruning ratio, but I will not cover them in this introduction.\nDoes pruning work? # Yes, and incredibly, if done correctly.\nThe first thing to notice is that we should always retrain the network after pruning. This is done because, when pruning aggressively (typically \u0026gt; 90%), the accuracy of the network drops significantly. Performing a fine-tuning step is crucial to retain information in the parameters that survived the pruning phase, and therefore stabilize the accuracy of the model.\nThis excellent picture from efficientml.ai very clearly depicts this concept\ncredits to efficientml.ai Computations with sparsity # When talking about the pruning granularity, we saw the conceptual difference between unstructured and structured pruning. Let’s consider again the matrix:\n\\(\\mathsf{P} \\in \\mathcal{R}^{m,n}\\)\nLet’s now assume another matrix, which will be multiplied with matrix \\(\\mathcal{P}\\):\n\\(\\mathsf{L} \\in \\mathcal{R}^{n,l}\\)\nComputing \\(\\mathsf{P}\\) x \\(\\mathsf{L}\\) in the most straightforward way possible, we perform a number of MACs equal to:\n\\(MACs = m \\times n \\times l\\)\nIf we manage to reduce the number of rows of \\(\\mathsf{P}\\) -therefore reducing \\(m\\)-, it’s easy to see how the number of computations is reduced, as the size of the matrix \\(\\mathsf{P}\\) itself.\nIn the unstructured case, instead, we did not shrink the dimension of the \\(\\mathsf{P}\\) matrix, but we just made it sparse, by setting some elements to 0. How are we getting a benefit from that?\nThat’s the catch: we don’t. If we don’t have the availability of an efficient system for sparsity, we will not get any benefit from having a sparse \\(\\mathsf{P}\\) matrix. This applies to both the computational and memory point of view.\nThe topic of efficient systems for sparsity computations is just too vast to be discussed here. Let me know if you are interested in the topic and would like to read a post that delves into it!\nConclusions # In this brief introduction to pruning for efficient neural networks’ inference, we learned about the three key concepts for this methodology: selecting the parameters to prune, the pruning granularity, and the pruning ratio.\nWe also saw how much benefit can be obtained by pruning a model, both in terms of computational and memory complexity.\nThere’s still much to say about this technique, so the learning doesn’t stop here! I hope this post was a good introduction, and that it conveyed well to you these basic concepts.\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/pruning-intro/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNOTE:\u003c/em\u003e\u003c/strong\u003e  This post was written before the Machine Learning Surgeon got in charge of the blog, that\u0026rsquo;s why there are no references to surgical operations!\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eLarge Language Model\u003c/strong\u003e. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models. Surely, you know the most popular one, ChatGPT, made by OpenAI.\u003c/p\u003e","title":"An Introduction to Sparsity for Efficient Neural Network Inference","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/pruning/","section":"Tags","summary":"","title":"Pruning","type":"tags"},{"content":" Machine Learning Surgery\u0026rsquo;s New Instrument: Triton # One of the most amazing things about being a Machine Learning Surgeon is that you never get bored at work. The reason? There\u0026rsquo;s always novelty, always new things to learn and discover, mainly because your colleagues are always busy creating new exciting things.\nA fairly new instrument is called Triton. Triton, developed and maintained by OpenAI, is by definition a language and a compiler for parallel programming. It was announced circa the end of 2021 and since then it grew a large community of users, thanks to its simplicity and power.\nIts easy of use is mainly due to the fact that Triton allows developers to write code at a higher level of abstraction, while still achieving performance comparable to hand-optimized CUDA code. It does this by automatically handling many of the complex optimizations that GPU programmers typically need to manage manually such as memory access patterns and thread blocking.\nThis is a game changer, since many developers never approached GPU programming because of CUDA\u0026rsquo;s steep learning curve and\n","externalUrl":null,"permalink":"/posts/intro_triton/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003eMachine Learning Surgery\u0026rsquo;s New Instrument: Triton \n    \u003cdiv id=\"machine-learning-surgerys-new-instrument-triton\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#machine-learning-surgerys-new-instrument-triton\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\u003cp\u003eOne of the most amazing things about being a Machine Learning Surgeon is that you never get bored at work. The reason? There\u0026rsquo;s always novelty, always new things to learn and discover, mainly because your colleagues are always busy creating new exciting things.\u003c/p\u003e","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]