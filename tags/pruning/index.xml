<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pruning on NeuronBit</title><link>https://dwarez.github.io/tags/pruning/</link><description>Recent content in Pruning on NeuronBit</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 Dario Salvati</copyright><lastBuildDate>Sun, 05 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dwarez.github.io/tags/pruning/index.xml" rel="self" type="application/rss+xml"/><item><title>An Introduction to Sparsity for Efficient Neural Network Inference</title><link>https://dwarez.github.io/posts/pruning/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://dwarez.github.io/posts/pruning/</guid><description>Large Language Model. How many times did you read that term? Nowadays, the popularity of Artificial Intelligence is to be attributed to the exceptional results obtained, in the past few years, by applications that leverage large models.</description></item></channel></rss>