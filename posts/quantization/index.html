<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models &#183; The ML Surgeon</title>
<meta name=title content="The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models &#183; The ML Surgeon"><meta name=description content="The low level side of machine learning."><meta name=keywords content="quantization,inference,optimization,"><link rel=canonical href=https://dwarez.github.io/posts/quantization/><link type=text/css rel=stylesheet href=/css/main.bundle.min.7e511937ade47127f0cc4d4943b7cc62b8f7cf37a9c19383694c5760ad479781345dbedf1b713a57a80254844a8966c81cb86298461c8f60c196036e053f9605.css integrity="sha512-flEZN63kcSfwzE1JQ7fMYrj3zzepwZODaUxXYK1Hl4E0Xb7fG3E6V6gCVIRKiWbIHLhimEYcj2DBlgNuBT+WBQ=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://dwarez.github.io/posts/quantization/"><meta property="og:site_name" content="The ML Surgeon"><meta property="og:title" content="The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models"><meta property="og:description" content="As humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, string theory posits that the universe’s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, loop quantum gravity suggests that space itself is made up of discrete “grains,” implying that reality might not be as continuous as it appears."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-10T00:00:00+00:00"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="Inference"><meta property="article:tag" content="Optimization"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models"><meta name=twitter:description content="As humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, string theory posits that the universe’s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, loop quantum gravity suggests that space itself is made up of discrete “grains,” implying that reality might not be as continuous as it appears."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"The Machine Learning Surgeon\u0027s Guide to Quantization: Precision Cuts for Smarter Models","headline":"The Machine Learning Surgeon\u0027s Guide to Quantization: Precision Cuts for Smarter Models","abstract":"\u003cp\u003eAs humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, \u003cstrong\u003estring theory\u003c\/strong\u003e posits that the universe\u0026rsquo;s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, \u003cstrong\u003eloop quantum gravity\u003c\/strong\u003e suggests that space itself is made up of discrete \u0026ldquo;grains,\u0026rdquo; implying that reality might not be as continuous as it appears.\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/dwarez.github.io\/posts\/quantization\/","author":{"@type":"Person","name":"Dario Salvati"},"copyrightYear":"2024","dateCreated":"2024-11-10T00:00:00\u002b00:00","datePublished":"2024-11-10T00:00:00\u002b00:00","dateModified":"2024-11-10T00:00:00\u002b00:00","keywords":["quantization","inference","optimization"],"mainEntityOfPage":"true","wordCount":"5402"}]</script><meta name=author content="Dario Salvati"><link href=https://linkedin.com/in/dwarez rel=me><link href=https://github.com/dwarez rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PEDMYR1V0K"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PEDMYR1V0K")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">The ML Surgeon</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Blog</p></a><a href=https://linkedin.com/in/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://github.com/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Blog</p></a></li><li class=mt-1><a href=https://linkedin.com/in/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://github.com/dwarez target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div><script>(function(){var e=$(".main-menu"),t=window.location.pathname;e.find('a[href="'+t+'"]').each(function(e,t){$(t).children("p").addClass("active")})})()</script></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/img/gpu_power_hu_57149ed645ba7af8.jpg)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>The ML Surgeon</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/quantization/>The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">The Machine Learning Surgeon's Guide to Quantization: Precision Cuts for Smarter Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span>
<span class=mb-[2px]><a href=https://github.com/DWarez/dwarez.github.io/tree/master/content/posts/quantization/index.md class="text-lg hover:text-primary-500" rel="noopener noreferrer" target=_blank title="Edit content"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentcolor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg>
</span></span></a></span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.63c8a202661f4a2063fdc2706685d668e8ea3da613da2224e9da527e5876e4f53dcac39ab60732626fb4151feae5d430d0cf44731e5d3c726522fcc1519c1547.js integrity="sha512-Y8iiAmYfSiBj/cJwZoXWaOjqPaYT2iIk6dpSflh25PU9ysOatgcyYm+0FR/q5dQw0M9Ecx5dPHJlIvzBUZwVRw=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/quantization/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Quantization
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/inference/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Inference
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/optimization/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Optimization</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Dario Salvati" src=/img/avatar_hu_20729d8348a32780.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Dario Salvati</div><div class="text-sm text-neutral-700 dark:text-neutral-400">A surgeon for Machine Learning models.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/dwarez target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/dwarez target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#what-is-quantization-the-anatomy-of-precision>What is Quantization: The Anatomy of Precision</a></li><li><a href=#numeric-data-types-the-skeleton-of-computation>Numeric Data Types: The Skeleton of Computation</a><ul><li><a href=#integers-the-bones-of-simplicity>Integers: The Bones of Simplicity</a></li><li><a href=#floating-point-numbers-the-lifeblood-of-precision>Floating Point Numbers: The Lifeblood of Precision</a><ul><li><a href=#beyond-fp32-cutting-into-smaller-tissue-layers>Beyond FP32: Cutting into Smaller Tissue Layers</a></li><li><a href=#even-smaller-dissecting-fp16-and-below>Even Smaller? Dissecting FP16 and Below</a></li><li><a href=#why-do-we-care-the-doctors-case-for-optimization>Why Do We Care? The Doctor&rsquo;s Case for Optimization</a></li></ul></li></ul></li><li><a href=#quantization-in-practice-performing-surgery-on-model-precision>Quantization in Practice: Performing Surgery on Model Precision</a><ul><li><a href=#how-naïve-quantization-algorithms-for-quick-fixes>How: Naïve Quantization Algorithms for Quick Fixes</a><ul><li><a href=#absolute-maximum-quantization-a-straightforward-incision>Absolute Maximum Quantization: A Straightforward Incision</a></li><li><a href=#zero-point-quantization-accounting-for-asymmetric-anatomy>Zero-Point Quantization: Accounting for Asymmetric Anatomy</a></li></ul></li><li><a href=#what-identifying-the-organs-to-quantize>What: Identifying the Organs to Quantize</a><ul><li><a href=#weights-and-activations-the-heart-and-brain-of-deep-learning>Weights and Activations: The Heart and Brain of Deep Learning</a></li></ul></li><li><a href=#when-choosing-the-right-time-for-precision-surgery>When: Choosing the Right Time for Precision Surgery</a><ul><li><a href=#static-quantization-prepping-the-patient-before-surgery>Static Quantization: Prepping the Patient Before Surgery</a></li><li><a href=#dynamic-quantization-on-the-fly-adjustments>Dynamic Quantization: On-the-Fly Adjustments</a></li><li><a href=#quantization-aware-training-training-with-surgical-foresight>Quantization Aware Training: Training with Surgical Foresight</a></li><li><a href=#mixed-precision-training-balancing-precision-and-efficiency>Mixed Precision Training: Balancing Precision and Efficiency</a></li></ul></li><li><a href=#why-quantization-as-the-life-saving-operation>Why: Quantization as the Life-Saving Operation</a><ul><li><a href=#dynamic-quantization-tackling-compute-bound-emergencies>Dynamic Quantization: Tackling Compute-Bound Emergencies</a></li><li><a href=#dynamic-quantization-tackling-compute-bound-bottlenecks>Dynamic Quantization: Tackling Compute-Bound Bottlenecks</a></li><li><a href=#additional-benefits-energy-efficiency-and-scalability-in-healthcare-systems>Additional Benefits: Energy Efficiency and Scalability in Healthcare Systems</a></li></ul></li></ul></li><li><a href=#quantization-in-frameworks-the-surgeons-toolkit>Quantization in Frameworks: The Surgeon&rsquo;s Toolkit</a><ul><li><a href=#pytorch-the-scalpel-for-precise-optimization>Pytorch: The Scalpel for Precise Optimization</a></li><li><a href=#gptq-surgical-innovation-in-post-training-quantization>GPTQ: Surgical Innovation in Post-Training Quantization</a></li><li><a href=#transformers-llmint8-mitigating-outliers-with-precision-cuts>Transformers&rsquo; <code>LLM.int8()</code>: Mitigating Outliers with Precision Cuts</a></li></ul></li><li><a href=#hardware-support-the-operating-table-for-modern-quantization>Hardware Support: The Operating Table for Modern Quantization</a></li><li><a href=#conclusion-the-final-diagnosis>Conclusion: The Final Diagnosis</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#what-is-quantization-the-anatomy-of-precision>What is Quantization: The Anatomy of Precision</a></li><li><a href=#numeric-data-types-the-skeleton-of-computation>Numeric Data Types: The Skeleton of Computation</a><ul><li><a href=#integers-the-bones-of-simplicity>Integers: The Bones of Simplicity</a></li><li><a href=#floating-point-numbers-the-lifeblood-of-precision>Floating Point Numbers: The Lifeblood of Precision</a><ul><li><a href=#beyond-fp32-cutting-into-smaller-tissue-layers>Beyond FP32: Cutting into Smaller Tissue Layers</a></li><li><a href=#even-smaller-dissecting-fp16-and-below>Even Smaller? Dissecting FP16 and Below</a></li><li><a href=#why-do-we-care-the-doctors-case-for-optimization>Why Do We Care? The Doctor&rsquo;s Case for Optimization</a></li></ul></li></ul></li><li><a href=#quantization-in-practice-performing-surgery-on-model-precision>Quantization in Practice: Performing Surgery on Model Precision</a><ul><li><a href=#how-naïve-quantization-algorithms-for-quick-fixes>How: Naïve Quantization Algorithms for Quick Fixes</a><ul><li><a href=#absolute-maximum-quantization-a-straightforward-incision>Absolute Maximum Quantization: A Straightforward Incision</a></li><li><a href=#zero-point-quantization-accounting-for-asymmetric-anatomy>Zero-Point Quantization: Accounting for Asymmetric Anatomy</a></li></ul></li><li><a href=#what-identifying-the-organs-to-quantize>What: Identifying the Organs to Quantize</a><ul><li><a href=#weights-and-activations-the-heart-and-brain-of-deep-learning>Weights and Activations: The Heart and Brain of Deep Learning</a></li></ul></li><li><a href=#when-choosing-the-right-time-for-precision-surgery>When: Choosing the Right Time for Precision Surgery</a><ul><li><a href=#static-quantization-prepping-the-patient-before-surgery>Static Quantization: Prepping the Patient Before Surgery</a></li><li><a href=#dynamic-quantization-on-the-fly-adjustments>Dynamic Quantization: On-the-Fly Adjustments</a></li><li><a href=#quantization-aware-training-training-with-surgical-foresight>Quantization Aware Training: Training with Surgical Foresight</a></li><li><a href=#mixed-precision-training-balancing-precision-and-efficiency>Mixed Precision Training: Balancing Precision and Efficiency</a></li></ul></li><li><a href=#why-quantization-as-the-life-saving-operation>Why: Quantization as the Life-Saving Operation</a><ul><li><a href=#dynamic-quantization-tackling-compute-bound-emergencies>Dynamic Quantization: Tackling Compute-Bound Emergencies</a></li><li><a href=#dynamic-quantization-tackling-compute-bound-bottlenecks>Dynamic Quantization: Tackling Compute-Bound Bottlenecks</a></li><li><a href=#additional-benefits-energy-efficiency-and-scalability-in-healthcare-systems>Additional Benefits: Energy Efficiency and Scalability in Healthcare Systems</a></li></ul></li></ul></li><li><a href=#quantization-in-frameworks-the-surgeons-toolkit>Quantization in Frameworks: The Surgeon&rsquo;s Toolkit</a><ul><li><a href=#pytorch-the-scalpel-for-precise-optimization>Pytorch: The Scalpel for Precise Optimization</a></li><li><a href=#gptq-surgical-innovation-in-post-training-quantization>GPTQ: Surgical Innovation in Post-Training Quantization</a></li><li><a href=#transformers-llmint8-mitigating-outliers-with-precision-cuts>Transformers&rsquo; <code>LLM.int8()</code>: Mitigating Outliers with Precision Cuts</a></li></ul></li><li><a href=#hardware-support-the-operating-table-for-modern-quantization>Hardware Support: The Operating Table for Modern Quantization</a></li><li><a href=#conclusion-the-final-diagnosis>Conclusion: The Final Diagnosis</a></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})(),function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=decodeURIComponent(t.attr("id")))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active")}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){n()})}}()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>As humans, we perceive space and time as a seamless, continuous flow. This perception leads us to believe that continuity—and perhaps even infinity—is a fundamental aspect of nature. However, some scientific theories challenge this assumption. For example, <strong>string theory</strong> posits that the universe&rsquo;s fundamental components are tiny, vibrating strings, where different vibrations define different particles. Similarly, <strong>loop quantum gravity</strong> suggests that space itself is made up of discrete &ldquo;grains,&rdquo; implying that reality might not be as continuous as it appears.</p><p>Why should this intrigue us, Machine Learning Surgeons? Because while we may experience continuity, <strong>machines do not</strong>. Machines operate strictly within the realm of discrete and finite data types, from bits and bytes to floating-point numbers. This disconnect raises a crucial question: <strong>How can machines represent signals that seem continuous to us?</strong></p><p>Consider the <strong>spikes generated by neurons in the brain</strong>. While neuronal activity appears continuous from a biological perspective, it can be interpreted as a series of discrete events—spikes or pulses. To model this activity on a machine, we sample these spikes at fixed intervals, effectively converting a seemingly continuous signal into a <strong>discrete representation</strong>. The choice of how finely we sample (and which data types we use) impacts the fidelity of our representation, affecting everything from signal reconstruction to downstream machine learning models.</p><p>This brings us to the core challenge: <strong>quantization</strong>—the process of mapping continuous values to discrete ones. Let&rsquo;s explore how this translation from the continuous to the discrete world shapes our data types and impacts machine learning performance!</p><h2 class="relative group">What is Quantization: The Anatomy of Precision<div id=what-is-quantization-the-anatomy-of-precision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#what-is-quantization-the-anatomy-of-precision aria-label=Anchor>#</a></span></h2><p>In the natural world, many signals—such as sound waves, light intensity, or even time itself—are (perceived) continuous. However, machines, by their very nature, operate in a discrete framework. They deal with bits, bytes, and finite representations of data. This fundamental limitation means that to store and process real-world signals, we must first translate them into a form that machines can understand. Enter <strong>quantization</strong>.</p><p>Quantization is the process of mapping a continuous range of values into a finite set of discrete levels. Think of it as breaking down a flowing river of data into buckets that can be cataloged and stored. For example:</p><ul><li>An audio signal, with its infinite variations in amplitude, must be sampled at specific intervals and its amplitude mapped to discrete levels.</li><li>An image, representing continuous changes in light and color, is pixelated into finite, numerical values.</li></ul><p>This conversion is essential for computation but comes with trade-offs. Quantization introduces <strong>approximations</strong>; a continuous signal can only be represented with a finite precision, leading to quantization errors. In fact, measure theory states that infinite precision is not achievable, not even in a theoretical setting.</p><p>The following image illustrates the process of quantization. First, the signal is sampled at a specific rate, meaning values are selected along the x-axis at regular intervals. Here, we&rsquo;ll assume the x-axis represents time. On the y-axis, we have the amplitude of the signal, which is then mapped to a finite set of discrete levels.</p><p>The difference between the actual signal value and its closest quantized level is known as the <strong>quantization error</strong>. This error is an unavoidable artifact of the process, stemming from the approximation required to fit continuous values into a discrete framework:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/sin_hu_e6cd6a7a4fb29f90.png 330w,
/posts/quantization/sin_hu_375508d4426e9ee.png 660w,
/posts/quantization/sin_hu_37e73ee6ff1a53cd.png 1024w,
/posts/quantization/sin_hu_11be037e58422655.png 2x" src=/posts/quantization/sin_hu_375508d4426e9ee.png alt=sin><figcaption>Credits to <a href=https://hanlab.mit.edu target=_blank>HanLab</a></figcaption></figure></p><p>At first glance, you might think this discussion has little to do with machine learning, especially since we&rsquo;re not directly talking about models. Why should we care about the quantization of real-valued, continuous signals like audio or images? And you&rsquo;d be partly correct—our primary concern isn&rsquo;t the raw quantization of these signals.</p><p>Instead, the point here is to emphasize the <strong>tradeoff</strong> between the true value of a signal and its <strong>representation within hardware</strong>. Neural networks aim to mimic the inner workings of the human brain, which constantly produces electrical impulses and signals. In machine learning, these impulses are represented as <strong>neuron activations</strong>. However, these activations must ultimately exist within a machine, and machines operate within the constraints of discreteness and finiteness. This means that the continuous signals we&rsquo;re trying to emulate must be mapped into a discrete, finite set of values.</p><p>This is where data types come into play. The choice of data types for representing weights and activations in neural networks is absolutely critical. It impacts not only the <strong>precision</strong> and <strong>accuracy</strong> of the computations but also the efficiency of the entire system. And, as you&rsquo;ll see shortly, the requirements for data representation often differ significantly between the training and inference phases of a model.</p><p>Before diving into those differences, let&rsquo;s take a moment to refresh our understanding of numeric data types and their implications.</p><h2 class="relative group">Numeric Data Types: The Skeleton of Computation<div id=numeric-data-types-the-skeleton-of-computation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#numeric-data-types-the-skeleton-of-computation aria-label=Anchor>#</a></span></h2><h3 class="relative group">Integers: The Bones of Simplicity<div id=integers-the-bones-of-simplicity class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#integers-the-bones-of-simplicity aria-label=Anchor>#</a></span></h3><p>Let&rsquo;s start with the simplest data type you can think of: the integer.</p><p>Representing an unsigned integer is straightforward. Given <code>n</code>, the number of bits used for representation, we simply use the binary representation of the number. Here&rsquo;s a quick refresher for those who might need it:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/int_repr_hu_8fa869675539afec.png 330w,
/posts/quantization/int_repr_hu_d28853953446a8b7.png 660w,
/posts/quantization/int_repr_hu_47993de13c5e95e.png 1024w,
/posts/quantization/int_repr_hu_3f2e9952be14b603.png 2x" src=/posts/quantization/int_repr_hu_d28853953446a8b7.png alt=int_repr><figcaption>Another good opportunity to checkout my great drawings!</figcaption></figure></p><p>In this case, the range of the representation is
\([0, 2^n - 1]\).</p><p>But what about signed integers? These require a way to handle both positive and negative numbers, and there are two common approaches for this:</p><p>One possible approach is called <strong>Sign-Magnitude Representation</strong>. In this method, the leftmost bit (most significant bit) represents the sign of the number:
\(0\) for positive and
\(1\) for negative. The remaining bits represent the magnitude. For example:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/sign_int_hu_30056d76e6c6b41.png 330w,
/posts/quantization/sign_int_hu_e53c27d899ef235a.png 660w,
/posts/quantization/sign_int_hu_207ebb1a5b60a72c.png 1024w,
/posts/quantization/sign_int_hu_7a1bee7d1eb35ce9.png 2x" src=/posts/quantization/sign_int_hu_e53c27d899ef235a.png alt=sign_int></figure></p><p>In this representation, the range of values is
\([-2^{n-1} + 1, 2^{n-1} - 1]\).</p><p>Alternatively, the <strong>Two&rsquo;s Complement Representation</strong> can be used. Here, the leftmost bit is treated as having a negative value, allowing for a more elegant way to represent signed numbers. This method is widely used in modern computing because it simplifies arithmetic operations. For example:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/twos_complement_hu_a35edbb2773d475a.png 330w,
/posts/quantization/twos_complement_hu_f5cbbe2fa24543b2.png 660w,
/posts/quantization/twos_complement_hu_abbc36b30eb5e329.png 1024w,
/posts/quantization/twos_complement_hu_28cc1ba40dc03c1a.png 2x" src=/posts/quantization/twos_complement_hu_f5cbbe2fa24543b2.png alt=twos_complement></figure></p><p>With two&rsquo;s complement, the range of values becomes
\([-2^{n-1}, 2^{n-1} - 1]\).</p><h3 class="relative group">Floating Point Numbers: The Lifeblood of Precision<div id=floating-point-numbers-the-lifeblood-of-precision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#floating-point-numbers-the-lifeblood-of-precision aria-label=Anchor>#</a></span></h3><p>Floating-point numbers are where things get interesting—and a little tricky. Unlike integers, they need to represent both whole numbers and fractions, which means we need more complex ways to store them. There are several standards for this, so let&rsquo;s dive in!</p><p>The most common one you&rsquo;ve probably heard of is <strong>IEEE 754</strong>, which defines the famous <strong>64-bit and 32-bit floating-point</strong> formats, also called <strong>FP64</strong> and <strong>FP32</strong>.</p><p><strong>FP64</strong> splits 64 bits into three parts: 1 <strong>sign</strong> bit, 11 <strong>exponent</strong> bits and 53 <strong>fraction</strong> bits (often called the <strong>mantissa</strong> or <strong>significant</strong>).</p><p>Likewise, <strong>FP32</strong> splits its 32 bits into three parts: 1 bit for the sign, 8 bits for the exponent, and 23 bits for the fraction.</p><p>Here&rsquo;s what an FP32 number looks like:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/fp32_hu_cd9ad48afeb58008.png 330w,
/posts/quantization/fp32_hu_d5fe614286036249.png 660w,
/posts/quantization/fp32_hu_c7b157ccc9dedbd7.png 1024w,
/posts/quantization/fp32_hu_5857d292ab753ab7.png 2x" src=/posts/quantization/fp32_hu_d5fe614286036249.png alt=fp32></figure></p><p>The value of an FP32 number is calculated using this formula:
\((-1)^{sign} \times (1+ fraction) \times 2^{exponent-127}\)</p><blockquote><p>Note: Don&rsquo;t stress about the formula! It&rsquo;s slightly different for subnormal numbers, but we can skip those for now.</p></blockquote><p>Now, what&rsquo;s the point of splitting numbers this way? Each part has a job:</p><ul><li>The <strong>sign</strong> bit is obvious—it tells you whether the number is <strong>positive</strong> or <strong>negative</strong>.</li><li>The <strong>exponent</strong> determines how big or small the number can get, <strong>controlling the numeric range</strong>.</li><li>The <strong>fraction</strong> determines the <strong>numeric precision</strong>.</li></ul><p>The kicker is that there&rsquo;s always a <strong>tradeoff</strong> between range and precision. With a fixed number of bits, you have to decide what&rsquo;s more important for your task. Do you need pinpoint accuracy, or do you need to handle really large (or tiny) numbers?</p><h4 class="relative group">Beyond FP32: Cutting into Smaller Tissue Layers<div id=beyond-fp32-cutting-into-smaller-tissue-layers class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#beyond-fp32-cutting-into-smaller-tissue-layers aria-label=Anchor>#</a></span></h4><p>FP32 offers a solid balance between precision, speed, and memory efficiency, especially compared to its big sibling, FP64. But here&rsquo;s the question: do we really need 32 bits for deep learning? As it turns out, the answer is often no. Experiments have shown <strong>we can go smaller</strong>, and that&rsquo;s where 16-bit data types come into play.</p><p>Let&rsquo;s start with FP16, or &ldquo;half-precision&rdquo;. This format uses 1 bit for the <strong>sign</strong>, 5 bits for the <strong>exponent</strong>, and 10 bits for the <strong>fraction</strong>.</p><p>This smaller footprint makes FP16 faster and more memory-efficient, which is a big deal for inference tasks where every millisecond counts. However, there&rsquo;s a catch: those 5 exponent bits don&rsquo;t provide <strong>enough dynamic range</strong> to handle the wild swings in gradient values during training. Training a model in pure FP16 often leads to a drop in accuracy because the format struggles to represent very small or very large numbers accurately.</p><p>But don&rsquo;t lose hope! There&rsquo;s a clever workaround called <strong>mixed precision training</strong>. The idea is simple: keep the heavy-lifting math (like gradient calculations) in FP32 while using FP16 for everything else. This way, you get the efficiency of FP16 without sacrificing accuracy. We&rsquo;ll dive into the details of this technique soon.</p><p>To address FP16&rsquo;s limitations for training, Google introduced <strong>BF16</strong> (Brain Float 16). BF16 keeps the 8-bit exponent from FP32 but reduces the fraction to 7 bits. This clever tweak gives BF16 nearly the same range as FP32, making it suitable for both training and inference. The reduced precision in the fraction doesn&rsquo;t hurt much in practice, especially for tasks like training deep neural networks where range matters more than pinpoint precision.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/fp_bf_hu_aa4460378c5cd5f4.png 330w,
/posts/quantization/fp_bf_hu_ed2fc154c6a6d65c.png 660w,
/posts/quantization/fp_bf_hu_78ddd914d33b2e56.png 1024w,
/posts/quantization/fp_bf_hu_a4557cbf93f1dd3.png 2x" src=/posts/quantization/fp_bf_hu_ed2fc154c6a6d65c.png alt=fp_bf><figcaption>A visual difference between FP16 and BF16</figcaption></figure></p><p>With BF16, you get a sweet spot: less memory usage than FP32 but enough dynamic range to handle training. That&rsquo;s why it&rsquo;s become a favorite in the deep learning community.</p><h4 class="relative group">Even Smaller? Dissecting FP16 and Below<div id=even-smaller-dissecting-fp16-and-below class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#even-smaller-dissecting-fp16-and-below aria-label=Anchor>#</a></span></h4><p>Why stop at 16 bits? When it comes to efficiency, smaller is better, and researchers are pushing the limits with even tinier formats.</p><p>One of the boldest moves is <strong>FP8</strong>. NVIDIA introduced FP8 to squeeze even more efficiency out of deep learning models. FP8 comes in two flavors: E4M3, using 4 bits for the exponent and 3 for the fraction; E5M2, using 5 bits for the exponent and 2 for the fraction.</p><p>If we extend the range to the maximum, we obtain the <strong>INT8</strong> data type, which has been a popular choice in the last years.</p><p>Both versions focus on maximizing throughput and minimizing memory usage, making them perfect for <strong>large-scale inference tasks</strong>.</p><p>But wait—why stop at 8 bits? Let&rsquo;s talk about 4-bit formats.</p><p><strong>INT4</strong> is a simple 4-bit signed integer. Nothing fancy, but incredibly efficient.
This get a bit more interesting with <strong>FP4</strong>, which comes in variants like E1M2, E2M1, and E3M0, where you can decide how to split the few available bits between the exponent and fraction. These formats are still experimental but hold promise for ultra-fast inference on edge devices and other resource-constrained environments.</p><h4 class="relative group">Why Do We Care? The Doctor&rsquo;s Case for Optimization<div id=why-do-we-care-the-doctors-case-for-optimization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-do-we-care-the-doctors-case-for-optimization aria-label=Anchor>#</a></span></h4><p>All this talk about data types isn&rsquo;t just academic—it&rsquo;s mission-critical for machine learning. The data type you choose <strong>affects everything</strong>: speed, memory usage, and even the accuracy of your models. The key point is that for <strong>training</strong>, you need formats with <strong>enough precision and range</strong> (like FP32 or BF16) to capture subtle gradients and updates. For <strong>inference</strong>, though, you can often afford to go lower (FP16, FP8, or even FP4), with the compromise of <strong>losing in terms of accuracy</strong>.</p><p>Picking the right data type is like choosing the right scalpel in an operating room—you need the perfect balance of precision and efficiency for the task at hand. And trust me, as Machine Learning Surgeons, we <em>always</em> care about the tools we use.</p><h2 class="relative group">Quantization in Practice: Performing Surgery on Model Precision<div id=quantization-in-practice-performing-surgery-on-model-precision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#quantization-in-practice-performing-surgery-on-model-precision aria-label=Anchor>#</a></span></h2><p>Alright, now that we&rsquo;ve covered the basics of numeric data types, it&rsquo;s time to roll up our sleeves and dive into quantization!</p><p>In this section, we&rsquo;ll explore the how, when, what, and why of quantization, starting with a simple explanation and building up from there.</p><h3 class="relative group">How: Naïve Quantization Algorithms for Quick Fixes<div id=how-na%C3%AFve-quantization-algorithms-for-quick-fixes class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#how-na%C3%AFve-quantization-algorithms-for-quick-fixes aria-label=Anchor>#</a></span></h3><p>There are many techniques for quantization, so it would be impossible to cover all of them here. However, let&rsquo;s focus on building a basic understanding of the process from a mathematical perspective. First and foremost, let&rsquo;s clarify something: when we <strong>quantize a tensor</strong>, we aim to <strong>reduce its values</strong>. Naturally, the values of the quantized tensor will exist in the same dimensional space but within a subset of its co-domain. To put it simply, we shrink the tensor&rsquo;s values (<strong>not its shape!</strong>) so that it can be represented using a data type that requires fewer bits.</p><p>For example, let&rsquo;s say we want to quantize a tensor in FP32 format, called
\(\mathrm{X}\) into INT8 format—a significant leap! Since we&rsquo;re targeting the INT8 data type, all the values of the FP32 tensor must be mapped to integers in the range
\([-128, 127]\).</p><p>Let&rsquo;s implement a very simple quantization technique called absolute maximum quantization.</p><h4 class="relative group">Absolute Maximum Quantization: A Straightforward Incision<div id=absolute-maximum-quantization-a-straightforward-incision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#absolute-maximum-quantization-a-straightforward-incision aria-label=Anchor>#</a></span></h4><p>The first step is to compute a scaling factor. This factor is calculated by dividing the <strong>absolute maximum value</strong> of the tensor
\(\mathrm{X}\) by the largest representable number in the target data type, which in this case is 127:</p><p>\(\mathrm{S}= \frac{max|\mathrm{X}|}{127}\)</p><p>Once we have the scaling factor
\(\mathrm{S}\), we scale all the values in
\(\mathrm{X}\), which we&rsquo;ll call
\(\mathrm{x_i}\) and then round them to get the quantized values:</p><p>\(\mathrm{q}_{i}= round(\frac{\mathrm{x_i}}{\mathrm{S}})\)</p><p>If any values end up outside the representable range, we clamp them to fall within the range of the target data type.</p><p><strong>Reconstructing</strong> the original values is straightforward. We just multiply the quantized values by the scaling factor:</p><p>\(\mathrm{\hat{\mathrm{x_i}}}= \mathrm{q_i} * \mathrm{S}\)</p><p>Of course, the reconstructed values won&rsquo;t be identical to the original ones due to approximations during the process. This difference is what we call the quantization error, something we touched on earlier.</p><p>One final thing to note about absolute maximum quantization: it&rsquo;s <strong>symmetric</strong>. This means the resulting tensor values are centered around zero, which is a nice property for certain applications.</p><h4 class="relative group">Zero-Point Quantization: Accounting for Asymmetric Anatomy<div id=zero-point-quantization-accounting-for-asymmetric-anatomy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#zero-point-quantization-accounting-for-asymmetric-anatomy aria-label=Anchor>#</a></span></h4><p>For certain types of tensors, we can reduce the reconstruction error from quantization by using <strong>asymmetric quantization</strong> methods, such as <strong>zero-point quantization</strong>. This approach introduces an offset to account for the asymmetric ranges between the source and target data types, making it particularly useful when the <strong>data isn&rsquo;t centered around zero</strong>.</p><p>Zero-point quantization is still very straightforward to implement.
To begin with, we compute the scaling factor. This factor represents the ratio between the range of values in the source tensor and the range of values that can be represented by the target data type. The computation adjusts for the difference in these ranges:</p><p>\(\mathrm{S}= \frac{max(\mathrm{X}) - min(\mathrm{X})}{max\_int - min\_int}\)</p><p>Once we have the scaling factor, the next step is to calculate the zero-point, which acts as the offset. This offset ensures that the minimum value of the source tensor aligns correctly with the minimum value of the target data type. The zero-point is determined based on the scaling factor and the range of values in the source tensor:</p><p>\(\mathrm{Z}= round(min\_int - \frac{min(\mathrm{X})}{\mathrm{S}})\)</p><p>With both the scaling factor and the zero-point ready, we quantize the tensor. Each original value is scaled and then shifted by the zero-point offset to ensure it fits properly into the target data type:</p><p>\(\mathrm{q}_{i}= round(\frac{\mathrm{x_i}}{\mathrm{S}}) + \mathrm{Z}\)</p><p><strong>Dequantization</strong> is just as simple. To reconstruct the original values, you reverse the process: first, shift the quantized values back by removing the zero-point offset, and then re-scale them to their original range. A word of caution here—make sure you shift back the values before applying the scaling factor, not the other way around!</p><p>\(\mathrm{\hat{\mathrm{x_i}}}= (\mathrm{q_i} - \mathrm{Z}) * \mathrm{S}\)</p><p>By using zero-point quantization, we can better preserve the original tensor&rsquo;s characteristics, especially when dealing with data that isn&rsquo;t symmetrically distributed. This method is a handy tool for minimizing quantization errors in such scenarios.</p><h3 class="relative group">What: Identifying the Organs to Quantize<div id=what-identifying-the-organs-to-quantize class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#what-identifying-the-organs-to-quantize aria-label=Anchor>#</a></span></h3><p>Now that we&rsquo;ve explored a naïve way to perform quantization, let&rsquo;s discuss what to quantize.</p><h4 class="relative group">Weights and Activations: The Heart and Brain of Deep Learning<div id=weights-and-activations-the-heart-and-brain-of-deep-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#weights-and-activations-the-heart-and-brain-of-deep-learning aria-label=Anchor>#</a></span></h4><p>To begin with, let&rsquo;s start at a very low level. Most of the computations in deep learning models involve matrix (tensor) multiplications. So, let&rsquo;s focus on the matrix multiplications between the input tensor
\(\mathrm{X}\) and the weights
\(\mathrm{W}\). A simple approach to quantization is to quantize the <strong>entire matrices</strong>. However, remember that the scaling factor depends on the values within the tensors. This means that using a finer granularity can help avoid or limit some potential <strong>numerical instability issues</strong>.</p><p>Consider a scenario where a tensor&rsquo;s values are sampled from a uniform distribution, but the minimum and maximum values are actually out-of-distribution (i.e., extreme values that don&rsquo;t represent the typical data). This can cause numerical instability during quantization because the scaling factor <strong>relies heavily</strong> on these two values.</p><p>To address this, we can use <strong>per-token</strong> and <strong>per-channel</strong> quantization. This approach involves quantizing a row of
\(\mathrm{X}\) and its corresponding column in
\(\mathrm{W}\) independently, so that the min and max values are computed at the token or channel level. By doing so, we mitigate the influence of out-of-distribution extreme values, leading to a more stable and accurate quantization process.</p><p>Up until now, we&rsquo;ve discussed quantizing tensors in general terms. However, in the context of deep learning, we must decide whether to quantize the weights, the activations, or both. This decision is important because it affects both the <strong>precision</strong> of the model and its <strong>computational speed</strong>. Therefore, it&rsquo;s crucial to make this choice thoughtfully and with a clear understanding of the trade-offs involved.</p><h3 class="relative group">When: Choosing the Right Time for Precision Surgery<div id=when-choosing-the-right-time-for-precision-surgery class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#when-choosing-the-right-time-for-precision-surgery aria-label=Anchor>#</a></span></h3><p>Deciding what to quantize is also closely related to when we want to perform quantization.</p><h4 class="relative group">Static Quantization: Prepping the Patient Before Surgery<div id=static-quantization-prepping-the-patient-before-surgery class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#static-quantization-prepping-the-patient-before-surgery aria-label=Anchor>#</a></span></h4><p>The most straightforward approach is <strong>post-training quantization</strong>. This involves taking a trained model and applying a chosen quantization algorithm.</p><p>Let&rsquo;s stop for a second and think about what we can quantize using this approach. We don&rsquo;t have any particular limitation in this case: we surely can <strong>quantize the weights</strong> and the model when loading it in memory. However, if we quantize only the weights, the activations of the model will keep their <strong>original data type</strong> (let&rsquo;s say BF16). This means that in order to perform the multiplication between the weights and the activations, we must <strong>dequantize the weights</strong> before that. This adds a significant overhead. At the same time, <strong>statically quantize the activations</strong> of the model seems kinda impossible, since activations depends on the input, which is only known at runtime, unlike the weights of the model that do not change overtime.</p><p>We can actually statically quantize the model&rsquo;s activations by using a <strong>calibration dataset</strong>, which will be used to observe the activations of the model and compute their distributions. These distributions are then used to determine how the specifically the different activations should be quantized at inference time.
In this way, we avoid the drawback of dequantization for allowing the matrix multiplication.</p><p>Here&rsquo;s a visualization of both cases:</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/quant_gemm_2_hu_161cb283eb3288fc.png 330w,
/posts/quantization/quant_gemm_2_hu_465a78f5cd23f82e.png 660w,
/posts/quantization/quant_gemm_2_hu_80290cdc85ae24c8.png 1024w,
/posts/quantization/quant_gemm_2_hu_bdd7ff1b1669f9a0.png 2x" src=/posts/quantization/quant_gemm_2_hu_465a78f5cd23f82e.png alt=quant_gemm_2><figcaption>If only the weights are quantized</figcaption></figure></p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/posts/quantization/quant_gemm_1_hu_24e8d1bf63607c17.png 330w,
/posts/quantization/quant_gemm_1_hu_9d1b77b7d5fb4030.png 660w,
/posts/quantization/quant_gemm_1_hu_e99c2d021490e6c0.png 1024w,
/posts/quantization/quant_gemm_1_hu_bf7ee2e66d463b02.png 2x" src=/posts/quantization/quant_gemm_1_hu_9d1b77b7d5fb4030.png alt=quant_gemm_1><figcaption>If both weights and activations are quantized</figcaption></figure></p><p>I know what you&rsquo;re thinking. The first case doesn&rsquo;t make any sense: why would be quantizing the weights if then we are dequantizing them before doing the GEMM? Trust me, there&rsquo;s a reason and I&rsquo;ll explain it in the next section.</p><h4 class="relative group">Dynamic Quantization: On-the-Fly Adjustments<div id=dynamic-quantization-on-the-fly-adjustments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dynamic-quantization-on-the-fly-adjustments aria-label=Anchor>#</a></span></h4><p>This approach applies to static quantization. However, we could also choose dynamic quantization, where <strong>activations are quantized on the fly</strong> during inference. In this case, there&rsquo;s no need for a calibration dataset, and the activation quantization becomes more accurate since it isn&rsquo;t constrained by a pre-computed, limited distribution derived from the calibration data. The tradeoff remains the same: increased latency in exchange for better accuracy.</p><h4 class="relative group">Quantization Aware Training: Training with Surgical Foresight<div id=quantization-aware-training-training-with-surgical-foresight class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#quantization-aware-training-training-with-surgical-foresight aria-label=Anchor>#</a></span></h4><p>But what if post-training quantization lowers the model&rsquo;s performance too much? In such cases, we can turn to a more sophisticated technique called <strong>Quantization-Aware Training</strong> (QAT). This method simulates quantized computations during the forward pass while retaining higher-precision weights during backpropagation. By making the training process aware of the quantization, QAT helps the model adapt to the lower precision of its parameters, resulting in significantly better accuracy retention.</p><h4 class="relative group">Mixed Precision Training: Balancing Precision and Efficiency<div id=mixed-precision-training-balancing-precision-and-efficiency class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mixed-precision-training-balancing-precision-and-efficiency aria-label=Anchor>#</a></span></h4><p>Since I brought it up earlier, let me briefly explain mixed precision training.
As mentioned before, FP16&rsquo;s dynamic range isn&rsquo;t large enough to store training gradients with sufficient precision. This limitation means that if we were to train entirely in FP16, we&rsquo;d likely end up with a poorly performing model.
This is where mixed precision training comes in. The idea is straightforward: we <strong>start with FP32 weights</strong> and then <strong>quantize them to FP16</strong> to speed up the inference during the forward pass. This results in FP16 gradients, which are then <strong>dequantized back to FP32</strong>. By doing this, we maintain numerical stability and avoid troublesome issues like vanishing or exploding gradients—fascinating phenomena to study but undesirable in practice.
After that, the process is business as usual. The optimizer step remains unchanged, wrapping up the training cycle with the stability of FP32 and the speed benefits of FP16.</p><h3 class="relative group">Why: Quantization as the Life-Saving Operation<div id=why-quantization-as-the-life-saving-operation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-quantization-as-the-life-saving-operation aria-label=Anchor>#</a></span></h3><p>Let&rsquo;s dive into the significant advantages offered by the various quantization techniques we&rsquo;ve explored. Quantization <strong>isn&rsquo;t just about reducing a model&rsquo;s memory footprint</strong>—though that&rsquo;s certainly a key benefit. It also unlocks improvements tailored to specific use cases. Understanding these unique advantages allows us to make informed choices about the best quantization strategy for a given scenario.</p><h4 class="relative group">Dynamic Quantization: Tackling Compute-Bound Emergencies<div id=dynamic-quantization-tackling-compute-bound-emergencies class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dynamic-quantization-tackling-compute-bound-emergencies aria-label=Anchor>#</a></span></h4><p>Picture this: you&rsquo;re working with a massive language model (LLM) boasting 70 billion parameters. In such cases, memory becomes the limiting factor, as <strong>loading the model&rsquo;s weights into memory dominates the inference process</strong>.</p><p>Enter weights-only static quantization. By reducing the precision of the weights, the memory requirements shrink dramatically, enabling <strong>faster weight loading</strong>. Although there&rsquo;s some overhead from the dequantization step (necessary for precise matrix multiplications between weights and activations), this tradeoff is well worth it in memory-bound applications.</p><p>In essence, static quantization relieves the memory bottleneck, leading to substantial improvements in overall inference performance. See? I told you it&rsquo;d make sense!</p><h4 class="relative group">Dynamic Quantization: Tackling Compute-Bound Bottlenecks<div id=dynamic-quantization-tackling-compute-bound-bottlenecks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dynamic-quantization-tackling-compute-bound-bottlenecks aria-label=Anchor>#</a></span></h4><p>Now, let&rsquo;s shift focus to dynamic quantization. While it also reduces the memory footprint by utilizing lower-bit precision data types, its real strength lies in <strong>speeding up computations</strong>. Dynamic quantization dynamically converts weights or activations into integers for matrix multiplications during inference.</p><p>Why is this impactful? Integer matrix multiplications are inherently faster than their floating-point counterparts. As a result, dynamic quantization is a game-changer for <strong>compute-bound scenarios</strong>, where the primary bottleneck is the sheer number of floating-point operations (FLOPs). By quantizing activations and leveraging the speed of integer arithmetic, we can significantly boost performance with only a minor hit to accuracy.</p><h4 class="relative group">Additional Benefits: Energy Efficiency and Scalability in Healthcare Systems<div id=additional-benefits-energy-efficiency-and-scalability-in-healthcare-systems class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#additional-benefits-energy-efficiency-and-scalability-in-healthcare-systems aria-label=Anchor>#</a></span></h4><p>Quantization&rsquo;s benefits extend beyond speed and memory. From an <strong>energy efficiency</strong> perspective, lower-precision computations consume considerably less power. For instance, a 32-bit floating-point multiplication requires about 3.7 picojoules (pJ), while an 8-bit integer multiplication demands just 0.2 pJ—a more than 18-fold reduction in power consumption! This is particularly advantageous for edge devices and large-scale deployments where energy efficiency is critical.</p><p><strong>Scalability</strong> also comes into play. Smaller models are inherently easier to handle due to their reduced memory and computational demands. This makes them more suitable for resource-constrained environments and simplifies scaling in production systems.</p><p>These considerations are crucial in real-world settings, where the performance of a machine learning system is evaluated not just by its metrics but also by its <strong>cost and return on investment</strong>. Not every organization can afford to operate like OpenAI, burning millions of dollars daily. For most companies, quantization offers a practical way to achieve high performance while keeping costs manageable.</p><h2 class="relative group">Quantization in Frameworks: The Surgeon&rsquo;s Toolkit<div id=quantization-in-frameworks-the-surgeons-toolkit class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#quantization-in-frameworks-the-surgeons-toolkit aria-label=Anchor>#</a></span></h2><p>Alright, I&rsquo;ll stop rambling now. By this point, you should have a solid grasp of what quantization is, the technical details of how it works, the methodologies we can apply, and the scenarios where each approach excels.</p><p>Now, let&rsquo;s roll up our sleeves and dive into how to implement quantization in different frameworks.</p><h3 class="relative group">Pytorch: The Scalpel for Precise Optimization<div id=pytorch-the-scalpel-for-precise-optimization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pytorch-the-scalpel-for-precise-optimization aria-label=Anchor>#</a></span></h3><p>Let&rsquo;s start simple. PyTorch, as always, is amazing. One of its recent innovations is <strong>Torch AO</strong> (Architecture Optimization), a framework designed to streamline optimization techniques like quantization. While it&rsquo;s still in beta, it offers a wide range of quantization options that are already incredibly powerful.</p><p>PyTorch supports three primary modes for quantization: <strong>Eager Mode</strong>, <strong>FX Graph Mode</strong>, and <strong>PyTorch 2 Export</strong>. I won&rsquo;t go into the details here, but you can check out the <a href=https://pytorch.org/docs/stable/quantization.html target=_blank>official documentation</a> for a deeper dive.</p><p>Quantizing an <code>nn.Module</code> using Torch AO is straightforward. The library abstracts much of the complexity involved in different quantization methodologies, and most approaches boil down to using the <code>quantize_</code> function.</p><p>This function takes several arguments, with the key ones being the <code>nn.Module</code> to be quantized and a callable apply_tensor_subclass that specifies the quantization type to perform.</p><p>For example, if we want to quantize weights to <strong>A16W4</strong> (activations in 16-bit and weights in 4-bit), we&rsquo;d use:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>quantize_</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>int4_weight_only</span><span class=p>(</span><span class=n>group_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span> <span class=n>use_hqq</span><span class=o>=</span><span class=kc>False</span><span class=p>))</span>
</span></span></code></pre></div><p>Here, <code>int4_weight_only</code> is the function that applies quantization to the weights. Notice that the function takes two arguments, namely <code>group_size</code> and <code>use_hqq</code>. The <strong>Group Size</strong> is a nifty trick for improving quantization precision. Smaller group sizes lead to better precision because, as we&rsquo;ve discussed, scaling factors rely on statistical properties like the minimum and maximum values of the tensor. If the tensor values are widely spread, this can introduce significant de-quantization errors. By dividing the tensor into smaller groups and calculating scaling factors for each, we reduce this error.
<strong>HQQ</strong> instead stands for Half-Quadratic Quantization, and it&rsquo;s a method for performing weight-only quantization without requiring a calibration dataset. It&rsquo;s particularly useful for quick deployment. If you&rsquo;re curious, you can read more in this <a href=https://mobiusml.github.io/hqq_blog/ target=_blank>this blogpost</a>.</p><p>But how does <code>quantize_</code> know which modules to quantize? That&rsquo;s where the filter_fn argument comes into play. This callable determines, module by module, whether quantization should be applied. By default, <code>quantize_</code> only quantizes linear layers using the internal <code>_is_linear</code> function.</p><p>Want to try dynamic quantization? It&rsquo;s just as simple:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>quantize_</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>int8_dynamic_activation_int8_weight</span><span class=p>())</span>
</span></span></code></pre></div><p>This applies <code>int8</code> dynamic symmetric per-token activation quantization and per-channel weight quantization.</p><p>Torch AO offers many other quantization options, but you get the idea. It&rsquo;s a versatile and powerful tool that makes implementing quantization in PyTorch easier than ever.</p><h3 class="relative group">GPTQ: Surgical Innovation in Post-Training Quantization<div id=gptq-surgical-innovation-in-post-training-quantization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gptq-surgical-innovation-in-post-training-quantization aria-label=Anchor>#</a></span></h3><p>GPTQ is a post-training quantization technique that leverages INT4 weights alongside FP16 activations. It builds upon the <strong>Optimal Brain Quantization</strong> (OBQ) framework, which originally introduced per-channel quantization. However, GPTQ incorporates key optimizations that make the process more efficient and scalable.</p><p>For instance, using GPTQ, Bloom 176B can be quantized in under 4 GPU-hours. That&rsquo;s a remarkable achievement, especially when compared to the original OBQ framework, which required 2 GPU-hours just to quantize a 336M parameter BERT model.</p><p>So, how can you apply GPTQ in practice? It&rsquo;s quite straightforward, especially with the <code>auto-gptq</code> library. Here&rsquo;s how you can get started:</p><p>First, define a GPTQ configuration:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gptq_config</span> <span class=o>=</span> <span class=n>GPTQConfig</span><span class=p>(</span><span class=n>bits</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>dataset</span><span class=o>=</span><span class=s2>&#34;c4&#34;</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></span></code></pre></div><p>In this example, we specify: the number of bits for weight quantization (e.g., bits=4 for int4) and the calibration dataset (e.g., &ldquo;c4&rdquo;).</p><p>The configuration can include many other parameters, so I highly recommend checking the official documentation.</p><p>Next, load your model with this configuration:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>quantized_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>quantization_config</span><span class=o>=</span><span class=n>gptq_config</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>And voilà! Your model is ready for deployment with efficient post-training quantization.</p><h3 class="relative group">Transformers&rsquo; <code>LLM.int8()</code>: Mitigating Outliers with Precision Cuts<div id=transformers-llmint8-mitigating-outliers-with-precision-cuts class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#transformers-llmint8-mitigating-outliers-with-precision-cuts aria-label=Anchor>#</a></span></h3><p>By now, you&rsquo;re likely familiar with the main tradeoff in quantization: dequantization error. This error largely stems from the scaling factor, which is derived from the statistical properties of the tensor being quantized. While techniques like group quantization aim to minimize this error by using multiple scaling factors, Hugging Face&rsquo;s Transformers library offers another innovative approach: <code>LLM.int8()</code>.
The core idea behind <code>LLM.int8()</code> is to tackle the dequantization error caused by <strong>outliers</strong>. Instead of attempting to quantize every parameter in the tensor, this method <strong>separates the outliers—parameters</strong> with unusually large magnitudes—and processes them differently.</p><p>Here&rsquo;s how it works. First, outliers are <strong>identified</strong> based on their <strong>magnitude</strong>. Anything above a <strong>threshold of 6</strong> is considered an outlier. Instead of forcing these extreme values into the quantization process, the method <strong>handles them separately</strong> in FP16, while the rest of the tensor is processed in INT8. Afterward, the <strong>two results are combined</strong>—dequantized INT8 data merged with FP16 outliers—to produce the final output.</p><p>This approach works brilliantly for large models, especially those with more than 6 billion parameters, where cumulative dequantization errors can wreak havoc as they propagate through layers. By dealing with outliers differently, <code>LLM.int8()</code> ensures much better precision.</p><p>The threshold is statically set to 6, therefore parameters with a magnitude larger than 6 will be considered outliers and will not be quantized. Is it a coincidence that the threshold is the same as for ReLU6? If you think about the motivation behind ReLU6, I&rsquo;d say it&rsquo;s probably not.</p><p>Of course, there&rsquo;s a catch. Since it performs two separate matrix multiplications, one for the outliers and another for the rest, the method <strong>isn&rsquo;t as fast as FP16</strong>. In fact, it&rsquo;s about 15-23% slower right now, though optimizations are in the works.</p><p>So, how do we actually quantize a model to <code>LLM.int8()</code>? It&rsquo;s pretty straightforward. First, you load the original FP16 or BF16 model as usual. Next, you create a replica using the <code>Linear8bitLt</code> class provided by <code>bitsandbytes</code>. After that, you simply copy the state dictionary from the original model to the quantized one. The magic happens when you move the quantized module to the device—that&rsquo;s when the actual quantization kicks in. Here&rsquo;s a quick example pulled straight from the official documentation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>bitsandbytes</span> <span class=k>as</span> <span class=nn>bnb</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bnb.nn</span> <span class=kn>import</span> <span class=n>Linear8bitLt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fp16_model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>int8_model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>Linear8bitLt</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>has_fp16_weights</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Linear8bitLt</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>has_fp16_weights</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>int8_model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>fp16_model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>int8_model</span> <span class=o>=</span> <span class=n>int8_model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=c1># Quantization happens here</span>
</span></span></code></pre></div><h2 class="relative group">Hardware Support: The Operating Table for Modern Quantization<div id=hardware-support-the-operating-table-for-modern-quantization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hardware-support-the-operating-table-for-modern-quantization aria-label=Anchor>#</a></span></h2><p>At the end of the day, all these methodologies and techniques are only as valuable as the practical benefits they deliver. While they might work beautifully in theory, the real-world performance hinges on <strong>hardware capabilities</strong>. Let&rsquo;s briefly go over the hardware requirements needed to truly leverage these quantization techniques, and then we&rsquo;ll wrap this up—this article has gone on long enough!</p><p><strong>Tensor Cores</strong>, introduced by NVIDIA in 2017 with the Volta architecture, are a game-changer for deep learning. These <strong>specialized cores</strong> were designed to accelerate matrix operations, the backbone of most deep learning workloads. Over the years, NVIDIA has iterated on Tensor Core technology, adding support for an expanding range of data types with each new GPU generation.</p><p>The first generation of Tensor Cores, debuting with the <strong>Volta</strong> architecture (e.g., the V100), supported <strong>FP16 operations</strong>. This innovation helped popularize mixed-precision training by making it both efficient and practical.</p><p>Next came the <strong>Turing</strong> architecture, which introduced a second generation of Tensor Cores capable of handling <strong>integer operations</strong>, including INT8 and INT4.</p><p>The <strong>Ampere</strong> architecture (think A100) further extended <strong>support to BF16</strong>, enabling even more flexibility for precision-constrained computations.</p><p>With the <strong>Hopper</strong> architecture (e.g., the H100), <strong>FP8</strong> was added to the mix, further improving performance for workloads requiring reduced precision. And with NVIDIA&rsquo;s latest <strong>Blackwell</strong> architecture, we now have support for <strong>FP4</strong>, pushing the boundaries even further.</p><p>Another critical factor to consider is <strong>memory bandwidth</strong>. Since quantization reduces the memory footprint of tensors, we need faster cache hierarchies and increased memory bandwidth to efficiently feed Tensor Cores and other low-latency hardware. Without these enhancements, the performance gains from quantization could be bottlenecked by data transfer speeds.</p><h2 class="relative group">Conclusion: The Final Diagnosis<div id=conclusion-the-final-diagnosis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion-the-final-diagnosis aria-label=Anchor>#</a></span></h2><p>Quantization isn&rsquo;t just a buzzword in the operating theater of machine learning; it&rsquo;s a surgical technique that enables us to enhance <strong>efficiency</strong>, <strong>scalability</strong>, and <strong>sustainability</strong> in the field. Whether reducing the memory footprint of a sprawling language model or improving computational speed for edge devices, quantization holds the scalpel for optimizing deep learning systems without sacrificing too much accuracy.</p><p>Yet, as any seasoned surgeon knows, the tools are only as good as the hands that wield them. Understanding the anatomy of <strong>numeric data types</strong>, the timing of <strong>precision adjustments</strong>, and the <strong>capabilities of hardware</strong> ensures that every cut is calculated. Frameworks like PyTorch and techniques like GPTQ empower practitioners to bring these theoretical innovations to life in their practice.</p><p>As we leave the operating room, the prescription is clear: quantization is no longer an optional refinement—it&rsquo;s a <strong>fundamental step</strong> in building systems that are not only powerful but also <strong>efficient</strong>, <strong>affordable</strong>, and <strong>environmentally</strong> conscious. For every aspiring surgeon of machine learning, this technique is not just a tool—it&rsquo;s the future.</p></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://dwarez.github.io/posts/quantization/&amp;title=The%20Machine%20Learning%20Surgeon%27s%20Guide%20to%20Quantization:%20Precision%20Cuts%20for%20Smarter%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://t.me/share/url?url=https://dwarez.github.io/posts/quantization/&amp;resubmit=true&amp;title=The%20Machine%20Learning%20Surgeon%27s%20Guide%20to%20Quantization:%20Precision%20Cuts%20for%20Smarter%20Models" title aria-label><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M248 8C111.033 8 0 119.033.0 256S111.033 504 248 504 496 392.967 496 256 384.967 8 248 8zM362.952 176.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452.0 013.53 6.716A43.765 43.765.0 01362.952 176.66z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://dwarez.github.io/posts/quantization/&amp;resubmit=true&amp;title=The%20Machine%20Learning%20Surgeon%27s%20Guide%20to%20Quantization:%20Precision%20Cuts%20for%20Smarter%20Models" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://dwarez.github.io/posts/quantization/&amp;subject=The%20Machine%20Learning%20Surgeon%27s%20Guide%20to%20Quantization:%20Precision%20Cuts%20for%20Smarter%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/posts/pruning-intro/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/pruning-intro/>An Introduction to Sparsity for Efficient Neural Network Inference</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">7 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/pruning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Pruning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/optimization/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Optimization
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/inference/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Inference</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/profiling-introduction/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/profiling-introduction/>Performing Kernel Surgery: Profiling CUDA Kernels with NVIDIA Nsight Compute</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">9 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/cuda/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cuda
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/profiling/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Profiling
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/optimization/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Optimization</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/matrix-multiplication/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/matrix-multiplication/>A Machine Learning Surgeon’s Toolkit: Advanced Matrix Multiplication in CUDA</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">16 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/cuda/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cuda
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/gpu/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Gpu
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/optimization/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Optimization
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/matrix-multiplication/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Matrix-Multiplication</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/dev-setup/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/dev-setup/>The Operating Room Setup</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">9 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/setup/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Setup
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/cuda/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cuda
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/cpp/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cpp
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/libtorch/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Libtorch</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/torch-compile/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/torch-compile/>Dissecting torch.compile: Surgical Precision in PyTorch Optimization</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">23 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/torch-compile/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Torch-Compile
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/compiler/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Compiler</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/posts/ten-minutes-to-rag/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/posts/ten-minutes-to-rag/>A quick incision: ten minutes to RAG</div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><span title="Reading time">8 mins</span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/rag/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Rag
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/llm/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Llm
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/tags/vector-db/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vector-Db</span></span></span></div></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section></div><script>var oid="views_posts/quantization/index.md",oid_likes="likes_posts/quantization/index.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/dev-setup/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">The Operating Room Setup</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"></span></span></a></span><span><a class="flex text-right group ml-3" href=/posts/speed-or-die/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Move Fast or Die Slow</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"></span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/authors/ title=Authors>Authors</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Dario Salvati</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script><a rel=me href=https://masto.ai/@blowfish></a></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://dwarez.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>