<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Basics on The ML Surgeon</title>
    <link>http://localhost:1313/tags/basics/</link>
    <description>Recent content in Basics on The ML Surgeon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2025 Dario Salvati</copyright>
    <lastBuildDate>Sat, 14 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/basics/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Intro to Triton</title>
      <link>http://localhost:1313/posts/intro-triton/</link>
      <pubDate>Sat, 14 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/intro-triton/</guid>
      <description>We all know that GPU programming is hard. There are many layers of complexity that make writing a kernel difficult: thinking in a parallel way (which goes against our natural sequential thinking), understanding the math behind a certain operation, keeping track of data movement, memory hierarchies, occupancy, concurrency, boundaries, and much more.</description>
      
    </item>
    
    <item>
      <title>Hello CUDA: A Surgical Dissection</title>
      <link>http://localhost:1313/posts/hello-cuda/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/hello-cuda/</guid>
      <description>In case you didn&amp;rsquo;t already know, CUDA is a parallel computing platform and API, developed by NVIDIA, that enables programmers to exploit certain types of GPUs. Not too long ago (around until 2007), GPUs were solely utilized for graphics rendering, and programmers had to depend on very specific graphics APIs to utilize them for solving general problems.</description>
      
    </item>
    
  </channel>
</rss>
